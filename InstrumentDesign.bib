Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@incollection{Iazzetta2000,
abstract = {Technique and technology are two cultural aspects that have been deeply involved with music, not only in relation to its production, but also in relation to the development of its theory and to the establishment of its cultural role. Since the beginning of the twentieth century the relation between music and technology became more intense due to a series of reasons, among them, the increasing knowledge about sound physics and sound cognition; the access to low cost electricity; and the use of electronic and digital technology to artificially generate and manipulate sounds. Before that, musical sounds were produced only by mechanical means. Although musical instruments, such as the violin, the piano or even the human voice, represented a wide variety of forms and mechanisms, all of them were based on the same principle of sound production, that is, the mechanic vibration of an elastic body. However, the appearance of electrical technologies and the use of electromagnetic signals brought the possibility of generating sounds without using mechanical devices.},
author = {Iazzetta, Fernando},
booktitle = {Trends in Gestural Control of Music},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Iazzetta - 2000 - Meaning in Musical Gesture.pdf:pdf},
keywords = {Iazzetta2000},
pages = {259--268},
title = {{Meaning in Musical Gesture}},
url = {http://www.music.mcgill.ca/{~}mwanderley/MUMT-615/Papers/Class03/P.Iaz.pdf},
year = {2000}
}
@techreport{Lippe,
abstract = {As the computer becomes more and more ubiquitous in society, the term “interactive” has become a widely used, but misunderstood term. In this paper, I discuss definitions of interactive music and interactive music systems, performance issues in interactive music, and performer/machine relationships that engender interaction in an attempt to explain how and why I pursue this discipline. Furthermore, I describe the function of computers in my compositions, and the manner in which I explore performer/machine interaction.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Lippe, Cort},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Lippe - Unknown - Real-Time Interaction Among Composers, Performers, and Computer Systems Cort.pdf:pdf},
institution = {The Hiller Computer Music Studios, Department of Music, University at Buffalo},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
title = {{Real-Time Interaction Among Composers, Performers, and Computer Systems Cort}}
}
@phdthesis{Lympouridis2012,
abstract = {This practice-led research investigates a design framework within an artistic context for theimplementationofWholeBodyInteractive(WBI)performancesystemsthatemployreal-timemo-tion capture technology. Following an Interaction Design perspective I engage in exploring therequirements for composers, dancers, musicians and performers and their expectations, withina series of transdisciplinary collaborative artistic projects. Integral to this investigation is a com-prehensive review and analysis of the progression of interactivity in ﬁne art, music, dance andperformance practices, presented in this thesis. As I am particularly concerned with the seamlesstransfer of the tacit skills and the implicit knowledge of non-digital artists and practitioners to aWBI performance setting, my practical explorations emerged in the contexts of music improvisa-tion - Untitled{\#}1 , contemporary dance - Untitled{\#}2 , contemporary music composition - Hiroshima, and traditional dance - Duende . Adopting a Holistic Design approach, the experience and know-ledge gained from my ﬁrst practical explorations led to the design and implementation of a WBIprototyping software environment called EnActor, used in tandem with the Orient wireless in-ertial motion capture system, developed by the Research Consortium in Speckled Computing,at the University of Edinburgh. EnActor provides a simple and effective solution to the prob-lem of linking physical actions to rich digital media responses and can serve as a blueprint forthe development of other WBI design software, since it has operated successfully as a prototype,addressing a wide range of WBI design briefs in various contexts. In this thesis I introduce therole of the WBI designer as a specialist interaction designer able to conceptualise WBI scenariosand implement complete systems that operate within various levels of body sensing and con-trol. I also propose the development of WBI systems that are autonomous and unsupervised,and I explore various compositional concepts and mappings that are implemented as automatic,semi-automatic or manual modules and ultimately arranged into layers and to series of blocksthat represent complete compositions. Following the understanding of interactivity as a property between systems, I identify the design of three basic types of WBI performance systems that dif-fer in how a user engages with them: methodical, empirical and dialectic. Overall this researchaims to facilitate designers and artists interested in the use of real-time motion capture systems indance, music, theatre and performance art applications.},
author = {Lympouridis, Evangelos},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Lympouridis - 2012 - Design Strategies for Whole Body Interactive Performance Systems.pdf:pdf},
pages = {175},
school = {The University of Edinburgh},
title = {{Design Strategies for Whole Body Interactive Performance Systems}},
year = {2012}
}
@incollection{Iazzetta2000,
abstract = {Technique and technology are two cultural aspects that have been deeply involved with music, not only in relation to its production, but also in relation to the development of its theory and to the establishment of its cultural role. Since the beginning of the twentieth century the relation between music and technology became more intense due to a series of reasons, among them, the increasing knowledge about sound physics and sound cognition; the access to low cost electricity; and the use of electronic and digital technology to artificially generate and manipulate sounds. Before that, musical sounds were produced only by mechanical means. Although musical instruments, such as the violin, the piano or even the human voice, represented a wide variety of forms and mechanisms, all of them were based on the same principle of sound production, that is, the mechanic vibration of an elastic body. However, the appearance of electrical technologies and the use of electromagnetic signals brought the possibility of generating sounds without using mechanical devices.},
author = {Iazzetta, Fernando},
booktitle = {Trends in Gestural Control of Music},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Iazzetta - 2000 - Meaning in Musical Gesture.pdf:pdf;:C$\backslash$:/{\_}ICSTdocs/7-literature/sources//Iazzetta - 2000 - Meaning in Musical Gesture.pdf:pdf},
keywords = {Iazzetta2000},
pages = {259--268},
title = {{Meaning in Musical Gesture}},
url = {http://www.music.mcgill.ca/{~}mwanderley/MUMT-615/Papers/Class03/P.Iaz.pdf},
year = {2000}
}
@incollection{Iazzetta2000a,
abstract = {Technique and technology are two cultural aspects that have been deeply involved with music, not only in relation to its production, but also in relation to the development of its theory and to the establishment of its cultural role. Since the beginning of the twentieth century the relation between music and technology became more intense due to a series of reasons, among them, the increasing knowledge about sound physics and sound cognition; the access to low cost electricity; and the use of electronic and digital technology to artificially generate and manipulate sounds. Before that, musical sounds were produced only by mechanical means. Although musical instruments, such as the violin, the piano or even the human voice, represented a wide variety of forms and mechanisms, all of them were based on the same principle of sound production, that is, the mechanic vibration of an elastic body. However, the appearance of electrical technologies and the use of electromagnetic signals brought the possibility of generating sounds without using mechanical devices.},
author = {Iazzetta, Fernando},
booktitle = {Trends in Gestural Control of Music},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Iazzetta - 2000 - Meaning in Musical Gesture(2).pdf:pdf},
keywords = {Iazzetta2000},
pages = {259--268},
title = {{Meaning in Musical Gesture}},
url = {http://www.music.mcgill.ca/{~}mwanderley/MUMT-615/Papers/Class03/P.Iaz.pdf},
year = {2000}
}
@article{Jorda2004c,
abstract = {Musical instruments are used to play and to produce music, transforming the actions of one or more performers into sound. This article explores some instrument design issues, structured into three distinct parts. The first section attempts to define what musical instruments are, how traditional instruments function and what they can do, and what future instruments could be, trying to figure out how we could better exploit their unlimited potential. The second section gives a quick review of the current know-how and the technical and conceptual frameworks in which new instrument designers and researchers are currently working. It is not an actual survey of new instruments and controllers, but more a survey of thoughts and knowledge about them. The third and last section studies the dynamic relationship that builds between the player and the instrument, introducing such concepts as efficiency, apprenticeship, and the learning curve. It explores generic properties of some musical instruments such as the diversity, variability or reproducibility of their musical output, the linearity or non-linearity of their behaviour, and tries to figure out how these aspects can bias the relationship between the instrument and the player, and how they may relate to more commonly studied concepts such as expressivity or virtuosity. The aim of this paper is the foundation of a theoretical framework in which the possibilities and the diversity of musical instruments, as well as the possibilities and expressive freedom of human performers, could all be evaluated. {\textcopyright} 2004, Taylor {\&} Francis Group, LLC.},
author = {Jord{\`{a}}, Sergi},
doi = {10.1080/0929821042000317886},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Jord{\`{a}} - 2004 - Instruments and Players Some Thoughts on Digital Lutherie.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
number = {3},
pages = {321--341},
title = {{Instruments and Players: Some Thoughts on Digital Lutherie}},
volume = {33},
year = {2004}
}
@phdthesis{Graugaard2006,
abstract = {This dissertation presents a new and expanded context for interactive music based on Moore's model for computer music and contextualises its findings using Lesaffre's taxonomy for musical feature extraction and analysis. In doing so, the dissertation examines music as an expressive art-form where musically significant data is present not only in the audio signal but also in human gestures and in physiological data. The dissertation shows the model's foundation in human perception of music as a performed art, and points to the relevance and feasibility of including expression and emotion as a high-level signal processing means for bridging man and machine. The resulting model is multi-level (physical, sensorial, perceptual, formal, expressive) and multi-modal (sound, human gesture, physiological) which makes it applicable to purely musical contexts, as well as intermodal contexts where music is combined with visual and/or physiological data. The model implies evaluating an interactive music system as a musical instrument design. Several properties are examined during the course of the dissertation and models based on acoustic music instruments have been avoided due to the expanded feature set of interactive music system. A narrowing down of the properties is attempted in the dissertation's conclusion together with a preliminary model circumscription. In particular it is pointed out that high-level features of real-time analysis, data storage and processing, and synthesis makes the system a musical instrument, and that the capability of real-time data storage and processing distinguishes the digital system as an unprecedented instrument, qualitatively different from all previous acoustic music instrument. It is considered that a digital system's particular form of sound synthesis only qualifies it as being of a category parallel to the acoustic instruments categories. The model is the result of the author's experiences with practical work with interactive systems developed 2001-06 for a body of commissioned works. The systems and their underlying procedures were conceived and developed addressing needs inherent to the artistic ambitions of each work, and have all been thoroughly tested in many performances. The papers forming part of the dissertation describe the artistic and technological problems and their solutions. The solutions are readily expandable to similar problems in other contexts, and they all relate to general issues of their particular applicative area.},
author = {Graugaard, Lars},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Graugaard - 2006 - Gesture and Emotion in Interactive Music Artistic and Technological Challenges.pdf:pdf},
number = {March},
pages = {1--60},
title = {{Gesture and Emotion in Interactive Music : Artistic and Technological Challenges}},
year = {2006}
}
@inproceedings{Fornari2012a,
abstract = {A common feature found in most musical instruments is the usage of a physical interface to retrieve musician gestures. This is found from traditional musical instruments (e.g. a piano that retrieves gestures throughout keyboards and pedals) to electronic musical instruments, with their digital interfaces, (e.g. MIDI controllers). This article introduces a new type of computer music instrument that does not require a physical (touchable) interface to mediate gestural retrieval. Rather, it is controlled directly by musician's movements without any physical part to be touched, grasped or attached. They are here named as Bodiless Musical Instruments (BMIs). This article presents two BMIs and discusses their computational implementation, artistic performance and aesthetic achievement. 0.},
author = {Fornari, Jose},
booktitle = {Congresso de Engenharia de {\'{A}}udio},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Fornari - 2012 - Designing Bodiless Musical Instruments.pdf:pdf},
number = {16},
title = {{Designing Bodiless Musical Instruments}},
year = {2012}
}
@article{Visi2017a,
abstract = {Music is a complex multimodal medium experienced not only via sounds but also through body movement. Musical instruments can be seen as technological objects coupled with a repertoire of gestures. We present technical and conceptual issues related to the digital representation and mediation of body movement in musical performance. The paper reports on a case study of a musical performance where motion sensor technologies tracked the movements of the musicians while they played their instruments. Motion data were used to control the electronic elements of the piece in real time. It is suggested that computable motion descriptors and machine learning techniques are useful tools for interpreting motion data in a meaningful manner. However, qualitative insights regarding how human body movement is understood and experienced are necessary to inform further development of motion-capture technologies for expressive purposes. Thus, musical performances provide an effective test bed for new modalities of human-computer interaction.},
author = {Visi, Federico and Coorevits, Esther and Schramm, Rodrigo and Miranda, Eduardo Reck},
doi = {10.17011/ht/urn.201705272518},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Visi et al. - 2017 - Musical instruments, body movement, space, and motion data Music as an emergent multimodal choreography.pdf:pdf},
issn = {17956889},
journal = {Human Technology},
keywords = {Motion sensor,Movement,Music,Musical instrument,Performance,Score},
number = {1},
pages = {58--81},
title = {{Musical instruments, body movement, space, and motion data: Music as an emergent multimodal choreography}},
volume = {13},
year = {2017}
}
@article{Doornbusch2003,
author = {Doornbusch, Paul},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Doornbusch - 2003 - Instruments from Now into the Future The Disembodied Voice.pdf:pdf},
journal = {Sounds Australian, Journal of the Australian Music Center},
number = {62},
pages = {1--6},
title = {{Instruments from Now into the Future : The Disembodied Voice}},
year = {2003}
}
@inproceedings{mmainsbridge:2014,
abstract = {This paper explores the challenge of achieving nuanced control and physical engagement with gestural interfaces in performance. Performances with a prototype gestural performance system, Gestate, provide the basis for insights into the application of gestural systems in live contexts. These reflections stem from a performer's perspective, outlining the experience of prototyping and performing with augmented instruments that extend vocal or instrumental technique through ancillary gestures. Successful implementation of rapidly evolving gestural technologies in real-time performance calls for new approaches to performing and musicianship, centred around a growing understanding of the body's physical and creative potential. For musicians hoping to incorporate gestural control seamlessly into their performance practice a balance of technical mastery and kinaesthetic awareness is needed to adapt existing systems to their own purposes. Within non-tactile systems, visual feedback mechanisms can support this process by providing explicit visual cues that compensate for the absence of haptic or tangible feedback. Experience gained through prototyping and performance can yield a deeper understanding of the broader nature of gestural control and the way in which performers inhabit their own bodies.},
address = {London, United Kingdom},
author = {Mainsbridge, Mary and Beilharz, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression (NIME)},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Mainsbridge, Beilharz - 2014 - Body As Instrument Performing with Gestural Interfaces.pdf:pdf},
month = {jun},
pages = {110--113},
publisher = {Goldsmiths, University of London},
title = {{Body As Instrument: Performing with Gestural Interfaces}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}393.pdf},
year = {2014}
}
@article{Visi2014a,
abstract = {This paper describes the implementation of gestural mapping strategies for performance with a traditional musical instrument and electronics. The approach adopted is informed by embodied music cognition and functional categories of musical gestures. Within this framework, gestures are not seen as means of control subordinated to the resulting musical sounds but rather as significant elements contributing to the formation of musical meaning similarly to auditory features. Moreover, the ecological knowledge of the gestural repertoire of the instrument is taken into account as it defines the action-sound relationships between the instrument and the performer and contributes to form expectations in the listeners. Subsequently, mapping strategies from a case study of electric guitar performance will be illustrated describing what motivated the choice of a multimodal motion capture system and how different solutions have been adopted considering both gestural meaning formation and technical constraints.},
author = {Visi, Federico and Schramm, Rodrigo and Miranda, Eduardo},
doi = {10.1145/2617995.2618013},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Visi, Schramm, Miranda - 2014 - Gesture in performance with traditional musical instruments and electronics Use of embodied music cognit.pdf:pdf},
isbn = {9781450328142},
journal = {ACM International Conference Proceeding Series},
keywords = {Embodied music cognition,Expressiveness,Gesture,Guitar,Mapping,Motion capture,Multimodal},
pages = {100--105},
title = {{Gesture in performance with traditional musical instruments and electronics: Use of embodied music cognition and multimodal motion capture to design gestural mapping strategies}},
year = {2014}
}
@phdthesis{Jorda2005,
abstract = {This is a dissertation about performing music with computers, and about constructing the tools that will facilitate playing and improvising with these computers. The primary aim of this research is to construct a theoretical framework that could serve in evaluating the potential, the possibilities and the diversity of new digital musical instruments, with the hope that these ideas may inspire and assist the construction of new and powerful instruments with which perform and listen to wonderful new and previously unheard music. Computer-based interactive music systems date back to the late 1960s, initially involving computer-controlled analog synthesizers in concerts or installations. The use of real-time algorithmic composition spread in the 1970s with the work of composers and performers such as David Behrman, Joel Chadabe, Salvatore Martirano, Gordon Mumma or Laurie Spiegel. However the most rapid period of growth probably occurred during the mid 1980s with the MIDI standardization and, subsequently, with the advent of data-flow graphical programming languages such as Max, which made the design and implementation of custom interactive systems simpler than ever before. In spite of this, nearly four decades after the works of these pioneers, the design of computer-based music instruments, and computer music performance and improvisation in general, still seem immature multidisciplinary areas in which knowledge does not behave in incremental and accumulative ways, resulting in the permanent textquoterightreinvention of the wheeltextquoteright. New digital instrument design is a broad field, encompassing highly technological areas (e.g. electronics and sensor technology, sound synthesis and processing techniques, software engineering, etc.), and disciplines related to the study of human behavior (e.g. psychology, physiology, ergonomics and human-computer interaction components, etc.). Much of this focused research attempts to solve independent parts of the problem an approach essential to achieve any progress in this field. However, as this dissertation will show, it is also clearly insufficient. I believe an approach dedicated to the integrated understanding of the whole is the key to achieving fruitful results. Integral studies and approaches, which consider not only ergonomic or technological but also psychological, philosophical, conceptual, musicological, historical and above all, musical issues, even if non-systematic by definition, are necessary for genuine progress. Putting forward the idea that a digital instrument is a conceptual whole, independent of its potential components and features (e.g. the ways it is controlled or its sonic or musical output tendencies), we will investigate the essence and the potential highlights of new digital instruments, the new musical models and the new music making paradigms they can convey. This dissertation begins with the assumption that better new musical instruments based on computers can only be conceived by exploring three parallel paths identifying the quintessence of new digital instruments; what they can bring of really original to the act of music performance; how can they redefine it; identifying the drawbacks or obsolescences of traditional instruments; what limitations or problems could be eliminated, improved or solved; without forgetting the essential generic assets of traditional instruments; those qualities that should never be forgotten nor discarded. The identification of these points is the primary aim of this thesis. There is a complex interconnected relationship between the tasks of imagining, designing and crafting musical computers, and performing and improvising with them. This relationship can only be understood as a permanent work in progress. This thesis comes from my own experience of fifteen years as a luthier-improviser. Therefore the dissertation is both theoretical (or conceptual) and experimental in approach, although the experiments it documents span years, even decades. To better organize this, the thesis is divided in three parts. Part I progressively enlightens the aforementioned three fundamental exploration paths. This is achieved by introducing the new possibilities offered by digital instruments, in addition to providing a thorough overview of current know-how and of the technical and conceptual frameworks in which new instrument designers and researchers are currently working on. Several taxonomies that will help us in developing a more synthetic and clear overview of the whole subject, are also presented. This first part concludes in chapter seven, presenting the first fundamental contribution of this dissertation; a theoretical framework for the evaluation of the expressive possibilities new digital musical instruments can offer to their performers. Part II describes in depth seven musical instruments, the implementations of my journeys into Digital Lutherie, developed during the previous decade. Since all seven are conceptually very different, each of them serves to illustrate several paradigms introduced in Part I. Presented in chronological order, these music instrument also help to clarify and understand of the path that has led me to the conception of the framework previously introduced. Part III incorporates the teachings and conclusions resulting from this evolutionary journey, and present the final milestone of this dissertation the presentation of possible solutions to better accomplish the goals presented at the end of the part I. Finally this dissertation concludes with what could be considered textquoterightmy digital lutherie decaloguetextquoteright which synthesizes most of the ideas introduced in the thesis. As a postlude, I offer the reacTable to be presented as future work. The reacTable is a digital instrument which constitutes the first one conceived from scratch, that takes into account all the concepts introduced in this thesis, the culmination thus far of my journey into Digital Lutherie},
author = {Jord{\`{a}}, Sergi},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Jord{\`{a}} - 2005 - Digital Lutherie Crafting musical computers for new musics' performance and improvisation.pdf:pdf},
school = {Universitat Pompeu Fabra},
title = {{Digital Lutherie Crafting musical computers for new musics' performance and improvisation}},
url = {http://dialnet.unirioja.es/servlet/tesis?codigo=19509},
year = {2005}
}
@article{Visi2014,
abstract = {This work describes a new approach to gesturemapping in a performance with a traditional musical instrument and live electronics inspired by theories of embodied music cognition (EMC) and musical gestures. Considerations on EMC and how gestures affect the experience of music inform different mapping strategies. Our intent is to enhance the expressive- ness and the liveness of performance by tracking gestures via a multimodal motion capture system and to use mo- tion data to control several features of the music. We then describe an application of such approach to a performance with electric guitar and live electronics, focusing both on aspects of meaning formation and motion capturing.},
author = {Visi, Federico and Schramm, Rodrigo and Miranda, Eduardo},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Visi, Schramm, Miranda - 2014 - Use of Body Motion to Enhance Traditional Musical Instruments.pdf:pdf},
journal = {Proceedings of the International Conference on New Interfaces for Musical Expression (NIME)},
keywords = {embodied music cog-,expressiveness,gesture,gesture recognition,guitar,liveness,mapping,multimodal,nime,nition},
pages = {601----604},
title = {{Use of Body Motion to Enhance Traditional Musical Instruments}},
url = {http://nime2014.org/proceedings/papers/460{\_}paper.pdf},
year = {2014}
}
@inproceedings{Mendes2015,
abstract = {Hyperinstruments (HI) is one of the paramount fronts of development in contemporary musical creation. The interaction between algorithms and minds has helped a myriad of artists to explore and transcend some frontiers of human sensation, perception and cognition, as well as promoting the distributed interaction and remote cooperation of artists in social networks of multimodal artwork performances. This work presents a study on this subject where sessions of free musical improvisation were performed by a group using HI open-source resources as their gestural control. The perceptual and cognitive implications of this human-machine free improvisation interaction are here presented, described and discussed.},
author = {Mendes, Eduardo Aparecido Lopes and Fornari, Jos{\'{e}} and Wanderley, Marcelo Mortensen},
booktitle = {Anais do XI Simp{\'{o}}sio Internacional de Cogni{\c{c}}{\~{a}}o e Artes Musicais},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Mendes, Fornari, Wanderley - 2015 - A study with hyperinstruments in free musical improvisation.pdf:pdf},
keywords = {Free Improvisation,Gesture Control,Human Machine Interaction},
title = {{A study with hyperinstruments in free musical improvisation}},
url = {http://www.nics.unicamp.br.sci-hub.io/revistas/ojs/index.php/nr/article/view/167},
year = {2015}
}
@article{Fornari2012,
abstract = {Formalized music is usually based on the asynchronous creation of musical structures by the composer, that are later expressed in the form of music notation. This can also be considered as an algorithmic structure that, once executed by the musical performer, reaches the state of sonic art; the music per se, formed by the organization of sounds along time. This is perceived by the listener whose cognition attributes to it a personal meaning. This arti- cle introduces a computational process that aims to artistically explore the inversion of its natural order, respectively given by: the composer, the performer and the listener. Throughout a computational model, listener perception data is retrieved and used to control the dynamic creation of musical notation in real-time. After this process is over, the final result is a structured musical notation, generated by a self-organizing process given by the systemic interaction of independent agents, that are: the computer, the composer, the performer and the listeners.},
author = {Fornari, Jos{\'{e}}},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Fornari - 2012 - Revista M{\'{u}}sica Hodie.pdf:pdf},
keywords = {Computer music,Interactive music,Musical notation},
number = {2},
pages = {120--132},
title = {{Revista M{\'{u}}sica Hodie}},
volume = {12},
year = {2012}
}
@article{Jensenius2008,
abstract = {Mobile music technology opens many new opportunities in terms of location-aware systems, social interaction etc., but we should not forget that many challenges faced in ''immobile'' music technology research are also apparent in mobile computing. This paper presents an overview of some challenges related to the design of action-sound relationships and music-movement correspondences, and suggests how these can be studied and tested in mobile devices.},
author = {Jensenius, Alexander Refsum},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Jensenius - 2008 - Some Challenges Related to Music and Movement in Mobile Music Technology.pdf:pdf},
journal = {Proceedings of the 5th International Mobile Music Workshop (MMW'08)},
keywords = {action-sound,music and movement},
pages = {19--22},
title = {{Some Challenges Related to Music and Movement in Mobile Music Technology}},
year = {2008}
}
@article{Jorda2004a,
abstract = {Musical instruments are used to play and to produce music, transforming the actions of one or more performers into sound. This article explores some instrument design issues, structured into three distinct parts. The first section attempts to define what musical instruments are, how traditional instruments function and what they can do, and what future instruments could be, trying to figure out how we could better exploit their unlimited potential. The second section gives a quick review of the current know-how and the technical and conceptual frameworks in which new instrument designers and researchers are currently working. It is not an actual survey of new instruments and controllers, but more a survey of thoughts and knowledge about them. The third and last section studies the dynamic relationship that builds between the player and the instrument, introducing such concepts as efficiency, apprenticeship, and the learning curve. It explores generic properties of some musical instruments such as the diversity, variability or reproducibility of their musical output, the linearity or non-linearity of their behaviour, and tries to figure out how these aspects can bias the relationship between the instrument and the player, and how they may relate to more commonly studied concepts such as expressivity or virtuosity. The aim of this paper is the foundation of a theoretical framework in which the possibilities and the diversity of musical instruments, as well as the possibilities and expressive freedom of human performers, could all be evaluated. {\textcopyright} 2004, Taylor {\&} Francis Group, LLC.},
author = {Jord{\`{a}}, Sergi},
doi = {10.1080/0929821042000317886},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Jord{\`{a}} - 2004 - Instruments and Players Some Thoughts on Digital Lutherie.pdf:pdf},
issn = {0929-8215},
journal = {Journal of New Music Research},
month = {sep},
number = {3},
pages = {321--341},
title = {{Instruments and Players: Some Thoughts on Digital Lutherie}},
url = {http://www.tandfonline.com/doi/abs/10.1080/0929821042000317886},
volume = {33},
year = {2004}
}
@article{Jorda2004a,
abstract = {Musical instruments are used to play and to produce music, transforming the actions of one or more performers into sound. This article explores some instrument design issues, structured into three distinct parts. The first section attempts to define what musical instruments are, how traditional instruments function and what they can do, and what future instruments could be, trying to figure out how we could better exploit their unlimited potential. The second section gives a quick review of the current know-how and the technical and conceptual frameworks in which new instrument designers and researchers are currently working. It is not an actual survey of new instruments and controllers, but more a survey of thoughts and knowledge about them. The third and last section studies the dynamic relationship that builds between the player and the instrument, introducing such concepts as efficiency, apprenticeship, and the learning curve. It explores generic properties of some musical instruments such as the diversity, variability or reproducibility of their musical output, the linearity or non-linearity of their behaviour, and tries to figure out how these aspects can bias the relationship between the instrument and the player, and how they may relate to more commonly studied concepts such as expressivity or virtuosity. The aim of this paper is the foundation of a theoretical framework in which the possibilities and the diversity of musical instruments, as well as the possibilities and expressive freedom of human performers, could all be evaluated. {\textcopyright} 2004, Taylor {\&} Francis Group, LLC.},
author = {Jord{\`{a}}, Sergi},
doi = {10.1080/0929821042000317886},
file = {:C$\backslash$:/Users/sschies1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jord{\`{a}} - 2004 - Instruments and Players Some Thoughts on Digital Lutherie.pdf:pdf},
issn = {0929-8215},
journal = {Journal of New Music Research},
month = {sep},
number = {3},
pages = {321--341},
title = {{Instruments and Players: Some Thoughts on Digital Lutherie}},
url = {http://www.tandfonline.com/doi/abs/10.1080/0929821042000317886},
volume = {33},
year = {2004}
}
@article{Portovedo2017,
abstract = {This paper discusses a two-layer augmentation strategy applied to a saxophone. Augmented instruments are defined as "acoustic (sometimes electric) musical instruments extended by the addition of several sensors, providing performers the ability to control extra sound or musical parameters". The first layer of augmentation is directly connected to the instrument and is based on several sensors (ribbon, trigger, pressure, accelerometer, gyroscope and keypad). The second layer is associated to the performer, who wears a MYO Armband. The second layer, devised initially to capture gestural data, is used to provide information for musical communication and creation, as performer gestures are perceived by the audience at the same time as they characterise and distinguish performer strategies. This paper also explores how this system can serve for the adaptation of existing pieces using electronics and external devices. Finally, we discuss how notation and composition are affected by this type of instruments.},
author = {Portovedo, Henrique and {Ferreira Lopes}, Paulo and Mendes, Ricardo},
doi = {10.1145/3106548.3106611},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Portovedo, Ferreira Lopes, Mendes - 2017 - Saxophone augmentation An hybrid augmented system of gestual symbiosis.pdf:pdf},
isbn = {9781450352734},
journal = {ACM International Conference Proceeding Series},
keywords = {Augmented Instruments,Gestural Interaction,Live Electronics,Saxophone},
pages = {157--160},
title = {{Saxophone augmentation: An hybrid augmented system of gestual symbiosis}},
volume = {Part F1309},
year = {2017}
}
@inproceedings{Schmeder2008,
abstract = {A general-purpose firmware for a low cost microcontroller is described that employs the Open Sound Control protocol over USB. The firmware is designed with considerations for integration in new musical interfaces and embedded devices. Features of note include stateless design, efficient floating-point support, temporally correct data handling, and protocol completeness. A timing performance analysis is conducted.},
address = {Genoa, Italy},
author = {Schmeder, Andrew Andy and Freed, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Schmeder, Freed - 2008 - uOSC The Open Sound Control Reference Platform for Embedded Devices.pdf:pdf},
keywords = {Open Sound Control,PIC microcontroller,USB,jitter,latency,nime08,open sound control,pic microcontroller,usb},
pages = {175--180},
title = {{uOSC : The Open Sound Control Reference Platform for Embedded Devices}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}175.pdf},
year = {2008}
}
@inproceedings{Beaudouin-Lafon2004,
abstract = {Although the power of personal computers has increased 1000-fold over the past 20 years, user interfaces remain essentially the same. Innovations in HCI research, particularly novel interaction techniques, are rarely incorporated into products. In this paper I argue that the only way to significantly improve user interfaces is to shift the research focus from designing interfaces to designing interaction. This requires powerful interaction models, a better understanding of both the sensory-motor details of interaction and a broader view of interaction in the context of use. It also requires novel interaction architectures that address reinterpretability, resilience and scalability.},
annote = {NULL},
author = {Beaudouin-Lafon, Michel},
booktitle = {Working Conference on Advanced Visual Interfaces (AVI'04)},
doi = {10.1145/989863.989865},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Beaudouin-Lafon - 2004 - Designing Interaction, Not Interfaces.pdf:pdf},
isbn = {1-58113-867-9},
issn = {03043800},
keywords = {design principles,instrumental interaction,interaction architecture,interaction model,interaction paradigm,situated interaction},
pages = {15--22},
pmid = {20403315},
title = {{Designing Interaction, Not Interfaces}},
url = {http://doi.acm.org/10.1145/989863.989865},
year = {2004}
}
@inproceedings{Schmeder2008,
abstract = {A general-purpose firmware for a low cost microcontroller is described that employs the Open Sound Control protocol over USB. The firmware is designed with considerations for integration in new musical interfaces and embedded devices. Features of note include stateless design, efficient floating-point support, temporally correct data handling, and protocol completeness. A timing performance analysis is conducted.},
address = {Genoa, Italy},
author = {Schmeder, Andrew Andy and Freed, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {:C$\backslash$:/Users/sschies1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmeder, Freed - 2008 - uOSC The Open Sound Control Reference Platform for Embedded Devices.pdf:pdf},
keywords = {Open Sound Control,PIC microcontroller,USB,jitter,latency,nime08,open sound control,pic microcontroller,usb},
pages = {175--180},
title = {{uOSC : The Open Sound Control Reference Platform for Embedded Devices}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}175.pdf},
year = {2008}
}
@article{Cox2004,
abstract = {The term 'interactivity' has become a buzzword over the past few years, finding its way into descriptions and discussions of musical works. Yet for all its popularity, 'interactivity' is also a somewhat elusive term. What does it mean, exactly, for a work to be 'interactive'? The following qualifying factors may be used to define and evaluate interactive works: 1) type of control input—sound and motion; 2) directness of control; 3) sound techniques under control; 4) performer type; and 5) performance type. Specific realizations of interactive musical works illustrate the respective issues. Analysis must take into account how the aesthetic appreciation of a work is affected by the type of interactivity involved for the composer, the performer, and the audience. Abstractor},
author = {Cox, Cathy},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Cox - 2004 - Interactive technologies in music composition towards a theory of interactivity.pdf:pdf},
journal = {Music research: new directions for a new century},
keywords = {interaction,music,technology},
mendeley-tags = {interaction,music,technology},
pages = {333--342},
title = {{Interactive technologies in music composition: towards a theory of interactivity}},
year = {2004}
}
@article{Aksnes2007,
author = {Aksnes, Hallgjerd and Kvifte, Tellef and Ruud, Even},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Aksnes, Kvifte, Ruud - 2007 - The Musical Gesture Project 2004 - 2007.pdf:pdf},
number = {Middleton 1993},
pages = {1--8},
title = {{The Musical Gesture Project 2004 - 2007}},
year = {2007}
}
@article{Hemery2015,
abstract = {A large variety of musical instruments, either acoustical or digital, are based on a keyboard scheme. Keyboard instruments can produce sounds through acoustic means but they are increasingly used to control digital sound synthesis processes with nowadays music. Interestingly, with all the different possibilities of sonic outcomes, the input remains a musical gesture. In this paper we present the conceptualization of a Natural User Interface (NUI), named the Intangible Musical Instrument (IMI), aiming to support both learning of expert musical gestures and performing music as a unified user experience. The IMI is designed to recognize metaphors of pianistic gestures, focusing on subtle uses of fingers and upper-body. Based on a typology of musical gestures, a gesture vocabulary has been created, hierarchized from basic to complex. These piano-like gestures are finally recognized and transformed into sounds.},
author = {Hemery, Edgar and Manitsaris, Sotiris and Moutarde, Fabien and Volioti, Christina and Manitsaris, Athanasios},
doi = {10.1016/j.promfg.2015.07.952},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Hemery et al. - 2015 - Towards the Design of a Natural User Interface for Performing and Learning Musical Gestures.pdf:pdf},
issn = {23519789},
journal = {Procedia Manufacturing},
keywords = {Ergonomics,Gesture recognition,Human factors,Interactive design,Musical interface,Natural user interface,User experience},
pages = {6329--6336},
publisher = {Elsevier B.V.},
title = {{Towards the Design of a Natural User Interface for Performing and Learning Musical Gestures}},
url = {http://dx.doi.org/10.1016/j.promfg.2015.07.952},
volume = {3},
year = {2015}
}
@inproceedings{Hewitt2003,
abstract = {This paper describes work in progress for the development of a gestural controller interface for contemporary vocal performance and electronic processing. The paper includes a preliminary investigation of the gestures and movements of vocalists who use microphones and microphone stands. This repertoire of gestures formsthe foundation of a well-practiced ‘language' and social code for communication between performersand audiences andservesasa basisfor alternate controller design principles. A prototype design, based on a modified microphone stand, is presented along with a discussion of possible controller mapping strategies and identification of directions for future research.},
address = {Montreal, QC, Canada},
author = {Hewitt, Donna and Stevenson, Ian and Ave, Peckham},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Hewitt, Stevenson, Ave - 2003 - E-mic Extended Mic-stand Interface Controller.pdf:pdf},
keywords = {Alternate controller,alternate controller,electronic music,gesture,microphone technique,performance,performance interface,vocal,vocal performance},
pages = {122--128},
title = {{E-mic: Extended Mic-stand Interface Controller}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}122.pdf},
year = {2003}
}
