Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Flety2005,
address = {Vancouver, BC, Canada},
author = {Fl{\'{e}}ty, Emmanuel},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {266--267},
title = {{The WiSe Box: a Multi-performer Wireless Sensor Interface using WiFi and OSC}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}266.pdf},
year = {2005}
}
@inproceedings{Kiser2006,
address = {Paris, France},
author = {Kiser, Spencer},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
pages = {75--76},
title = {{spinCycle: a Color-Tracking Turntable Sequencer}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}075.pdf},
year = {2006}
}
@inproceedings{Martin:2013,
abstract = {This paper describes the development of an Apple iPhone based mobile computersystem for vibraphone and its use in a series of the author's performanceprojects in 2011 and 2012.This artistic research was motivated by a desire to develop an alternative tolaptop computers for the author's existing percussion and computer performancepractice. The aims were to develop a light, compact and flexible system usingmobile devices that would allow computer music to infiltrate solo and ensembleperformance situations where it is difficult to use a laptop computer.The project began with a system that brought computer elements to NordligVinter, a suite of percussion duos, using an iPhone, RjDj, Pure Data and ahome-made pickup system. This process was documented with video recordings andanalysed using ethnographic methods.The mobile computer music setup proved to be elegant and convenient inperformance situations with very little time and space to set up, as well as inperformance classes and workshops. The simple mobile system encouragedexperimentation and the platforms used enabled sharing with a wider audience.},
address = {Daejeon, Republic of Korea},
author = {Martin, Charles},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Apple iOS,artistic research,collaborative performance practice,ethnography,mobile computer music,percussion},
pages = {377--380},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Performing with a Mobile Computer System for Vibraphone}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Steiner2009,
address = {Pittsburgh, PA, United States},
author = {Steiner, Hans-Christoph},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {arduino,microcontroller,nime09,processing,pure data},
pages = {125--130},
title = {{Firmata : Towards Making Microcontrollers Act Like Extensions of the Computer}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}125.pdf},
year = {2009}
}
@inproceedings{Schacher:2013,
abstract = {This article documents a class that teaches gestural interaction and juxtaposestraditional instrumental skills with digital musical instrument concepts. Inorder to show the principles and reflections that informed the choices made indeveloping this syllabus, fundamental elements of an instrument-bodyrelationship and the perceptual import of sensori-motor integration areinvestigated. The methods used to let participants learn in practicalexperimental settings are discussed, showing a way to conceptualise andexperience the entire workflow from instrumental sound to electronictransformations by blending gestural interaction with digital musicalinstrument techniques and traditional instrumental playing skills. Thetechnical interfaces and software that were deployed are explained, focussingof the interactive potential offered by each solution. In an attempt tosummarise and evaluate the impact of this course, a number of insights relatingto this specific pedagogical situation are put forward. Finally, concreteexamples of interactive situations that were developed by the participants areshown in order to demonstrate the validity of this approach.},
address = {Daejeon, Republic of Korea},
author = {Schacher, Jan C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Schacher - 2013 - Hybrid Musicianship - Teaching Gestural Interaction with Traditional and Digital Instruments.pdf:pdf},
keywords = {digital musical instruments,enactive approach,gestural interaction,mapping,pedagogy},
pages = {55--60},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Hybrid Musicianship - Teaching Gestural Interaction with Traditional and Digital Instruments}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Buch2010,
address = {Sydney, Australia},
author = {Buch, Benjamin and Coussement, Pieter and Schmidt, L{\"{u}}der},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {dynamic mapping,embodiment,finite state au-,human-robot interaction,new media art,nime10,structured,tomata},
number = {June},
pages = {15--18},
title = {{"playing robot" : An Interactive Sound Installation in Human-Robot Interaction Design for New Media Art}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}411.pdf},
year = {2010}
}
@inproceedings{Bouillot2007,
address = {New York City, NY, United States},
author = {Bouillot, Nicolas},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {chronization,distributed metronome,end-to-end delays,nime07,nmp,remote real-time musical interaction,syn-,user experiments},
number = {section 3},
pages = {142--147},
title = {{nJam User Experiments : Enabling Remote Musical Interaction from Milliseconds to Seconds}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}142.pdf},
year = {2007}
}
@inproceedings{Shahar:2012,
abstract = {SoundStrand is a tangible music composition tool. It demonstrates a paradigm developed to enable music composition through the use of tangible interfaces. This paradigm attempts to overcome the contrast between the relatively small of amount degrees of freedom usually demonstrated by tangible interfaces and the vast number of possibilities that musical composition presents.
SoundStrand is comprised of a set of physical objects called cells, each representing a musical phrase. Cells can be sequentially connected to each other to create a musical theme. Cells can also be physically manipulated to access a wide range of melodic, rhythmic and harmonic variations. The SoundStrand software assures that as the cells are manipulated, the melodic flow, harmonic transitions and rhythmic patterns of the theme remain musically plausible while preserving the user's intentions.},
address = {Ann Arbor, Michigan},
author = {Shahar, Eyal},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Tangible,algorithmic,composition,computer assisted},
publisher = {University of Michigan},
title = {{SoundStrand: a Tangible Interface for Composing Music with Limited Degrees of Freedom}},
year = {2012}
}
@inproceedings{Gelineck2009,
address = {Pittsburgh, PA, United States},
author = {Gelineck, Steven and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {cal modeling,creativ-,electronic musicians,evaluation,exploration,interfaces,knobs,nime09,physi-,sliders},
mendeley-tags = {evaluation},
pages = {13--18},
title = {{A Quantitative Evaluation of the Differences between Knobs and Sliders}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}013.pdf},
year = {2009}
}
@inproceedings{Kikukawa:2013,
abstract = {So far, there are few studies of string instruments with bows because there aremany parameters to acquire skills and it is difficult to measure theseparameters. Therefore, the aim of this paper is to propose a design of alearning environment for a novice learner to acquire an accurate fingerposition skill. For achieving the aim, we developed a learning environmentwhich can diagnose learner's finger position and give the learner advice byusing magnetic position sensors. The system shows three windows; a fingerposition window for visualization of finger position, a score window fordiagnosing finger position along the score and command prompt window forshowing states of system and advices. Finally, we evaluated the system by anexperiment. The experimental group improved accuracy values about fingerpositions and also improved accuracy of pitches of sounds compared withcontrol group. These results shows significant differences.},
address = {Daejeon, Republic of Korea},
author = {Kikukawa, Fumitaka and Ishihara, Sojiro and Soga, Masato and Taki, Hirokazu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Finger Position,Learning Environment,Magnetic Position Sensors,Skill,String Instruments},
pages = {271--276},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Development of A Learning Environment for Playing Erhu by Diagnosis and Advice regarding Finger Position on Strings}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Bau2008,
abstract = {We combine two concepts, the musical instrument as metaphorand technology probes, to explore how tangible interfaces canexploit the semantic richness of sound. Using participatorydesign methods from Human-Computer Interaction (HCI), wedesigned and tested the A20, a polyhedron-shaped, multichannel audio input/output device. The software maps soundaround the edges and responds to the user's gestural input,allowing both aural and haptic modes of interaction as well asdirect manipulation of media content. The software is designedto be very flexible and can be adapted to a wide range ofshapes. Our tests of the A20's perceptual and interactionproperties showed that users can successfully detect soundplacement, movement and haptic effects on this device. Ourparticipatory design workshops explored the possibilities of theA20 as a generative tool for the design of an extended,collaborative personal music player. The A20 helped users toenact scenarios of everyday mobile music player use and togenerate new design ideas.},
address = {Genoa, Italy},
author = {Bau, Olivier and Tanaka, Atau and Mackay, Wendy E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Generative design tools,Instrument building,Multi-faceted audio,Personal music devices,Tangible user interfaces,Technology probes},
pages = {91--96},
title = {{The A20 : Musical Metaphors for Interface Design}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}091.pdf},
year = {2008}
}
@inproceedings{Oore2005,
address = {Vancouver, BC, Canada},
author = {Oore, Sageev},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
keywords = {learning new instruments,performance},
pages = {60--64},
title = {{Learning Advanced Skills on New Instruments}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}060.pdf},
volume = {c},
year = {2005}
}
@inproceedings{Caramiaux:2013,
abstract = {We present an overview of machine learning (ML) techniques and theirapplication in interactive music and new digital instruments design. We firstgive to the non-specialist reader an introduction to two ML tasks,classification and regression, that are particularly relevant for gesturalinteraction. We then present a review of the literature in current NIMEresearch that uses ML in musical gesture analysis and gestural sound control.We describe the ways in which machine learning is useful for creatingexpressive musical interaction, and in turn why live music performance presentsa pertinent and challenging use case for machine learning.},
address = {Daejeon, Republic of Korea},
author = {Caramiaux, Baptiste and Tanaka, Atau},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Analysis,Control,Data mining,Gesture,Machine Learning,Musical Expression,Musical Gestures,Sound},
month = {may},
pages = {513--518},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Machine Learning of Musical Gestures}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Sarwate:2013,
abstract = {The Variator is a compositional assistance tool that allows users to quicklyproduce and experiment with variations on musical objects, such as chords,melodies, and chord progressions. The transformations performed by the Variatorcan range from standard counterpoint transformations (inversion, retrograde,transposition) to more complicated custom transformations, and the system isbuilt to encourage the writing of custom transformations.This paper explores the design decisions involved in creating a compositionalassistance tool, describes the Variator interface and a preliminary set ofimplemented transformation functions, analyzes the results of the evaluationsof a prototype system, and lays out future plans for expanding upon thatsystem, both as a stand-alone application and as the basis for an opensource/collaborative community where users can implement and share their owntransformation functions.},
address = {Daejeon, Republic of Korea},
author = {Sarwate, Avneesh and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Composition assistance tool,computer-aided composition,social composition},
pages = {279--282},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Variator: A Creativity Support Tool for Music Composition}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Coghlan2008,
address = {Genoa, Italy},
author = {Coghlan, Niall and Knapp, Benjamin},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {affective computing,biosignals,chairs,context -awareness,digital artefacts,embedded systems,emotional state,networking,nime08,sensing,ubiquitous computing},
pages = {233--236},
title = {{Sensory Chairs : A System for Biosignal Research and Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}233.pdf},
year = {2008}
}
@inproceedings{Fox2005,
abstract = {This paper describes the design of SoniMime, a system forthe sonification of hand movement for real-time timbre shaping. We explore the application of the tristimulus timbremodel for the sonification of gestural data, working towardthe goals of musical expressivity and physical responsiveness. SoniMime uses two 3-D accelerometers connected toan Atmel microprocessor which outputs OSC control messages. Data filtering, parameter mapping, and sound synthesis take place in Pd running on a Linux computer.},
address = {Vancouver, BC, Canada},
author = {Fox, Jesse and Carlile, Jennifer},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Human Computer Interac- tion,Musical Controller,Sonification},
pages = {242--243},
title = {{SoniMime: Movement Sonification for Real-Time Timbre Shaping}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}242.pdf},
year = {2005}
}
@inproceedings{McCormack:2012,
address = {Ann Arbor, Michigan},
author = {McCormack, Matthew and Figg, Jenn},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
publisher = {University of Michigan},
title = {{Kinetic Light Drums / Community Beacons}},
year = {2012}
}
@inproceedings{Matsumura2005,
address = {Vancouver, BC, Canada},
author = {Matsumura, Seiichiro and Arakawa, Chuichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {273},
title = {{Hop Step Junk: Sonic Visualization using Footsteps}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}273.pdf},
year = {2005}
}
@inproceedings{Hayes2010,
address = {Sydney, Australia},
author = {Hayes, Ted},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {controller,human computer interaction,interface,live performance,neural network,nime10,sequencer},
number = {June},
pages = {23--25},
title = {{Neurohedron : A Nonlinear Sequencer Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}023.pdf},
year = {2010}
}
@inproceedings{Fyans2011,
abstract = {We present observations from two separate studies of spectators' perceptions of musical performances, one involvingtwo acoustic instruments, the other two electronic instruments. Both studies followed the same qualitative method,using structured interviews to ascertain and compare spectators' experiences. In this paper, we focus on outcomespertaining to perceptions of the performers' skill, relatingto concepts of embodiment and communities of practice.},
address = {Oslo, Norway},
author = {Fyans, A Cavan and Gurevich, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {control,effort,embodiment,perception,skill,spectator},
pages = {495--498},
title = {{Perceptions of Skill in Performances with Acoustic and Electronic Instruments}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}495.pdf},
year = {2011}
}
@inproceedings{GreshamLancaster2009,
address = {Pittsburgh, PA, United States},
author = {Gresham-Lancaster, Scot and Bull, Steve},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {338},
title = {{Cellphonia: 4'33"}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}338.pdf},
year = {2009}
}
@inproceedings{Lee2007a,
abstract = {We present REXband, an interactive music exhibit for collaborative improvisation to medieval music. This audio-only system consists of three digitally augmented medieval instrument replicas: thehurdy gurdy, harp, and frame drum. The instruments communicatewith software that provides users with both musical support andfeedback on their performance using a "virtual audience" set in amedieval tavern. REXband builds upon previous work in interactivemusic exhibits by incorporating aspects of e-learning to educate, inaddition to interaction design patterns to entertain; care was alsotaken to ensure historic authenticity. Feedback from user testingin both controlled (laboratory) and public (museum) environmentshas been extremely positive. REXband is part of the RegensburgExperience, an exhibition scheduled to open in July 2007 to showcase the rich history of Regensburg, Germany.},
address = {New York City, NY, United States},
author = {Lee, Eric and Wolf, Marius and Jansen, Yvonne and Borchers, Jan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {augmented instru- ments,e-learning,education,interactive music exhibits,medieval music},
pages = {172--177},
title = {{REXband : A Multi-User Interactive Exhibit for Exploring Medieval Music}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}172.pdf},
year = {2007}
}
@inproceedings{Beilharz2010,
address = {Sydney, Australia},
author = {Beilharz, Kirsty and {Vande Moere}, Andrew and Stiel, Barbara and Calo, Claudia and Tomitsch, Martin and Lombard, Adrian},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {bimodal display,design aesthetics,multimodal expression,nime10,physical computing,sonification,visualisation,wearable display},
number = {Nime},
pages = {323--326},
title = {{Expressive Wearable Sonification and Visualisation : Design and Evaluation of a Flexible Display}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}323.pdf},
year = {2010}
}
@inproceedings{Butler2008,
abstract = {In this paper I discuss the importance of and need forpedagogical materials to support the development of newinterfaces and new instruments for electronic music. I describemy method for creating a graduated series of pedagogicaletudes composed using Max/MSP. The etudes will helpperformers and instrument designers learn the most commonlyused basic skills necessary to perform with interactiveelectronic music instruments. My intention is that the finalseries will guide a beginner from these initial steps through agraduated method, eventually incorporating some of the moreadvanced techniques regularly used by electronic musiccomposers.I describe the order of the series, and discuss the benefits (bothto performers and to composers) of having a logical sequence ofskill-based etudes. I also connect the significance of skilledperformers to the development of two essential areas that Iperceive are still just emerging in this field: the creation of acomposed repertoire and an increase in musical expressionduring performance.},
address = {Genoa, Italy},
author = {Butler, Jennifer},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {composition,etudes,max,msp,musical controllers,musical expression,nime08,pedagogy,repertoire},
pages = {77--80},
title = {{Creating Pedagogical Etudes for Interactive Instruments}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}077.pdf},
year = {2008}
}
@inproceedings{Young2008,
address = {Genoa, Italy},
author = {Young, Diana},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {bowing,classification,gesture,playing technique,principal component anal- ysis},
pages = {44--48},
title = {{Classification of Common Violin Bowing Techniques Using Gesture Data from a Playable Measurement System}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}044.pdf},
year = {2008}
}
@inproceedings{Keefe2011,
address = {Oslo, Norway},
author = {Keefe, Patrick O and Essl, Georg},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {camera phone,mo-,mobile performance,visual interaction},
number = {June},
pages = {191--196},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{The Visual in Mobile Music Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}191.pdf},
year = {2011}
}
@inproceedings{Derbinsky:2012,
abstract = {This paper presents a system for mobile percussive collaboration. We show that reinforcement learning can incrementally learn percussive beat patterns played by humans and supports realtime collaborative performance in the absence of one or more performers. This work leverages an existing integration between urMus and Soar and addresses multiple challenges involved in the deployment of machine-learning algorithms for mobile music expression, including tradeoffs between learning speed {\&} quality; interface design for human collaborators; and real-time performance and improvisation.},
address = {Ann Arbor, Michigan},
author = {Derbinsky, Nate and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Mobile music,cognitive architecture,machine learning},
publisher = {University of Michigan},
title = {{Exploring Reinforcement Learning for Mobile Percussive Collaboration}},
year = {2012}
}
@inproceedings{Fencott2010,
abstract = {This research is concerned with issues of privacy, awareness and the emergence of roles in the process of digitallymediated collaborative music making. Specifically we areinterested in how providing collaborators with varying degrees of privacy and awareness of one another influencesthe group interaction. A study is presented whereby ninegroups of co-located musicians compose music together using three different interface designs. We use qualitative andquantitative data to study and characterise the musician'sinteraction with each other and the software. We show thatwhen made available to them, participants make extensiveuse of a private working area to develop musical contributions before they are introduced to the group. We also arguethat our awareness mechanisms change the perceived quality of the musical interaction, but have no impact on theway musicians interact with the software. We then reflecton implications for the design of new collaborative musicmaking tools which exploit the potential of digital technologies, while at the same time support creative musicalinteraction.},
address = {Sydney, Australia},
author = {Fencott, Robin and Bryan-Kinns, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Awareness,Collaboration,Design,En- gagement,Evaluation.,Group Music Making,Interaction,Music,Privacy},
pages = {198--203},
title = {{Hey Man, You're Invading my Personal Space ! Privacy and Awareness in Collaborative Music}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}198.pdf},
year = {2010}
}
@inproceedings{Paine2010,
address = {Sydney, Australia},
author = {Paine, Garth},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {2,instrument,interface,nime10,organology,range of,revising definitions,submissions across a wide,taxonomy,the tiem survey received},
number = {Nime},
pages = {436--439},
title = {{Towards a Taxonomy of Realtime Interfaces for Electronic Music Performance}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}436.pdf},
year = {2010}
}
@inproceedings{Rothman2010,
address = {Sydney, Australia},
author = {Rothman, Paul},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {controller,live performance,midi,nime10,open-,programmable},
number = {Nime},
pages = {431--435},
title = {{The Ghost : An Open-Source, User Programmable MIDI Performance Controller}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}431.pdf},
year = {2010}
}
@inproceedings{Bukvic2009,
address = {Pittsburgh, PA, United States},
author = {Bukvic, Ivika and Standley, Eric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {337},
title = {{Elemental {\&} Cyrene Reefs}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}337.pdf},
year = {2009}
}
@inproceedings{Fan:2013,
abstract = {We show how body-centric sensing can be integrated in musical interface toenable more flexible gestural control. We present a barehanded body-centricinteraction paradigm where users are able to interact in a spontaneous waythroughperforming gestures. The paradigm employs a wearable camera and see-throughdisplay to enable flexible interaction in the 3D space. We designed andimplemented a prototype called Air Violin, a virtual musical instrument usingdepth camera, to demonstrate the proposed interaction paradigm. We describedthe design and implementation details.},
address = {Daejeon, Republic of Korea},
author = {Fan, Xin and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Kinect,NIME,gesture,interaction,musical instrument},
pages = {122--123},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Air Violin: A Body-centric Style Musical Instrument}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Melo:2012,
abstract = {This paper describes the conceptualization and development of an open source tool for controlling the sound of a saxophone via the gestures of its performer. The motivation behind this work is the need for easy access tools to explore, compose and perform electroacoustic music in Colombian music schools and conservatories. This work led to the adaptation of common hardware to be used as a sensor attached to an acoustic instrument and the development of software applications to record, visualize and map performers gesture data into signal processing parameters. The scope of this work suggested that focus was to be made on a specific instrument so the saxophone was chosen. Gestures were selected in an iterative process with the performer, although a more ambitious strategy to figure out main gestures of an instruments performance was first defined. Detailed gesture-to-sound processing mappings are exposed in the text. An electroacoustic musical piece was successfully rehearsed and recorded using the Gest-O system.},
address = {Ann Arbor, Michigan},
author = {Melo, Jonh and G{\'{o}}mez, Daniel and Vargas, Miguel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Electroacoustic music,expanded instrument,gesture.,saxophone},
month = {may},
publisher = {University of Michigan},
title = {{Gest-O: Performer gestures used to expand the sounds of the saxophone}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}262.pdf},
year = {2012}
}
@inproceedings{Bergsland2011,
address = {Oslo, Norway},
author = {Bergsland, Andreas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {LPC,analysis,csound,modeling,software instrument},
pages = {523--526},
title = {{phrases from Paul Lansky {\{}$\backslash$textquoteright{\}} s Six Fantasies}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}523.pdf},
year = {2011}
}
@inproceedings{Fels2004,
address = {Hamamatsu, Japan},
author = {Fels, Sidney S and Kaastra, Linda and Takahashi, Sachiyo and Mccaig, Graeme},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
isbn = {6048224583},
keywords = {a four-person chorus along,authors,figure1,mikrophonie ii,musician-centred design,playing tooka,two musicians,two-person musical instrument,with music from an},
pages = {1--6},
title = {{Evolving Tooka: from Experiment to Instrument}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}001.pdf},
year = {2004}
}
@inproceedings{nime2014:jjeon,
abstract = {This paper presents an enhanced sonic data communication method using TAPIR (Theoretically Audible, but Practically Inaudible Range: frequencies above 18kHz) sound and a software toolkit as its implementation. Using inaudible sound as a data medium, a digital data network among the audience and performer can be easily built with microphones and speakers, without requiring any additional hardware. ``TAPIR Sound Tag" is a smart device framework for inaudible data communication that can be easily embedded in audience participatory performances and interactive arts. With a bandwidth of 900 Hz, a high transmission rate of 200 bps can be achieved, enabling peer-to-peer or broadcasting real-time data communication among smart devices. This system can be used without any advanced knowledge in signal processing and communication system theory; simply specifying carrier frequency and bandwidth with a few lines of code can start data communication. Several usage scenarios of the system are also presented, such as participating in an interactive performance by adding and controlling sound, and collaborative completion of an artist's work by audience. We expect this framework to provide a new way of audience interaction to artists, as well as further promoting audience participation by simplifying the process: using personal smart devices as a medium and not requiring additional hardware or complex settings.},
address = {London, United Kingdom},
author = {Jeon, Jimin and Chae, Gunho and Lee, Edward Jangwon and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {367--370},
publisher = {Goldsmiths, University of London},
title = {{TAPIR Sound Tag: An Enhanced Sonic Communication Framework for Audience Participatory Performance}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}461.pdf},
year = {2014}
}
@inproceedings{tmurray-browne:2014,
abstract = {We introduce Harmonic Motion, a free open source toolkit for artists, musicians and designers working with gestural data. Extracting musically useful features from captured gesture data can be challenging, with projects often requiring bespoke processing techniques developed through iterations of tweaking equations involving a number of constant values -sometimes referred to as `magic numbers'. Harmonic Motion provides a robust interface for rapid prototyping of patches to process gestural data and a framework through which approaches may be encapsulated, reused and shared with others. In addition, we describe our design process in which both personal experience and a survey of potential users informed a set of specific goals for the software.},
address = {London, United Kingdom},
author = {Murray-Browne, Tim and Plumbley, Mark},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {213--216},
publisher = {Goldsmiths, University of London},
title = {{Harmonic Motion: A Toolkit for Processing Gestural Data for Interactive Sound}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}273.pdf},
year = {2014}
}
@inproceedings{Franco:2004,
abstract = { This paper begins by evaluating various systems in terms of factors for building interactive audiovisual environments. The main issues for flexibility and expressiveness in the generation of dynamic sounds and images are then isolated. The design and development of an audiovisual system prototype is described at the end. },
address = {Hamamatsu, Japan},
author = {Franco, Enrique and Griffith, Niall J and Fernstr{\"{o}}m, Mikael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {Audiovisual,composition,expressiveness.,gesture,image,mapping,performance,representation},
pages = {165--168},
title = {{Issues for Designing a Flexible Expressive Audiovisual System for Real-time Performance {\&} Composition}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}165.pdf},
year = {2004}
}
@inproceedings{Poepel2007,
address = {New York City, NY, United States},
author = {Poepel, Cornelius and Marx, G{\"{u}}nter},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {electronic bowed string instrument,evaluation of computer based,interdisciplinary user group,musical instruments,nime07},
pages = {281--284},
title = {{{\textgreater}hot{\_}strings SIG{\textless}}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}281.pdf},
year = {2007}
}
@inproceedings{Marshall2011,
abstract = {This paper deals with the effects of integrated vibrotactile feedback on the "feel" of a digital musical instrument(DMI). Building on previous work developing a DMI withintegrated vibrotactile feedback actuators, we discuss howto produce instrument-like vibrations, compare these simulated vibrations with those produced by an acoustic instrument and examine how the integration of this feedbackeffects performer ratings of the instrument. We found thatintegrated vibrotactile feedback resulted in an increase inperformer engagement with the instrument, but resulted ina reduction in the perceived control of the instrument. Wediscuss these results and their implications for the design ofnew digital musical instruments.},
address = {Oslo, Norway},
author = {Marshall, Mark T and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Marshall, Wanderley - 2011 - Examining the Effects of Embedded Vibrotactile Feedback on the Feel of a Digital Musical Instrument.pdf:pdf},
keywords = {Digital Musical Instruments,Feel,Loudspeakers,Vibrotactile Feedback},
pages = {399--404},
title = {{Examining the Effects of Embedded Vibrotactile Feedback on the Feel of a Digital Musical Instrument}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}399.pdf},
year = {2011}
}
@inproceedings{Hawryshkewich2010,
address = {Sydney, Australia},
author = {Hawryshkewich, Andrew and Pasquier, Philippe and Eigenfeldt, Arne},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {interactive music interface,learning,machine,markov models,midi,nime10,percussion,real-time},
number = {Nime},
pages = {100--105},
title = {{Beatback : A Real-time Interactive Percussion System for Rhythmic Practise and Exploration}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}100.pdf},
year = {2010}
}
@inproceedings{Fraietta2005a,
address = {Vancouver, BC, Canada},
author = {Fraietta, Angelo},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {46--49},
title = {{The Smart Controller Workbench}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}046.pdf},
year = {2005}
}
@inproceedings{Dobrian2003,
address = {Montreal},
author = {Dobrian, Christopher and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
keywords = {Motion capture,gestural control,mapping.},
pages = {161--163},
title = {{Gestural Control of Music Using the Vicon 8 Motion Capture System}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}161.pdf},
year = {2003}
}
@inproceedings{nime2014:tkelkar,
abstract = {We propose a new musical interface, TrAP (TRace-A-Phrase) for generating phrases of Hindustani Classical Music (HCM). In this system the user traces melodic phrases on a tablet interface to create phrases in a raga. We begin by analyzing tracings drawn by 28 participants, and train a classifier to categorize them into one of four melodic categories from the theory of Hindustani Music. Then we create a model based on note transitions from the raga grammar for the notes used in the singable octaves in HCM. Upon being given a new tracing, the system segments the tracing and computes a final phrase that best approximates the tracing.},
address = {London, United Kingdom},
author = {Roy, Udit and Kelkar, Tejaswinee and Indurkhya, Bipin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {243--246},
publisher = {Goldsmiths, University of London},
title = {{TrAP: An Interactive System to Generate Valid Raga Phrases from Sound-Tracings}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}424.pdf},
year = {2014}
}
@inproceedings{Dolphin2009,
abstract = {SpiralSet is a sound toy incorporating game enginesoftware used in conjunction with a spectral synthesissound engine constructed in Max/MSP/Jitter. SpiralSetwas presented as an interactive installation piece at theSonic Arts Expo 2008, in Brighton, UK. A custom madesensor-based interface is used for control of the system.The user interactions are designed to be quickly accessiblein an installation context, yet allowing the potential forsonic depth and variation.},
address = {Pittsburgh, PA, United States},
author = {Dolphin, Andy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Animated Interfaces,Game Engines,Max/MSP.,Open Work,Sound Toys,Spectral Synthesis},
pages = {56--57},
title = {{SpiralSet : A Sound Toy Utilizing Game Engine Technologies}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}056.pdf},
year = {2009}
}
@inproceedings{jfrancoise1:2014,
abstract = {We present a set of probabilistic models that support the design of movement and sound relationships in interactive sonic systems. We focus on a mapping--by--demonstration approach in which the relationships between motion and sound are defined by a machine learning model that learns from a set of user examples. We describe four probabilistic models with complementary characteristics in terms of multimodality and temporality. We illustrate the practical use of each of the four models with a prototype application for sound control built using our Max implementation.},
address = {London, United Kingdom},
author = {Fran{\c{c}}oise, Jules and Schnell, Norbert and Borghesi, Riccardo and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {287--292},
publisher = {Goldsmiths, University of London},
title = {{Probabilistic Models for Designing Motion and Sound Relationships}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}482.pdf},
year = {2014}
}
@inproceedings{Birchfield2006,
address = {Paris, France},
author = {Birchfield, David and Phillips, Kelly and Kidan{\'{e}}, Assegid and Lorig, David},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {arts,embedded electronics,installation art,interactivity,music,network systems,public art,sculpture,sound},
pages = {43--48},
title = {{Interactive Public Sound Art: a case study}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}043.pdf},
year = {2006}
}
@inproceedings{Pritchard2006,
address = {Paris, France},
author = {Pritchard, Bob and Fels, Sidney S},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {gesture,parallel formant speech synthesizer,speech synthesis},
pages = {272--276},
title = {{GRASSP: Gesturally-Realized Audio, Speech and Song Performance}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}272.pdf},
year = {2006}
}
@inproceedings{Mattek2010b,
author = {Mattek, A and Freeman, M and Humphrey, E},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
file = {::},
keywords = {computer-assisted composition,multi-touch interfaces},
number = {Nime},
pages = {479--480},
title = {{Revisiting Cagean Composition Methodology with a Modern Computational Implementation}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}479.pdf},
year = {2010}
}
@inproceedings{Park:2012,
abstract = {This paper describes an interactive gestural microphone for vocal performance named Voicon. Voicon is a non-invasive and gesture-sensitive microphone which allows vocal performers to use natural gestures to create vocal augmentations and modifications by using embedded sensors in a microphone. Through vocal augmentation and modulation, the performers can easily generate desired amount of the vibrato and achieve wider vocal range. These vocal en- hancements will deliberately enrich the vocal performance both in its expressiveness and the dynamics. Using Voicon, singers can generate additional vibrato, control the pitch and activate customizable vocal effect by simple and intuitive gestures in live and recording context.},
address = {Ann Arbor, Michigan},
author = {Park, Yongki and Heo, Hoon and Lee, Kyogu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {Gesture,Microphone,Performance In- terface,Vocal Performance},
publisher = {University of Michigan},
title = {{Voicon: An Interactive Gestural Microphone For Vocal Performance}},
year = {2012}
}
@inproceedings{Schlei2010,
address = {Sydney, Australia},
author = {Schlei, Kevin},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {instrument mapping,multi-,multi-point,multi-touch interface,nime10,point data analysis,trackpad instrument},
number = {June},
pages = {15--18},
title = {{Relationship-Based Instrument Mapping of Multi-Point Data Streams Using a Trackpad Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}136.pdf},
year = {2010}
}
@inproceedings{VonFalkenstein2011,
address = {Oslo, Norway},
author = {von Falkenstein, Jan T},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
keywords = {accelerometer,drawing,gliss,interface,ios,ipad,iphone,music,sequencer,upic},
number = {June},
pages = {527--528},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Gliss : An Intuitive Sequencer for the iPhone and iPad}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}527.pdf},
year = {2011}
}
@inproceedings{Matsumura2005,
address = {Vancouver, BC, Canada},
author = {Matsumura, Seiichiro and Arakawa, Chuichi},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {273},
title = {{Hop Step Junk: Sonic Visualization using Footsteps}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}273.pdf},
year = {2005}
}
@inproceedings{nime2014:kkeatch,
abstract = {Sounds of Solitaire is a novel interface for musical expression based on an extended peg solitaire board as a generator of live musical composition. The classic puzzle game, for one person, is extended by mapping the moves of the game through a self contained system using Arduino and Raspberry Pi, triggering both analogue and digital sound. The solitaire board, as instrument, is presented as a wood and Perspex box with the hardware inside. Ball bearings function as both solitaire pegs and switches, while a purpose built solenoid controlled monochord and ball bearing run provide the analogue sound source, which is digitally manipulated in real-time, according to the sequences of game moves. The creative intention of Sounds of Solitaire is that the playful approach to participation in a musical experience, provided by the material for music making in real-time, demonstrates an integrated approach to concepts of composing, performing and listening.},
address = {London, United Kingdom},
author = {Keatch, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {102--105},
publisher = {Goldsmiths, University of London},
title = {{An Exploration of Peg Solitaire as a Compositional Tool}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}466.pdf},
year = {2014}
}
@inproceedings{Wessel2007,
address = {New York City, NY, United States},
author = {Wessel, David and Avizienis, Rimas and Freed, Adrian and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {high-resolution gestural signals,nime07,pressure and force sensing},
pages = {41--45},
title = {{A Force Sensitive Multi-Touch Array Supporting Multiple {\{}2-D{\}} Musical Control Structures}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}041.pdf},
year = {2007}
}
@inproceedings{Lee:2013a,
abstract = {We introduce a form of networked music performance where a performer plays amobile music instrument while it is being implemented on the fly by a livecoder. This setup poses a set of challenges in performing a music instrumentwhich changes over time and we suggest design guidelines such as making asmooth transition, varying adoption of change, and sharing information betweenthe pair of two performers. A proof-of-concept instrument is implemented on amobile device using UrMus, applying the suggested guidelines. We wish that thismodel would expand the scope of live coding to the distributed interactivesystem, drawing existing performance ideas of NIMEs.},
address = {Daejeon, Republic of Korea},
author = {Lee, Sang Won and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {live coding,mobile music,network music,on-the-fly instrument},
pages = {493--498},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Live Coding The Mobile Music Instrument}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Menzies:2012,
abstract = {The Highland piping tradition requires the performer to learn and accurately reproduce a diverse array of ornaments, which can be a daunting prospect to the novice piper. This paper presents a system which analyses a player's technique using sensor data obtained from an electronic bagpipe chanter interface. Automatic recognition of a broad range of piping embellishments allows real-time visual feedback to be generated, enabling the learner to ensure that they are practicing each movement correctly.
The electronic chanter employs a robust and responsive infrared (IR) sensing strategy, and uses audio samples from acoustic recordings to produce a high quality bagpipe sound. Moreover, the continuous nature of the IR sensors offers the controller a considerable degree of flexibility, indicating sig- nificant potential for the inclusion of extended and novel techniques for musical expression in the future.},
address = {Ann Arbor, Michigan},
author = {Menzies, Duncan and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Great Highland Bagpipe,OSC.,SuperCollider,continuous infrared sensors,ornament recognition,practice tool},
publisher = {University of Michigan},
title = {{An Electronic Bagpipe Chanter for Automatic Recognition of Highland Piping Ornamentation}},
year = {2012}
}
@inproceedings{Deutscher2005,
address = {Vancouver, BC, Canada},
author = {Deutscher, Meghan and Fels, Sidney S and Hoskinson, Reynald and Takahashi, Sachiyo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {274},
title = {{Echology}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}274.pdf},
year = {2005}
}
@inproceedings{Jaimovich2010,
address = {Sydney, Australia},
author = {Jaimovich, Javier},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {body impedance,conductivity,human network,interactive sound installation,nime10,site-specific sound installation,skin},
number = {Nime},
pages = {391--394},
title = {{Ground Me ! An Interactive Sound Art Installation}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}391.pdf},
year = {2010}
}
@inproceedings{Rebelo2006,
address = {Paris, France},
author = {Rebelo, Pedro and Renaud, Alain B},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {audio server,centralized,improvisation,laptop ensemble,networked audio technologies},
pages = {53--56},
title = {{The Frequencyliator  Distributing Structures for Networked Laptop Improvisation}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}053.pdf},
year = {2006}
}
@inproceedings{nime2014:lpardue,
abstract = {This paper presents a multi-modal approach to musical instrument pitch tracking combining audio and position sensor data. Finger location on a violin fingerboard is measured using resistive sensors, allowing rapid detection of approximate pitch. The initial pitch estimate is then used to restrict the search space of an audio pitch tracking algorithm. Most audio-only pitch tracking algorithms face a fundamental tradeoff between accuracy and latency, with longer analysis windows producing better pitch estimates at the cost of noticeable lag in a live performance environment. Conversely, sensor-only strategies struggle to achieve the fine pitch accuracy a human listener would expect. By combining the two approaches, high accuracy and low latency can be simultaneously achieved.},
address = {London, United Kingdom},
author = {Pardue, Laurel and Nian, Dongjuan and Harte, Christopher and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {54--59},
publisher = {Goldsmiths, University of London},
title = {{Low-Latency Audio Pitch Tracking: A Multi-Modal Sensor-Assisted Approach}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}336.pdf},
year = {2014}
}
@inproceedings{Hindman2005,
address = {Vancouver, BC, Canada},
author = {Hindman, David and Kiser, Spencer},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {254--255},
title = {{Sonictroller}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}254.pdf},
year = {2005}
}
@inproceedings{Merrill2003,
abstract = {This paper describes a system which uses the output fromhead-tracking and gesture recognition software to drive aparameterized guitar effects synthesizer in real-time.},
address = {Montreal},
author = {Merrill, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
keywords = {Head-tracking,continuous control,gestural control,parameterized effects processor.},
pages = {218--219},
title = {{Head-Tracking for Gestural and Continuous Control of Parameterized Audio Effects}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}218.pdf},
year = {2003}
}
@inproceedings{Hahnel2010,
address = {Sydney, Australia},
author = {H{\"{a}}hnel, Tilo and Berndt, Axel},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {articulation,expressive performance,historically informed,nime10},
number = {Nime},
pages = {277--282},
title = {{Expressive Articulation for Synthetic Music Performances}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}277.pdf},
year = {2010}
}
@inproceedings{Jenkins:2013,
abstract = {This paper presents a minimally-invasive, wireless optical sensorsystem for use with any conventional piston valve acoustic trumpet. Itis designed to be easy to install and remove by any trumpeter. Ourgoal is to offer the extended control afforded by hyperinstrumentswithout the hard to reverse or irreversible invasive modificationsthat are typically used for adding digital sensing capabilities. Weutilize optical sensors to track the continuous position displacementvalues of the three trumpet valves. These values are trasmittedwirelessly and can be used by an external controller. The hardware hasbeen designed to be reconfigurable by having the housing 3D printed sothat the dimensions can be adjusted for any particular trumpetmodel. The result is a low cost, low power, easily replicable sensorsolution that offers any trumpeter the ability to augment their ownexisting trumpet without compromising the instrument's structure orplaying technique. The extended digital control afforded by our systemis interweaved with the natural playing gestures of an acoustictrumpet. We believe that this seemless integration is critical forenabling effective and musical human computer interaction.Keywords: hyperinstrument, trumpet, minimally-invasive, gesture sensing,wireless, I2C},
address = {Daejeon, Republic of Korea},
author = {Jenkins, Leonardo and Trail, Shawn and Tzanetakis, George and Driessen, Peter and Page, Wyatt},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Jenkins et al. - 2013 - An Easily Removable, wireless Optical Sensing System (EROSS) for the Trumpet.pdf:pdf},
keywords = {I2C,gesture sensing,hyperinstrument,minimally-invasive,trumpet,wireless},
pages = {352--357},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{An Easily Removable, wireless Optical Sensing System (EROSS) for the Trumpet}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Hindman2005,
abstract = {The Sonictroller was originally conceived as a means ofintroducing competition into an improvisatory musicalperformance. By reverse-engineering a popular video gameconsole, we were able to map sound information (volume,pitch, and pitch sequences) to any continuous or momentaryaction of a video game sprite.},
address = {Vancouver, BC, Canada},
author = {Hindman, David and Kiser, Spencer},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Mortal Kombat,Nintendo,controller,guitar,music,sound,trumpet,video game,voice},
pages = {254--255},
title = {{Sonictroller}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}254.pdf},
year = {2005}
}
@inproceedings{Kim:2013,
abstract = {"We discuss how to model ""gestures"" in music performance with statisticallatent-states models. A music performance can be described with compositionaland expressive properties varying over time. In those property changes we oftenobserve particular patterns, and such a pattern can be understood as a""gesture"", since it serves as a medium transferring specific emotions. Assuminga finite number of latent states on each property value changes, we candescribe those gestures with statistical latent-states models, and train themby unsupervised learning algorithms. In addition, model entropy provides us ameasure for different effects of each properties on the gesture implementation.Test result on some of real performances indicates that the trained modelscould capture the structure of gestures observed in the given performances, anddetect their boundaries. The entropy-based measure was informative tounderstand the effectiveness of each property on the gesture implementation.Test result on large corpora indicates that our model has potentials for afurther model improvement."},
address = {Daejeon, Republic of Korea},
author = {Kim, Taehun and Weinzierl, Stefan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Musical gestures,performance analysis,unsupervised machine learning},
pages = {427--430},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Modelling Gestures in Music Performance with Statistical Latent-State Models}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Han2007,
address = {New York City, NY, United States},
author = {Han, Chang Min},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {1,1872-1915,alexander scriabin,historic background,nime07,sonification,synaesthesia,touch screen,was a russian composer,whose},
pages = {388--389},
title = {{Project Scriabin v.3}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}388.pdf},
year = {2007}
}
@inproceedings{nime2014:hportner,
abstract = {The Chimaera is a touch-less, expressive, polyphonic and electronic music controller based on magnetic field sensing. An array of hall-effect sensors and their vicinity make up a continuous 2D interaction space. The sensors are excited with Neodymium magnets worn on fingers. The device continuously tracks position and vicinity of multiple present magnets along the sensor array to produce event signals accordingly. Apart from the two positional signals, an event also carries the magnetic field polarization, a unique identifier and group association. We like to think of it as a mixed analog/digital offspring of theremin and trautonium. These general-purpose event signals are transmitted and eventually translated into musical events according to custom mappings on a host system. With its touch-less control (no friction), high update rates (2-4kHz), its quasi-continuous spatial resolution and its low-latency ({\textless}1 ms), the Chimaera can react to most subtle motions instantaneously and allows for a highly dynamic and expressive play. Its open source design additionally gives the user all possibilities to further tune hardware and firmware to his or her needs. The Chimaera is network-oriented and configured with and communicated by OSC (Open Sound Control), which makes it straight-forward to integrate into any setup.},
address = {London, United Kingdom},
author = {Portner, Hanspeter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {501--504},
publisher = {Goldsmiths, University of London},
title = {{CHIMAERA - The Poly-Magneto-Phonic Theremin - An Expressive Touch-Less Hall-Effect Sensor Array}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}397.pdf},
year = {2014}
}
@inproceedings{Jones2009a,
address = {Pittsburgh, PA, United States},
author = {Jones, Randy and Driessen, Peter and Schloss, Andrew and Tzanetakis, George},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::;::},
keywords = {capacitive,multitouch,nime09,percus-,sensors,tactile},
pages = {236--241},
title = {{A Force-Sensitive Surface for Intimate Control}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}236.pdf},
year = {2009}
}
@inproceedings{Zamorano:2012,
abstract = {This paper introduces Simpletones, an interactive sound system that enables a sense of musical collaboration for non-musicians. Participants can easily create simple sound compositions in real time by collaboratively operating physical artifacts as sound controllers. The physical configuration of the artifacts requires coordinated actions between participants to control sound (thus requiring, and emphasizing collaboration).
Simpletones encourages playful human-to-human interaction by introducing a simple interface and a set of basic rules [1]. This enables novices to focus on the collaborative aspects of making music as a group (such as synchronization and taking collective decisions through non-verbal communication) to ultimately engage a state of group flow[2].
This project is relevant to a contemporary discourse on musical expression because it allows novices to experience the social aspects of group music making, something that is usually reserved only for trained performers [3].},
address = {Ann Arbor, Michigan},
author = {Zamorano, Francisco},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Artifacts,Collaboration,Color Tracking,Computer Vision,State of Flow.},
publisher = {University of Michigan},
title = {{Simpletones: A System of Collaborative Physical Controllers for Novices}},
year = {2012}
}
@inproceedings{Hsu2007,
address = {New York City, NY, United States},
author = {Hsu, William},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {free improvisation,interactive music systems,nime07,timbral analysis},
pages = {367--370},
title = {{Design Issues in Interaction Modeling for Free Improvisation}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}367.pdf},
year = {2007}
}
@inproceedings{Lippit2006a,
author = {Lippit, Takuro Mizuta},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
file = {::},
keywords = {2,aspects of turntable music,dj,improvisation,max,microcontroller,msp,physical computing,pic,turntable music,turntablist},
pages = {71--74},
title = {{Turntable Music in the Digital Era : Designing Alternative Tools for}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}071.pdf},
year = {2006}
}
@inproceedings{Muller2009,
abstract = {Vibetone is a musical input device which was build to explore tactile feedback in gesture based interaction. It is a prototype aimed to allow the performer to play both continuously and discrete pitched sounds in the same space. Our primary focus is on tactile feedback to guide the artist's movements during his performance. Thus, also untrained users are enabled to musical expression through bodily actions and precisely arm movements, guided through tactile feedback signals. },
address = {Pittsburgh, PA, United States},
author = {M{\"{u}}ller, Alexander and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {MIDI controller,gestural interaction,intuitive interaction,tactile feedback},
pages = {33--34},
title = {{Utilizing Tactile Feedback to Guide Movements Between Sounds}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}033.pdf},
year = {2009}
}
@inproceedings{Choi:2013,
abstract = {The advent of Web Audio API in 2011 marked a significant advance for web-basedmusic systems by enabling real-time sound synthesis on web browsers simply bywriting JavaScript code. While this powerful functionality has arrived there isa yet unaddressed need for an extension to the API to fully reveal itspotential. To meet this need, a JavaScript library dubbed WAAX was created tofacilitate music and audio programming based on Web Audio API bypassingunderlying tasks and augmenting useful features. In this paper, we describecommon issues in web audio programming, illustrate how WAAX can speed up thedevelopment, and discuss future developments.},
address = {Daejeon, Republic of Korea},
author = {Choi, Hongchan and Berger, Jonathan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Chrome,JavaScript,Web Audio API,audience participation,collaborative music making,web-based music system},
pages = {499--502},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{WAAX: Web Audio API eXtension}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Fiebrink2009,
address = {Pittsburgh, PA, United States},
author = {Fiebrink, Rebecca and Trueman, Dan and Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {and he does it,any code,audience,in front of an,joe does all of,live,machine learning,mapping,minutes,nime09,on stage during a,performance,this in a few,tools,without writing},
pages = {280--285},
title = {{A Meta-Instrument for Interactive, On-the-Fly Machine Learning}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}280.pdf},
year = {2009}
}
@inproceedings{Hong:2013,
abstract = {Laptap is a laptop-based, real-time sound synthesis/control system for musicand multimedia performance. The system produces unique sounds by positive audiofeedback between the on-board microphone and the speaker of a laptop com-puter. Users can make a variety of sounds by touching the laptop computer inseveral different ways, and control their timbre with the gestures of the otherhand above the mi- crophone and the speaker to manipulate the characteristicsof the acoustic feedback path. We introduce the basic con- cept of this audiofeedback system, describe its features for sound generation and manipulation,and discuss the result of an experimental performance. Finally we suggest somerelevant research topics that might follow in the future.},
address = {Daejeon, Republic of Korea},
author = {Hong, Dae Ryong and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Laptop music,audio feedback,audio visualization,gestural control,hand gesture,laptop computer,musical mapping,musical notation},
pages = {233--236},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Laptap: Laptop Computer as a Musical Instrument using Audio Feedback}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Savary:2012,
abstract = {Dirty Tangible Interfaces (DIRTI) are a new concept in interface design that forgoes the dogma of repeatability in favor of a richer and more complex experience, constantly evolving, never reversible, and infinitely modifiable. We built a prototype based on granular or liquid interaction material placed in a glass dish, that is analyzed by video tracking for its 3D relief. This relief, and the dynamic changes applied to it by the user, are interpreted as activation profiles to drive corpus-based concatenative sound synthesis, allowing one or more players to mold sonic landscapes and to plow through them in an inherently collaborative, expressive, and dynamic experience.},
address = {Ann Arbor, Michigan},
author = {Savary, Matthieu and Schwarz, Diemo and Pellerin, Denis},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Corpus-based concatenative synthesis,Non- standard interaction,Tangible interface},
publisher = {University of Michigan},
title = {{DIRTI --- Dirty Tangible Interfaces}},
year = {2012}
}
@inproceedings{Modler2008,
address = {Genoa, Italy},
author = {Modler, Paul and Myatt, Tony},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {nime08},
pages = {358--359},
title = {{Video Based Recognition of Hand Gestures by Neural Networks for the Control of Sound and Music}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}358.pdf},
year = {2008}
}
@inproceedings{Rothman2010,
abstract = {The Ghost has been developed to create a merger between the standard MIDI keyboard controller, MIDI/digital guitars and alternative desktop controllers. Using a custom software editor, The Ghost's controls can be mapped to suit the users performative needs. The interface takes its interaction and gestural cues from the guitar but it is not a MIDI guitar. The Ghost's hardware, firmware and software will be open sourced with the hopes of creating a community of users that are invested in creating music with controller.},
address = {Sydney, Australia},
author = {Rothman, Paul},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Controller,Live Performance,MIDI,Open- Source,Programmable},
pages = {431--435},
title = {{The Ghost : An Open-Source, User Programmable MIDI Performance Controller}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}431.pdf},
year = {2010}
}
@inproceedings{Bergsland:2012,
abstract = {As a part of the research project Voice Meetings, a solo live- electronic vocal performance was presented for 63 students. Through a mixed method approach applying both written and oral response, feedback from one blindfolded and one seeing audience group was collected and analyzed.
There were marked differences between the groups regarding focus, in that the participants in blindfolded group tended to focus on fewer aspects, have a heightened focus and be less distracted than the seeing group. The seeing group, on its part, focused more on the technological instruments applied in the performance, the performer herself and her actions. This study also shows that there were only minor differences between the groups regarding the experience of skill and control, and argues that this observation can be explained by earlier research on skill in NIMEs.},
address = {Ann Arbor, Michigan},
author = {Bergsland, Andreas and {\AA}se, Tone},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Performance,acousmatic listening,audience reception,live-electronics,qualitative research,voice},
publisher = {University of Michigan},
title = {{Using a seeing/blindfolded paradigm to study audience experiences of live-electronic performances with voice}},
year = {2012}
}
@inproceedings{Fyfe2010,
address = {Sydney, Australia},
author = {Fyfe, Lawrence and Lynch, Sean and Hull, Carmen and Carpendale, Sheelagh},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {gesture,multi-touch,nime10,open sound,physical model,tabletop},
number = {Nime},
pages = {360--363},
title = {{SurfaceMusic : Mapping Virtual Touch-based Instruments to Physical Models}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}360.pdf},
year = {2010}
}
@inproceedings{Honigman:2013,
abstract = {This paper describes a new framework for music creation using 3D audio andvisual techniques. It describes the Third Room, which uses a Kinect to placeusers in a virtual environment to interact with new instruments for musicalexpression. Users can also interact with smart objects, including the Ember(modified mbira digital interface) and the Fluid (a wireless six degrees offreedom and touch controller). This project also includes new techniques for 3Daudio connected to a 3D virtual space using multi-channel speakers anddistributed robotic instruments.},
address = {Daejeon, Republic of Korea},
author = {Honigman, Colin and Walton, Andrew and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Arduino,Interface,Kinect Camera,Natural Interaction,Robotics,Third Space,Virtual Reality},
pages = {29--34},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{The Third Room: A 3D Virtual Music Framework}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Ramakrishnan2006,
address = {Paris, France},
author = {Ramakrishnan, Chandrasekhar and Go{\ss}man, Joachim and Br{\"{u}}mmer, Ludger},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
keywords = {acousmatic music,ambisonics,panning,sound spatialization,vbap,vector based additive,wave field synthesis},
pages = {140--143},
title = {{The ZKM Klangdom}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}140.pdf},
year = {2006}
}
@inproceedings{Choe2011,
address = {Oslo, Norway},
author = {Choe, Souhwan and Lee, Kyogu},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {1 overview of swaf,2,map,sound,sound archive,soundscape,soundscape composition,soundscape documentation,web application framework},
number = {June},
pages = {533--534},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{SWAF: Towards a Web Application Framework for Composition and Documentation of Soundscape}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}533.pdf},
year = {2011}
}
@inproceedings{Camurri2007,
abstract = {In this paper, we present a new system, the Orchestra Explorer, enabling a novel paradigm for active fruition of sound and music content. The Orchestra Explorer allows users to physically navigate inside a virtual orchestra, to actively explore the music piece the orchestra is playing, to modify and mold the sound and music content in real-time through their expressive full-body movement and gesture. An implementation of the Orchestra Explorer was developed and presented in the framework of the science exhibition "Cimenti di Invenzione e Armonia", held at Casa Paganini, Genova, from October 2006 to January 2007. },
address = {New York City, NY, United States},
author = {Camurri, Antonio and Canepa, Corrado and Volpe, Gualtiero},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Active listening of music,expressive interfaces,full-body motion analysis and expressive gesture,multimodal interactive systems for music and perf},
pages = {56--61},
title = {{Active Listening to a Virtual Orchestra Through an Expressive Gestural Interface : The Orchestra Explorer}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}056.pdf},
year = {2007}
}
@inproceedings{Hsu2009,
address = {Pittsburgh, PA, United States},
author = {Hsu, William and Sosnick, Marc},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {evaluation,evaluation tests,human computer,interaction,interactive music systems,nime09},
mendeley-tags = {evaluation},
pages = {25--28},
title = {{Evaluating Interactive Music Systems : An HCI Approach}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}025.pdf},
year = {2009}
}
@inproceedings{Bouillot2007,
abstract = {Remote real-time musical interaction is a domain where endto-end latency is a well known problem. Today, the mainexplored approach aims to keep it below the musicians perception threshold. In this paper, we explore another approach, where end-to-end delays rise to several seconds, butcomputed in a controlled (and synchronized) way dependingon the structure of the musical pieces. Thanks to our fullydistributed prototype called nJam, we perform user experiments to show how this new kind of interactivity breaks theactual end-to-end latency bounds.},
address = {New York City, NY, United States},
author = {Bouillot, Nicolas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {NMP.,Remote real-time musical interaction,distributed metronome,end-to-end delays,syn- chronization,user experiments},
pages = {142--147},
title = {{nJam User Experiments : Enabling Remote Musical Interaction from Milliseconds to Seconds}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}142.pdf},
year = {2007}
}
@inproceedings{Gillian2011a,
address = {Oslo, Norway},
author = {Gillian, Nicholas and Knapp, Benjamin and O'Modhrain, Sile},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {gesture recognition,machine learning,musician-computer},
number = {June},
pages = {343--348},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{A Machine Learning Toolbox For Musician Computer Interaction}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}343.pdf},
year = {2011}
}
@inproceedings{Newton2011,
abstract = {This paper examines the creation of augmented musicalinstruments by a number of musicians. Equipped with asystem called the Augmentalist, 10 musicians created newaugmented instruments based on their traditional acousticor electric instruments. This paper discusses the ways inwhich the musicians augmented their instruments, examines the similarities and differences between the resultinginstruments and presents a number of interesting findingsresulting from this process.},
address = {Oslo, Norway},
author = {Newton, Dan and Marshall, Mark T},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Newton, Marshall - 2011 - Examining How Musicians Create Augmented Musical Instruments.pdf:pdf},
keywords = {Augmented Instruments,Digital Musical Instruments,Instrument Design,Performance},
pages = {155--160},
title = {{Examining How Musicians Create Augmented Musical Instruments}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}155.pdf},
year = {2011}
}
@inproceedings{Marier2010b,
author = {Marier, Martin},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
file = {::},
keywords = {electroacoustic music,expressivity,interface,nime10,performance},
number = {Nime},
pages = {356--359},
title = {{The Sponge: A Flexible Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}356.pdf},
year = {2010}
}
@inproceedings{Nagashima2003,
address = {Montreal},
author = {Nagashima, Yoichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
pages = {48--53},
title = {{Bio-Sensing Systems and Bio-Feedback Systems for Interactive Media Arts}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}048.pdf},
year = {2003}
}
@inproceedings{Ng2008,
address = {Genoa, Italy},
author = {Ng, Kia and Nesi, Paolo},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {augmented instrument,education,gesture,interactive,interface,motion,multimedia,music,nime08,notation,sensor,sonification,technology-enhanced learning,visualisation},
pages = {225--228},
title = {{i-Maestro : Technology-Enhanced Learning and Teaching for Music}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}225.pdf},
year = {2008}
}
@inproceedings{Greenlee:2013,
abstract = {In the design of recent systems, I have advanced techniques that positiongraphic synthesis methods in the context of solo, improvisational performance.Here, the primary interfaces for musical action are prepared works on paper,scanned by digital video cameras which in turn pass image data on to softwarefor analysis and interpretation as sound synthesis and signal processingprocedures. The focus of this paper is on one of these techniques, a process Idescribe as graphic waveshaping. A discussion of graphic waveshaping in basicform and as utilized in my performance work, (title omitted), is offered. Inthe latter case, the performer's objective is to guide the interpretation ofimages as sound, constantly tuning and retuning the conversion while selectingand scanning images from a large catalog. Due to the erratic nature of thesystem and the precondition that image to sound relationships are unfixed, theperformance situation is replete with the discovery of new sounds and thecircumstances that bring them into play. Graphic waveshaping may be understood as non-linear distortion synthesis withtime-varying transfer functions stemming from visual scan lines. As a form ofgraphic synthesis, visual images function as motivations for sound generation.There is a strategy applied for creating one out of the other. However, counterto compositionally oriented forms of graphic synthesis where one may assignimage characteristics to musical parameters such as pitches, durations,dynamics, etc., graphic waveshaping is foremost a processing technique, as itdistorts incoming signals according to graphically derived transfer functions.As such, it may also be understood as an audio effect; one that in myimplementations is particularly feedback dependent, oriented towards shapingthe erratic behavior of synthesis patches written in Max/MSP/Jitter. Used inthis manner, graphic waveshaping elicits an emergent system behaviorconditioned by visual features.},
address = {Daejeon, Republic of Korea},
author = {Greenlee, Shawn},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Graphic waveshaping,drawn sound,graphic sound,graphic synthesis,waveshaping synthesis},
pages = {287--290},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Graphic Waveshaping}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Jacquemin2006,
address = {Paris, France},
author = {Jacquemin, Christian and de Laubier, Serge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {audio-visual composition,feedback,transmodality},
pages = {156--161},
title = {{Transmodal Feedback as a New Perspective for Audio-visual Effects}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}156.pdf},
year = {2006}
}
@inproceedings{FASCIANI:2012,
abstract = {Sound generators and synthesis engines expose a large set of parameters, allowing run-time timbre morphing and exploration of sonic space. However, control over these high-dimensional interfaces is constrained by the physical limitations of performers. In this paper we propose the exploitation of vocal gesture as an extension or alternative to traditional physical controllers. The approach uses dynamic aspects of vocal sound to control variations in the timbre of the synthesized sound. The mapping from vocal to synthesis parameters is automatically adapted to information extracted from vocal examples as well as to the relationship between parameters and timbre within the synthesizer. The mapping strategy aims to maximize the breadth of the explorable perceptual sonic space over a set of the synthesizer's real-valued parameters, indirectly driven by the voice-controlled interface.},
address = {Ann Arbor, Michigan},
author = {FASCIANI, STEFANO and WYSE, LONCE},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
publisher = {University of Michigan},
title = {{A Voice Interface for Sound Generators: adaptive and automatic mapping of gestures to sound}},
year = {2012}
}
@inproceedings{Melo2005,
abstract = {The Swayway is an audio/MIDI device inspired by the simpleconcept of the wind chime.This interactive sculpture translates its swaying motion,triggered by the user, into sound and light. Additionally, themotion of the reeds contributes to the visual aspect of thepiece, converting the whole into a sensory and engagingexperience.},
address = {Vancouver, BC, Canada},
author = {Melo, Mauricio and Fan, Doria},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Interactive sound sculpture,LEDs,flex sensors,midi chimes,sound installation.},
pages = {262--263},
title = {{Swayway - Midi Chimes}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}262.pdf},
year = {2005}
}
@inproceedings{Thorogood2013,
abstract = {Soundscape composition in improvisation and performance contexts involves manyprocesses that can become over- whelming for a performer, impacting on thequality of the composition. One important task is evaluating the mood of acomposition for evoking accurate associations and mem- ories of a soundscape. Anew system that uses supervised machine learning is presented for theacquisition and re- altime feedback of soundscape affect. A model of sound-scape mood is created by users entering evaluations of au- dio environmentsusing a mobile device. The same device then provides feedback to the user ofthe predicted mood of other audio environments. We used a features vector ofTotal Loudness and MFCC extracted from an audio signal to build a multipleregression models. The evaluation of the system shows the tool is effective inpredicting soundscape affect.},
address = {Daejeon},
author = {Thorogood, Miles and Pasquier, Philippe},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {affect grid,audio features,machine learning,performance,soundscape},
pages = {256--260},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Impress: A Machine Learning Approach to Soundscape Affect Classification for a Music Performance Environment}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Kartadinata2006,
address = {Paris, France},
author = {Kartadinata, Sukandar},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {actuators,digital sensors,fpga,osc,sensor interfaces},
pages = {93--96},
title = {{The Gluion Advantages of an FPGA-based Sensor Interface}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}093.pdf},
year = {2006}
}
@inproceedings{Crevoisier2011,
abstract = {The Surface Editor is a software tool for creating control interfaces and mapping input actions to OSC or MIDI actions very easily and intuitively. Originally conceived to be used with a tactile interface, the Surface Editor has been extended to support the creation of graspable interfaces as well. This paper presents a new framework for the generic mapping of user actions with graspable objects on a surface. We also present a system for detecting touch on thin objects, allowing for extended interactive possibilities. The Surface Editor is not limited to a particular tracking system though, and the generic mapping approach for objects can have a broader use with various input interfaces supporting touch and/or objects. },
address = {Oslo, Norway},
author = {Crevoisier, Alain and Picard-Limpens, C{\'{e}}cile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {NIME,graspable interfaces.,interaction,mapping,tangibles,user-defined interfaces},
pages = {236--239},
title = {{Mapping Objects with the Surface Editor}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}236.pdf},
year = {2011}
}
@inproceedings{BryanKinns2010,
address = {Sydney, Australia},
author = {Bryan-Kinns, Nick and Fencott, Robin and Metatla, Oussama and Nabavian, Shahin and Sheridan, Jennifer G},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {awareness,collaboration,composition,improvisation,interactional,music,mutual engagement,nime10,sound},
number = {Nime},
pages = {403--406},
title = {{Interactional Sound and Music : Listening to CSCW, Sonification, and Sound Art}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}403.pdf},
year = {2010}
}
@inproceedings{Fyans2010,
address = {Sydney, Australia},
author = {Fyans, A Cavan and Gurevich, Michael and Stapleton, Paul},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {error,intention,mental model,nime10,qualitative,spectator},
number = {Nime},
pages = {451--454},
title = {{Examining the Spectator Experience}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}451.pdf},
year = {2010}
}
@inproceedings{Hoste:2013,
abstract = {Nowadays many music artists rely on visualisations and light shows to enhanceand augment their live performances. However, the visualisation and triggeringof lights is normally scripted in advance and synchronised with the concert,severely limiting the artist's freedom for improvisation, expression and ad-hocadaptation of their show. These scripts result in performances where thetechnology enforces the artist and their music to stay in synchronisation withthe pre-programmed environment. We argue that these limitations can be overcomebased on emerging non-invasive tracking technologies in combination with anadvanced gesture recognition engine.We present a solution that uses explicit gestures and implicit dance moves tocontrol the visual augmentation of a live music performance. We furtherillustrate how our framework overcomes existing limitations of gestureclassification systems by delivering a precise recognition solution based on asingle gesture sample in combination with expert knowledge. The presentedsolution enables a more dynamic and spontaneous performance and, when combinedwith indirect augmented reality, results in a more intense interaction betweenthe artist and their audience.},
address = {Daejeon, Republic of Korea},
author = {Hoste, Lode and Signer, Beat},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {3D gesture recognition,Expressive control,Kinect,augmented reality,declarative language,live music performance},
pages = {13--18},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Expressive Control of Indirect Augmented Reality During Live Music Performances}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Johnston:2013,
abstract = {This paper describes an audio-visual performance system based on real-timefluid simulation. The aim is to provide a rich environment for works whichblur the boundaries between dance and instrumental performance -- and sound andvisuals -- while maintaining transparency for audiences and new performers. The system uses infra-red motion tracking to allow performers to manipulate areal-time fluid simulation, which in turn provides control data forcomputer-generated audio and visuals. It also provides a control andconfiguration system which allows the behaviour of the interactive system to bechanged over time, enabling the structure within which interactions take placeto be `composed'.},
address = {Daejeon, Republic of Korea},
author = {Johnston, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {composition,dance,fluid simulation,performance},
pages = {132--135},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Fluid Simulation as Full Body Audio-Visual Instrument}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Feng2010,
address = {Sydney, Australia},
author = {Feng, Xiaoyang},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {acoustic,arduino,figure 2,front view of irregular,incurve,interactive,midi,motor control,nime,nime10,performance,real time,robotics,servo,string instrument},
number = {Nime},
pages = {377--379},
title = {{Irregular Incurve}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}377.pdf},
year = {2010}
}
@inproceedings{Roberts:2012,
address = {Ann Arbor, Michigan},
annote = {Designing mobile interfaces for computer-based musical performance is generally a time-consuming task that can be exasperating for performers. Instead of being able to experiment freely with physical interfaces{\&}{\#}039; affordances, performers must spend time and attention on non-musical tasks including network configuration, development environments for the mobile devices, defining OSC address spaces, and handling the receipt of OSC in the environment that will control and produce sound. Our research seeks to overcome such obstacles by minimizing the code needed to both generate and read the output of interfaces on mobile devices. For iOS and Android devices, our implementation extends the application Control to use a simple set of OSC messages to define interfaces and automatically route output. On the desktop, our implementations in Max/MSP/Jitter, LuaAV, and Su- perCollider allow users to create mobile widgets mapped to sonic parameters with a single line of code. We believe the fluidity of our approach will encourage users to incorporate mobile devices into their everyday performance practice.},
author = {Roberts, Charles and Wakefield, Graham and Wright, Matt},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Android,LuaAV,Max/MSP/Jitter,Mobile,NIME,OSC,SuperCollider,Zeroconf,iOS},
publisher = {University of Michigan},
title = {{Mobile Controls On-The-Fly: An Abstraction for Distributed NIMEs}},
year = {2012}
}
@inproceedings{Singer2004,
address = {Hamamatsu, Japan},
author = {Singer, Eric and Feddersen, Jeff and Redmon, Chad and Bowen, Bil},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {additional computer or special,commands allows,familiar tools with no,improvisations,the musician or composer,to control the instrument,use of standard midi,using},
pages = {181--184},
title = {{LEMUR's Musical Robots}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}181.pdf},
year = {2004}
}
@inproceedings{Yang:2012,
abstract = {We augment the piano keyboard with a 3D gesture space using Microsoft Kinect for sensing and top-down projection for visual feedback. This interface provides multi-axial gesture controls to enable continuous adjustments to multiple acoustic parameters such as those on the typical digital synthesizers. We believe that using gesture control is more visceral and aesthetically pleasing, especially during concert performance where the visibility of the performer's action is important. Our system can also be used for other types of gesture interaction as well as for pedagogical applications.},
address = {Ann Arbor, Michigan},
author = {Yang, Qi and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {NIME,depth camera,gesture,musical instrument,piano,tabletop projection},
publisher = {University of Michigan},
title = {{Augmented Piano Performance using a Depth Camera}},
year = {2012}
}
@inproceedings{nime2014:ldahl,
abstract = {Motion sensing technologies enable musical interfaces where a performer moves their body "in the air" without manipulating or contacting a physical object. These interfaces work well when the movement and sound are smooth and continuous, but it has proven difficult to design a system which triggers discrete sounds with precision that allows for complex rhythmic performance. We conducted a study where participants perform ``air-drumming" gestures in time to rhythmic sounds. These movements are recorded, and the timing of various movement features with respect to the onset of audio events is analyzed. A novel algorithm for detecting sudden changes in direction is used to find the end of the strike gesture. We find that these occur on average after the audio onset and that this timing varies with the tempo of the movement. Sharp peaks in magnitude acceleration occur before the audio onset and do not vary with tempo. These results suggest that detecting peaks in acceleration will lead to more naturally responsive air gesture instruments.},
address = {London, United Kingdom},
author = {Dahl, Luke},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {201--206},
publisher = {Goldsmiths, University of London},
title = {{Triggering Sounds from Discrete Air Gestures: What Movement Feature Has the Best Timing?}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}514.pdf},
year = {2014}
}
@inproceedings{Nymoen2011,
address = {Oslo, Norway},
author = {Nymoen, Kristian and Skogstad, St{\aa}le A and Jensenius, Alexander R},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
number = {June},
pages = {312--315},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{SoundSaber - A Motion Capture Instrument}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}312.pdf},
year = {2011}
}
@inproceedings{Lyon2009,
address = {Pittsburgh, PA, United States},
author = {Lyon, Eric and Knapp, Benjamin and Ouzounian, Gascia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {327},
title = {{Biomuse Trio}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}327.pdf},
year = {2009}
}
@inproceedings{Cook2005,
address = {Vancouver, BC, Canada},
author = {Cook, Perry R},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {236--237},
title = {{Real-Time Performance Controllers for Synthesized Singing}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}236.pdf},
year = {2005}
}
@inproceedings{OModhrain2004,
address = {Hamamatsu, Japan},
author = {O'Modhrain, Sile and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {granular synthesis,haptic,musical instrument},
pages = {74--79},
title = {{PebbleBox and CrumbleBag: Tactile Interfaces for Granular Synthesis}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}074.pdf},
year = {2004}
}
@inproceedings{Baalman2009a,
address = {Pittsburgh, PA, United States},
author = {Baalman, Marije A and Smoak, Harry C and Salter, Christopher L and Malloch, Joseph and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {1,collaborative performance,data exchange,in-,interactive art works,introduction and background,max,msp,nime09,open-,sensor data,soundcontrol,supercollider,teractive performance},
pages = {131--134},
title = {{Sharing Data in Collaborative, Interactive Performances : the SenseWorld DataNetwork}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}131.pdf},
year = {2009}
}
@inproceedings{Traube2003,
address = {Montreal, QC, Canada},
author = {Traube, Caroline and Depalle, Philippe and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Traube, Depalle, Wanderley - 2003 - Indirect Acquisition of Instrumental Gesture Based on Signal , Physical and Perceptual Information.pdf:pdf},
pages = {42--47},
title = {{Indirect Acquisition of Instrumental Gesture Based on Signal , Physical and Perceptual Information}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}042.pdf},
year = {2003}
}
@inproceedings{Schacher2009,
abstract = {In this paper mappings and adaptation in the context of interactive sound installations are discussed. Starting from an ecological perspective on non-expert audience interaction a brief overview and discussion of mapping strategies with a special focus on adaptive systems using machine learning algorithms is given. An audio-visual interactive installation is analyzed and its implementation used to illustrate the issues of audience engagement and to discuss the efficiency of adaptive mappings. },
address = {Pittsburgh, PA, United States},
author = {Schacher, Jan C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Interaction,adaptive mapping,audience engagement,machine learning},
pages = {286--289},
title = {{Action and Perception in Interactive Sound Installations : An Ecological Approach}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}286.pdf},
year = {2009}
}
@inproceedings{Kim2007,
address = {New York City, NY, United States},
author = {Kim, Juno and Schiemer, Greg and Narushima, Terumi},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {1,algorithmic composition,expressive control interfaces,eye movement recording,microtonal tuning,midi,nime07,pure data,video},
pages = {50--55},
title = {{Oculog : Playing with Eye Movements}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}050.pdf},
year = {2007}
}
@inproceedings{Kobayashi2006,
address = {Paris, France},
author = {Kobayashi, Shigeru and Endo, Takanori and Harada, Katsuhiko and Oishi, Shosei},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {learning,rapid prototyping,reconfigurable,sensor interface},
pages = {346--351},
title = {{GAINER: A Reconfigurable I/O Module and Software Libraries for Education}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}346.pdf},
year = {2006}
}
@inproceedings{Couturier:2002,
abstract = {This paper describes a virtual musical instrument based on the scanned synthesis technique and implemented in Max-Msp. The device is composed of a computer and three gesture sensors. The timbre of the produced sound is rich and changing. The instrument proposes an intuitive and expressive control of the sound thanks to a complex mapping between gesture and sound. },
address = {Dublin},
author = {Couturier, Jean-Michel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {graphics tablet,meta-parameters,multi-touch tactile,scanned synthesis,surface},
pages = {43--45},
title = {{A Scanned Synthesis Virtual Instrument}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}043.pdf},
year = {2002}
}
@inproceedings{Hattwick:2012,
abstract = {The configurability and networking abilities of digital musical instruments increases the possibilities for collaboration in musical performances. Computer music ensembles such as laptop orchestras are becoming increasingly common and provide laboratories for the exploration of these possibilities. However, much of the literature regarding the creation of DMIs has been focused on individual expressivity, and their potential for collaborative performance has been under-utilized. This paper makes the case for the benefits of an approach to digital musical instrument design that begins with their collaborative potential, examines several frameworks and sets of principles for the creation of digital musical instruments, and proposes a dimension space representation of collaborative approaches which can be used to evaluate and guide future DMI creation. Several examples of DMIs and compositions are then evaluated and discussed in the context of this dimension space.},
address = {Ann Arbor, Michigan},
author = {Hattwick, Ian and Wanderley, Marcelo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {collaborative,digital music ensemble,digital musical instrument,dimension space,dme,dmi},
month = {may},
publisher = {University of Michigan},
title = {{A Dimension Space for Evaluating Collaborative Musical Performance Systems}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}150.pdf},
year = {2012}
}
@inproceedings{Wilkerson2002,
address = {Dublin, Ireland},
author = {Wilkerson, Carr and Serafin, Stefania and Ng, Carmen},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {called,chenier,cots sont pas sale,crowned the,king of zydeco,les hari-,referring to hard times,salted,t,t even enough,the snap beans aren,when there wasn,which means},
pages = {195--198},
title = {{The Mutha Rubboard Controller}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}195.pdf},
year = {2002}
}
@inproceedings{Oore2005,
abstract = {When learning a classical instrument, people often eithertake lessons in which an existing body of "technique" is delivered, evolved over generations of performers, or in somecases people will "teach themselves" by watching people playand listening to existing recordings. What does one do witha complex new digital instrument?In this paper I address this question drawing on my experience in learning several very different types of sophisticatedinstruments: the Glove Talk II real-time gesture-to-speechinterface, the Digital Marionette controller for virtual 3Dpuppets, and pianos and keyboards. As the primary userof the first two systems, I have spent hundreds of hourswith Digital Marionette and Glove-Talk II, and thousandsof hours with pianos and keyboards (I continue to work asa professional musician). I will identify some of the underlying principles and approaches that I have observed duringmy learning and playing experience common to these instruments. While typical accounts of users learning new interfaces generally focus on reporting beginner's experiences, forvarious practical reasons, this is fundamentally different byfocusing on the expert's learning experience.},
address = {Vancouver, BC, Canada},
author = {Oore, Sageev},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {learning new instruments,performance},
pages = {60--64},
title = {{Learning Advanced Skills on New Instruments}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}060.pdf},
year = {2005}
}
@inproceedings{Schmeder2008,
abstract = {A general-purpose firmware for a low cost microcontroller is described that employs the Open Sound Control protocol over USB. The firmware is designed with considerations for integration in new musical interfaces and embedded devices. Features of note include stateless design, efficient floating-point support, temporally correct data handling, and protocol completeness. A timing performance analysis is conducted.},
address = {Genoa, Italy},
author = {Schmeder, Andrew and Freed, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Schmeder, Freed - 2008 - uOSC The Open Sound Control Reference Platform for Embedded Devices.pdf:pdf},
keywords = {jitter,latency,nime08,open sound control,pic microcontroller,usb},
pages = {175--180},
title = {{uOSC : The Open Sound Control Reference Platform for Embedded Devices}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}175.pdf},
year = {2008}
}
@inproceedings{nime2014:pdahlstedt,
abstract = {Two related versions of an unstable live algorithm for the Disklavier player piano are presented. The underlying gen- erative feedback system consists of four virtual musicians, listening to each other in a circular configuration. There is no temporal form, and all parameters of the system are controlled by the performer through an intricate but direct mapping, in an attempt to combine the experienced mu- sician's physical control of gesture and phrasing, with the structural complexities and richness of generative music. In the first version, Circle Squared, the interface is an array of pressure sensors, and the performer performs on the sys- tem without participating directly, like a puppet master. In the second version, control parameters are derived directly from playing on the same piano that performs the output of the system. Here, the performer both plays with and on the system in an intricate dance with the unpredictable output of the unstable virtual ensemble. The underlying mapping strategies are presented, together with the struc- ture of the generative system. Experiences from a series of performances are discussed, primarily from the perspective of the improvising musician.},
address = {London, United Kingdom},
author = {Dahlstedt, Palle},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {114--117},
publisher = {Goldsmiths, University of London},
title = {{Circle Squared and Circle Keys - Performing on and with an Unstable Live Algorithm for the Disklavier}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}534.pdf},
year = {2014}
}
@inproceedings{Wozniewski2008,
address = {Genoa, Italy},
author = {Wozniewski, Mike and Bouillot, Nicolas and Settel, Zack and Cooperstock, Jeremy R},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {audio streaming,collaborative interfaces,locative media,mobile music,nime08,sonic navigation,spatial interaction,wireless},
pages = {13--18},
title = {{Large-Scale Mobile Audio Environments for Collaborative Musical Interaction}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}013.pdf},
year = {2008}
}
@inproceedings{Friberg2011,
address = {Oslo, Norway},
author = {Friberg, Anders and K{\"{a}}llblad, Anna},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {choreography,dance,gestures,interactive,music installation},
number = {June},
pages = {128--131},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Experiences from Video-Controlled Sound Installations}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}128.pdf},
year = {2011}
}
@inproceedings{Raisanen2008,
abstract = {This paper describes the Sormina, a new virtual and tangibleinstrument, which has its origins in both virtual technology andthe heritage of traditional instrument design. The motivationbehind the project is presented, as well as hardware andsoftware design. Insights gained through collaboration withacoustic musicians are presented, as well as comparison tohistorical instrument design.},
address = {Genoa, Italy},
author = {R{\"{a}}is{\"{a}}nen, Juhani},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Gestural controller,design.,digital musical instrument,music history,usability},
pages = {57--60},
title = {{Sormina -- a New Virtual and Tangible Instrument}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}057.pdf},
year = {2008}
}
@inproceedings{Francois2006,
address = {Paris, France},
author = {Francois, Alexander R and Chew, Elaine},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Interactive Systems,Music soft- ware,Software Architecture},
pages = {150--155},
title = {{An Architectural Framework for Interactive Music Systems}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}150.pdf},
year = {2006}
}
@inproceedings{Kapur2007,
abstract = {This paper describes a system enabling a human to perform music with a robot in real-time, in the context of North Indian classical music. We modify a traditional acoustic sitar into a hyperinstrument in order to capture performance gestures for musical analysis. A custom built four-armed robotic Indian drummer was built using a microchip, solenoids, aluminum and folk frame drums. Algorithms written towards "intelligent" machine musicianship are described. The final goal of this research is to have a robotic drummer accompany a professional human sitar player live in performance. },
address = {New York City, NY, United States},
author = {Kapur, Ajay and Singer, Eric and Benning, Manjinder S and Tzanetakis, George and Trimpin, Trimpin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Electronic Sitar,Hyperinstruments,Music Information Retrieval (MIR).,Musical Robotics},
pages = {238--241},
title = {{Integrating HyperInstruments , Musical Robots {\&} Machine Musicianship for North {\{}India{\}}n Classical Music}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}238.pdf},
year = {2007}
}
@inproceedings{Remus2006,
address = {Paris, France},
author = {R{\'{e}}mus, Jacques},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {camera musicale,interface,jacques r{\'{e}}mus,machines,musical camera,musical hand,non haptic instrument,s mappings,sculptures and mechanical musical,sound},
pages = {250--253},
title = {{Non Haptic Control of Music by Video Analysis of Hand Movements: 14 Years of Experience with the {\{}$\backslash$guillemotleft{\}}Cam{\'{e}}ra Musicale{\{}$\backslash$guillemotright{\}}}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}250.pdf},
year = {2006}
}
@inproceedings{Hewitt2003,
address = {Montreal, QC, Canada},
author = {Hewitt, Donna and Stevenson, Ian},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Hewitt, Stevenson - 2003 - E-mic Extended Mic-stand Interface Controller.pdf:pdf},
pages = {122--128},
title = {{E-mic: Extended Mic-stand Interface Controller}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}122.pdf},
year = {2003}
}
@inproceedings{Rodrigues2005,
abstract = {We present our work in the development of an interface for an actor/singer and its use in performing. Our work combines aspects of theatrical music with technology. Our interface has allowed the development of a new vocabulary for musical and theatrical expression and the possibility for merging classical and experimental music. It gave rise to a strong, strange, unpredictable, yet coherent, "character" and opens up the possibility for a full performance that will explore aspects of voice, theatrical music and, in the future, image projection. },
address = {Vancouver, BC, Canada},
author = {Rodrigues, Paulo Maria and Gir{\~{a}}o, Luis Miguel and Gehlhaar, Rolf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Theatrical music,computer interaction,gestural control.,voice},
pages = {164--167},
title = {{CyberSong}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}164.pdf},
year = {2005}
}
@inproceedings{Huott2005,
address = {Vancouver, BC, Canada},
author = {Huott, Robert},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Musical controller,mapping system,sensate surface},
pages = {244--245},
title = {{Precise Control on Compound Curves}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}244.pdf},
year = {2005}
}
@inproceedings{Mital:2013,
abstract = {We present an interactive content-based MIR environment specifically designedto aid in the exploration of databases of experimental electronic music,particularly in cases where little or no metadata exist. In recent years,several rare archives of early experimental electronic music have becomeavailable. The Daphne Oram Collection contains one such archive, consisting ofapproximately 120 hours of 1/4 inch tape recordings and representing a perioddating from circa 1957. This collection is recognized as an importantmusicological resource, representing aspects of the evolution of electronicmusic practices, including early tape editing methods, experimental synthesistechniques and composition. However, it is extremely challenging to derivemeaningful information from this dataset, primarily for three reasons. First,the dataset is very large. Second, there is limited metadata - some titles,track lists, and occasional handwritten notes exist, but where this is true,the reliability of the annotations are unknown. Finally, and mostsignificantly, as this is a collection of early experimental electronic music,the sonic characteristics of the material are often not consistent withtraditional musical information. In other words, there is no score, no knowninstrumentation, and often no recognizable acoustic source. We present amethod for the construction of a frequency component dictionary derived fromthe collection via Probabilistic Latent Component Analysis (PLCA), anddemonstrate how an interactive 3D visualization of the relationships betweenthe PLCA-derived dictionary and the archive is facilitating researcher'sunderstanding of the data.},
address = {Daejeon, Republic of Korea},
author = {Mital, Parag Kumar and Grierson, Mick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {3d browser,content-based information retrieval,daphne oram,interactive visualization,mfcc,mir,plca},
pages = {227--232},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Mining Unlabeled Electronic Music Databases through 3D Interactive Visualization of Latent Component Relationships}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Heinz2010,
address = {Sydney, Australia},
author = {Heinz, Sebastian and O'Modhrain, Sile},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {collaborative performances,nime10,social,tangible user interfaces},
number = {Nime},
pages = {339--342},
title = {{Designing a Shareable Musical TUI}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}339.pdf},
year = {2010}
}
@inproceedings{Trail2014,
abstract = {The El-Lamellophone (El-La) is a Lamellophone hyperinstrument incorporating electronic sensors and integrated DSP. Initial investigations have been made into digitallycontrolled physical actuation of the acoustic tines. An embedded Linux micro-computer supplants the laptop. A piezoelectric pickup is mounted to the underside of the body of the instrument for direct audio acquisition providing a robust signal with little interference. The signal is used for electric sound-reinforcement, creative signal processing and audio analysis developed in Puredata (Pd). This signal inputs and outputs the micro computer via stereo 1/8th inch phono jacks. Sensors provide gesture recognition affording the performer a broader, more dynamic range of musical human computer interaction (MHCI) over specific DSP functions. Work has been done toward electromagnetic actuation of the tines, aiming to allow performer control and sensation via both traditional Lamellophone techniques, as well as extended playing techniques that incorporate shared human/computer control of the resulting sound. The goal is to achieve this without compromising the traditional sound production methods of the acoustic instrument while leveraging inherent performance gestures with embedded continuous controller values essential to MHCI. The result is an intuitive, performer designed, hybrid electro-acoustic instrument, idiomatic computer interface, and robotic acoustic instrument in one framework.},
address = {London, United Kingdom},
author = {Trail, Shawn and MacConnell, Duncan and Jenkins, Leo and Snyder, Jeff and Tzanetakis, George and Driessen, Peter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {537--540},
publisher = {Goldsmiths, University of London},
title = {{El-Lamellophone - A Low-cost, DIY, Open Framework for Acoustic Lemellophone Based Hyperinstruments}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}492.pdf},
year = {2014}
}
@inproceedings{Sa2007,
address = {New York City, NY, United States},
author = {Sa, Adriana},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {428},
title = {{Thresholds}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}428.pdf},
year = {2007}
}
@inproceedings{Henriques2009,
address = {Pittsburgh, PA, United States},
author = {Henriques, Tom{\'{a}}s},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {a,a double slide mechanism,computer music,consisting of a set,hardware and software design,making,musical instrument,nime09,of two,sensor technologies,sensors used in their,these are as follows},
pages = {260--261},
title = {{Double Slide Controller}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}260.pdf},
year = {2009}
}
@inproceedings{Young2007,
abstract = {This paper presents a newly created database containingcalibrated gesture and audio data corresponding to variousviolin bowstrokes, as well as video and motion capture datain some cases. The database is web-accessible and searchable by keywords and subject. It also has several importantfeatures designed to improve accessibility to the data and tofoster collaboration between researchers in fields related tobowed string synthesis, acoustics, and gesture.},
address = {New York City, NY, United States},
author = {Young, Diana and Deshmane, Anagha},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {audio,bowed string,bowing,bowing parame- ters,bowstroke,gesture,technique,violin},
pages = {352--357},
title = {{Bowstroke Database : A Web-Accessible Archive of Violin Bowing Data}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}352.pdf},
year = {2007}
}
@inproceedings{Takahashi2007,
address = {New York City, NY, United States},
author = {Takahashi, Masato and Tanaka, Hiroya},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {429},
title = {{bog : Instrumental Aliens}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}429.pdf},
year = {2007}
}
@inproceedings{Hughes2004,
address = {Hamamatsu, Japan},
author = {Hughes, Stephen and Cannon, Cormac and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {capacitive,midi,variable tonehole control,woodwind controller},
pages = {199--200},
title = {{Epipe : A Novel Electronic Woodwind Controller}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}199.pdf},
year = {2004}
}
@inproceedings{Arellano2014,
abstract = {This paper presents a new circular tangible interface where one or multiple users can collaborate and interact in real time by placing and moving passive wooden pucks on a transparent tabletop in order to create music. The design encourages physical intuition and visual feedback on the music being created. An arm with six optical sensors rotates beneath a transparent surface, triggering sounds based on the objects placed above. The interface's simplicity and tangibility make it easy to learn and suitable for a broad range of users.},
address = {London, United Kingdom},
author = {Arellano, Daniel G{\'{a}}bana and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {84--85},
publisher = {Goldsmiths, University of London},
title = {{Radear: A Tangible Spinning Music Sequencer}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}324.pdf},
year = {2014}
}
@inproceedings{Han:2013,
abstract = {This paper proposes a musical performance feedback system based on real-time audio-score alignment for musical instrument education of beginner musicians. In the proposed system, we do not make use of symbolic data such as MIDI, but acquire a real-time audio input from on-board microphone of smartphone. Then, the system finds onset and pitch of the note from the signal, to align this information with the ground truth musical score. Real-time alignment allows the system to evaluate whether the user played the correct note or not, regardless of its timing, which enables user to play at their own speed, as playing same tempo with original musical score is problematic for beginners. As an output of evaluation, the system notifies the user about which part they are currently performing, and which note were played incorrectly.},
address = {Daejeon, Republic of Korea},
author = {Han, Yoonchang and Kwon, Sejun and Lee, Kibeom and Lee, Kyogu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Music education,Music performance analysis,Real-time score following},
pages = {120--121},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Musical Performance Evaluation System for Beginner Musician based on Real-time Score Following}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Court2007,
address = {New York City, NY, United States},
author = {Dearraindrop, ,},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {421},
title = {{Miller}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}421.pdf},
year = {2007}
}
@inproceedings{Quintas2010,
address = {Sydney, Australia},
author = {Quintas, Rudolfo},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {electronic music performance,glitch music,hyper-instruments,nime10},
number = {June},
pages = {15--18},
title = {{Glitch Delighter : Lighter's Flame Base Hyper-Instrument for Glitch Music in Burning The Sound Performance}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}212.pdf},
year = {2010}
}
@inproceedings{Hansen2011,
address = {Oslo, Norway},
author = {Hansen, Anne-Marie S and Anderson, Hans J and Raudaskoski, Pirkko},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {collaborative interfaces,games,improvisation,interactive music,novice,play,social interaction},
number = {June},
pages = {220--223},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Play Fluency in Music Improvisation Games for Novices}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}220.pdf},
year = {2011}
}
@inproceedings{Maki-patola2005,
address = {Vancouver, BC, Canada},
author = {M{\"{a}}ki-patola, Teemu and Laitinen, Juha and Kanerva, Aki and Takala, Tapio},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {11--16},
title = {{Experiments with Virtual Reality Instruments}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}011.pdf},
year = {2005}
}
@inproceedings{Fohl:2013,
abstract = {This paper presents the design and implementation of agesture control interface for a wave field synthesis system.The user's motion is tracked by a IR-camera-based trackingsystem. The developed connecting software processes thetracker data to modify the positions of the virtual soundsources of the wave field synthesis system. Due to the mod-ular design of the software, the triggered actions of the ges-tures may easily be modified. Three elementary gestureswere designed and implemented: Select / deselect, circularmovement and radial movement. The guidelines for gesturedesign and detection are presented, and the user experiencesare discussed.},
address = {Daejeon, Republic of Korea},
author = {Fohl, Wolfgang and Nogalski, Malte},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Wave field synthesis,gesture control},
pages = {341--346},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Gesture Control Interface for a Wave Field Synthesis System}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Fohl:2013,
abstract = {This paper presents the design and implementation of agesture control interface for a wave field synthesis system.The user's motion is tracked by a IR-camera-based trackingsystem. The developed connecting software processes thetracker data to modify the positions of the virtual soundsources of the wave field synthesis system. Due to the mod-ular design of the software, the triggered actions of the ges-tures may easily be modified. Three elementary gestureswere designed and implemented: Select / deselect, circularmovement and radial movement. The guidelines for gesturedesign and detection are presented, and the user experiencesare discussed.},
address = {Daejeon, Republic of Korea},
author = {Fohl, Wolfgang and Nogalski, Malte},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Wave field synthesis,gesture control},
month = {may},
pages = {341--346},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Gesture Control Interface for a Wave Field Synthesis System}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{nime2014:operrotin,
abstract = {Conceiving digital musical instruments might be challenging in terms of spectator accessibility. Depending on the interface and the complexity of the software used as a transition between the controller and sound, a musician performance can be totally opaque for the audience and loose its interest. This paper examines the possibility of adding a visual feedback to help the public understanding, and add expressivity to the performance. It explores the various mapping organizations between controller and sound, giving different spaces of representation for the visual feedback. It can be either an amplification of the controller parameters, or a representation of the related musical parameters. Different examples of visualization are presented and evaluated, taking the Cantor Digitalis as a support. It appears the representation of musical parameters, little used compared to the representation of controllers, received a good opinion from the audience, highlighting the musical intention of the performers.},
address = {London, United Kingdom},
author = {Perrotin, Olivier and D'Alessandro, Christophe},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {605--608},
publisher = {Goldsmiths, University of London},
title = {{Visualizing Gestures in the Control of a Digital Musical Instrument}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}406.pdf},
year = {2014}
}
@inproceedings{Ferguson2006,
address = {Paris, France},
author = {Ferguson, Sam},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {interactive sonification,music,sonification,sound visualization},
pages = {384--389},
title = {{Learning Musical Instrument Skills Through Interactive Sonification}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}384.pdf},
year = {2006}
}
@inproceedings{Henson:2012,
abstract = {This paper introduces the concept of Kugelschwung, a digital musical instrument centrally based around the use of pendulums and lasers to create unique and highly interactive electronic ambient soundscapes. Here, we explore the underlying design and physical construction of the instrument, as well as its implementation and feasibility as an instrument in the real world. To conclude, we outline potential expansions to the instrument, describing how its range of applications can be extended to accommodate a variety of musical styles.},
address = {Ann Arbor, Michigan},
author = {Henson, Jamie and Collins, Benjamin and Giles, Alexander and Webb, Kathryn and Livingston, Matthew and Mortensson, Thomas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {electronic,expressive performance,instrument design,laser,pendulums,sampler,soundscape},
publisher = {University of Michigan},
title = {{Kugelschwung - a Pendulum-based Musical Instrument}},
year = {2012}
}
@inproceedings{nime2014:drector,
abstract = {Actuated instruments is a growing area of activity for research and composition, yet there has been little focus on membrane-based instruments. This paper describes a novel design for an internally actuated drum based on the mechanical principles of a loudspeaker. Implementation is described in detail; in particular, two modes of actuation, a moving-coil electromagnet and a moving-magnet design, are described. We evaluate the drum using a synthesized frequency sweep, and find that the instrument has a broad frequency response and exhibits qualities of both a drum and speaker.},
address = {London, United Kingdom},
author = {Rector, David and Topel, Spencer},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {395--398},
publisher = {Goldsmiths, University of London},
title = {{Internally Actuated Drums for Expressive Performance}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}559.pdf},
year = {2014}
}
@inproceedings{Hamel2006,
address = {Paris, France},
author = {Hamel, Keith},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
pages = {380--383},
title = {{Integrated Interactive Music Performance Environment}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}380.pdf},
volume = {1},
year = {2006}
}
@inproceedings{Paine2002,
address = {Dublin, Ireland},
author = {Paine, Garth},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {keywords are your choice},
pages = {143--144},
title = {{GESTATION}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}143.pdf},
year = {2002}
}
@inproceedings{Berthaut2011,
abstract = {First Person Shooters are among the most played computer videogames. They combine navigation, interaction and collaboration in3D virtual environments using simple input devices, i.e. mouseand keyboard. In this paper, we study the possibilities broughtby these games for musical interaction. We present the Couacs, acollaborative multiprocess instrument which relies on interactiontechniques used in FPS together with new techniques adding theexpressiveness required for musical interaction. In particular, theFaders For All game mode allows musicians to perform patternbased electronic compositions.},
address = {Oslo, Norway},
author = {Berthaut, Florent and Katayose, Haruhiro and Wakama, Hironori and Totani, Naoyuki and Sato, Yuichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {3D interaction,collaborative,first person shooters,fps,multiprocess instrument,the couacs},
pages = {44--47},
title = {{First Person Shooters as Collaborative Multiprocess Instruments}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}044.pdf},
year = {2011}
}
@inproceedings{Jo2008,
address = {Genoa, Italy},
author = {Jo, Kazuhiro and Nagano, Norihisa},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {installation,nime08,plug-in,software,sound and image processing},
pages = {315--318},
title = {{Monalisa : "See the Sound , Hear the Image"}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}315.pdf},
year = {2008}
}
@inproceedings{Mann2009,
address = {Pittsburgh, PA, United States},
author = {Mann, Yotam and Lubow, Jeff and Freed, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {86--89},
title = {{The Tactus : a Tangible , Rhythmic Grid Interface Using Found-Objects}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}086.pdf},
year = {2009}
}
@inproceedings{Hunt2002,
address = {Dublin, Ireland},
author = {Hunt, Andy D and Wanderley, Marcelo M and Paradis, Matthew},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {electronic instruments and t,electronic musical instruments,h e,human-computer interaction,mapping strategies},
pages = {88--93},
title = {{The importance of Parameter Mapping in Electronic Instrument Design}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}088.pdf},
year = {2002}
}
@inproceedings{Bragg:2013,
abstract = {This paper presents a graph-theoretic model that supports the design andanalysis of data flow within digital musical instruments (DMIs). The state ofthe art in DMI design fails to provide any standards for the scheduling ofcomputations within a DMI's data flow. It does not provide a theoreticalframework within which we can analyze different scheduling protocols and theirimpact on the DMI's performance. Indeed, the mapping between the DMI's sensoryinputs and sonic outputs is classically treated as a black box. DMI designersand builders are forced to design and schedule the flow of data through thisblack box on their own. Improper design of the data flow can produceundesirable results, ranging from overflowing buffers that cause system crashesto misaligned sensory data that result in strange or disordered sonic events.In this paper, we attempt to remedy this problem by providing a framework forthe design and analysis of the DMI data flow. We also provide a schedulingalgorithm built upon that framework that guarantees desirable properties forthe resulting DMI.},
address = {Daejeon, Republic of Korea},
author = {Bragg, Danielle and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {DMI design,data flow,mapping function},
pages = {237--242},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Synchronous Data Flow Modeling for DMIs}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Henriques2008,
address = {Genoa, Italy},
author = {Henriques, Tom{\'{a}}s},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {computer music,musical instrument,nime08,sensor technologies},
pages = {307--310},
title = {{META-EVI Innovative Performance Paths with a Wind Controller}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}307.pdf},
year = {2008}
}
@inproceedings{Partridge2009,
address = {Pittsburgh, PA, United States},
author = {Partridge, Grant and Irani, Pourang and Fitzell, Gordon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {collaborative composition,collaborative instruments,customizable instruments,dio interfaces,group improvisation,nime09,spatial au-,tabletop computers},
pages = {78--81},
title = {{Let Loose with WallBalls, a Collaborative Tabletop Instrument for Tomorrow}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}078.pdf},
year = {2009}
}
@inproceedings{Pardo:2012,
abstract = {Potential users of audio production software, such as parametric audio equalizers, may be discouraged by the complexity of the interface. A new approach creates a personalized on-screen slider that lets the user manipulate the audio in terms of a descriptive term (e.g. "warm"), without the user needing to learn or use the interface of an equalizer. This system learns mappings by presenting a sequence of sounds to the user and correlating the gain in each frequency band with the user's preference rating. The system speeds learning through transfer learning. Results on a study of 35 participants show how an effective, personalized audio manipulation tool can be automatically built after only three ratings from the user.},
address = {Ann Arbor, Michigan},
author = {Pardo, Bryan and Little, David and Gergle, Darren},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Human computer interaction,multimedia production,music,transfer learning},
publisher = {University of Michigan},
title = {{Towards Speeding Audio EQ Interface Building with Transfer Learning}},
year = {2012}
}
@inproceedings{Cappelen2011,
abstract = {The traditional role of the musical instrument is to be the working tool of the professional musician. On the instrument the musician performs music for the audience to listen to. In this paper we present an interactive installation, where we expand the role of the instrument to motivate musicking and cocreation between diverse users. We have made an open installation, where users can perform a variety of actions in several situations. By using the abilities of the computer, we have made an installation, which can be interpreted to have many roles. It can both be an instrument, a co-musician, a communication partner, a toy, a meeting place and an ambient musical landscape. The users can dynamically shift between roles, based on their abilities, knowledge and motivation. },
address = {Oslo, Norway},
author = {Cappelen, Birgitta and Anderson, Anders-Petter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {design,genre,interaction,interactive installation,music instrument,musicking,narrative,open,role,sound art},
pages = {511--514},
title = {{Expanding the Role of the Instrument}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}511.pdf},
year = {2011}
}
@inproceedings{nime2014:ahornof,
abstract = {Although new sensor devices and data streams are increasingly used for musical expression, and although eye-tracking devices have become increasingly cost-effective and prevalent in research and as a means of communication for people with severe motor impairments, eye-controlled musical expression nonetheless remains somewhat elusive and minimally explored. This paper (a) identifies a number of fundamental human eye movement capabilities and constraints which determine in part what can and cannot be musically expressed with eye movements, (b) reviews prior work on eye-controlled musical expression, and (c) analyzes and provides a taxonomy of what has been done, and what will need to be addressed in future eye-controlled musical instruments. The fundamental human constraints and processes that govern eye movements create a challenge for eye-controlled music in that the instrument needs to be designed to motivate or at least permit specific unique visual goals, each of which when accomplished must then be mapped, using the eye tracker and some sort of sound generator, to different musical outcomes. The control of the musical instrument is less direct than if it were played with muscles that can be controlled in a more direct manner, such as the muscles in the hands.},
address = {London, United Kingdom},
author = {Hornof, Anthony},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {461--466},
publisher = {Goldsmiths, University of London},
title = {{The Prospects For Eye-Controlled Musical Performance}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}562.pdf},
year = {2014}
}
@inproceedings{Singer2003,
address = {Montreal, QC, Canada},
author = {Singer, Eric},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {220--221},
title = {{Sonic Banana: A Novel Bend-Sensor-Based MIDI Controller}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}220.pdf},
year = {2003}
}
@inproceedings{Patten2002,
address = {Dublin, Ireland},
author = {Patten, James and Recht, Ben and Ishii, Hiroshi},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {midi,musical controllers,rf tagging,tangible interfaces},
pages = {148--153},
title = {{Audiopad: A Tag-based Interface for Musical Performance}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}148.pdf},
year = {2002}
}
@inproceedings{Reboursiere2010,
address = {Sydney, Australia},
author = {Reboursi{\`{e}}re, Lo{\"{i}}c and Frisson, Christian and L{\"{a}}hdeoja, Otso and Mills, John A and Picard-Limpens, C{\'{e}}cile and Todoroff, Todor},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {audio synthesis,augmented guitar,digital audio effects,gestural sensing,hexaphonic guitar,multimodal interaction,nime10,polyphonic tran-,scription},
number = {Nime},
pages = {415--418},
title = {{Multimodal Guitar : A Toolbox For Augmented Guitar Performances}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}415.pdf},
year = {2010}
}
@inproceedings{Flety2004,
address = {Hamamatsu, Japan},
author = {Fl{\'{e}}ty, Emmanuel and Leroy, Nicolas and Ravarini, Jean-Christophe and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {11,2,802,available technology,computer music,ethernet,gesture,sensors},
pages = {157--160},
title = {{Versatile Sensor Acquisition System Utilizing Network Technology}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}157.pdf},
year = {2004}
}
@inproceedings{Kirk2009,
address = {Pittsburgh, PA, United States},
author = {Kirk, Jonathon and Weisert, Lee},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {290--292},
title = {{The Argus Project : Underwater Soundscape Composition with Laser- Controlled Modulation}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}290.pdf},
year = {2009}
}
@inproceedings{Marquez-Borbon2011,
address = {Oslo, Norway},
author = {Marquez-Borbon, Adnan and Gurevich, Michael and Fyans, A Cavan and Stapleton, Paul},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {dmis,experiment,instrument design,methodology},
number = {June},
pages = {373--376},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Designing Digital Musical Interactions in Experimental Contexts}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}373.pdf},
year = {2011}
}
@inproceedings{Donaldson2007,
abstract = {Chroma based representations of acoustic phenomenon are representations of sound as pitched acoustic energy. A framewise chroma distribution over an entire musical piece is a useful and straightforward representation of its musical pitch over time. This paper examines a method of condensing the block-wise chroma information of a musical piece into a two dimensional embedding. Such an embedding is a representation or map of the different pitched energies in a song, and how these energies relate to each other in the context of the song. The paper presents an interactive version of this representation as an exploratory analytical tool or instrument for granular synthesis. Pointing and clicking on the interactive map recreates the acoustical energy present in the chroma blocks at that location, providing an effective way of both exploring the relationships between sounds in the original piece, and recreating a synthesized approximation of these sounds in an instrumental fashion. },
address = {New York City, NY, United States},
author = {Donaldson, Justin and Knopke, Ian and Raphael, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Chroma,dimensionality reduction,granular synthesis},
pages = {213--219},
title = {{Chroma Palette : Chromatic Maps of Sound As Granular Synthesis Interface}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}213.pdf},
year = {2007}
}
@inproceedings{Weitzner:2012,
abstract = {massMobile is a client-server system for mass audience participation in live performances using smartphones. It was designed to flexibly adapt to a variety of participatory performance needs and to a variety of performance venues. It allows for real time bi-directional communication between performers and audiences utilizing existing wireless 3G, 4G, or WiFi networks. In this paper, we discuss the goals, design, and implementation of the framework, and we describe several projects realized with massMobile.},
address = {Ann Arbor, Michigan},
author = {Weitzner, Nathan and Freeman, Jason and Garrett, Stephen and Chen, Yan-Ling},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Weitzner et al. - 2012 - massMobile - an Audience Participation Framework.pdf:pdf},
keywords = {audience participation,mobile,network music,performance,smartphone},
publisher = {University of Michigan},
title = {{massMobile - an Audience Participation Framework}},
year = {2012}
}
@article{Delic:2002:ADT:763940.763939,
address = {New York, NY, USA},
author = {Delic, Kemal A},
doi = {10.1145/763940.763939},
issn = {1530-2180},
journal = {Ubiquity},
month = {oct},
number = {October},
publisher = {ACM},
title = {{Architecting Durable Things}},
url = {http://doi.acm.org/10.1145/763940.763939},
volume = {2002},
year = {2002}
}
@inproceedings{Bowers2006,
address = {Paris, France},
author = {Bowers, John and Villar, Nicolas},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {ad hoc instruments,expression,music,new interfaces for musical,performance,physical interfaces,pin,play},
pages = {234--239},
title = {{Creating Ad Hoc Instruments with Pin{\&}Play{\&}Perform}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}234.pdf},
year = {2006}
}
@inproceedings{nime2014:gdublon,
abstract = {We present the FingerSynth, a wearable musical instrument made up of a bracelet and set of rings that enable its player to produce sound by touching nearly any surface in their environment. Each ring contains a small, independently controlled exciter transducer commonly used for auditory bone conduction. The rings sound loudly when they touch a hard object, and are practically silent otherwise. When a wearer touches their own (or someone else's) head, the contacted person hears the sound through bone conduction, inaudible to others. The bracelet contains a microcontroller, a set of FET transistors, an accelerometer, and a battery. The microcontroller generates a separate audio signal for each ring, switched through the FETs, and can take user input through the accelerometer in the form of taps, flicks, and other gestures. The player controls the envelope and timbre of the sound by varying the physical pressure and the angle of their finger on the surface, or by touching differently resonant surfaces. Because its sound is shaped by direct, physical contact with objects and people, the FingerSynth encourages players to experiment with the materials around them and with one another, making music with everything they touch.},
address = {London, United Kingdom},
author = {Dublon, Gershon and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {134--135},
publisher = {Goldsmiths, University of London},
title = {{FingerSynth: Wearable Transducers for Exploring the Environment and Playing Music Everywhere}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}379.pdf},
year = {2014}
}
@inproceedings{Baalman2003,
address = {Montreal, QC, Canada},
author = {Baalman, Marije A},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {19--23},
title = {{The STRIMIDILATOR: a String Controlled MIDI-Instrument}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}019.pdf},
year = {2003}
}
@inproceedings{Collicutt2009,
abstract = {This paper presents an evaluation and comparison of four input devices for percussion tasks: a standard tom drum, Roland V-Drum, and two established examples of gestural controllers: the Buchla Lightning II, and the Radio Baton. The primary goal of this study was to determine how players' actions changed when moving from an acoustic instrument like the tom drum, to a gestural controller like the Buchla Lightning, which bears little resemblance to an acoustic percussion instrument. Motion capture data was analyzed by comparing a subject's hand height variability and timing accuracy across the four instruments as they performed simple musical tasks. Results suggest that certain gestures such as hand height amplitude can be adapted to these gestural controllers with little change and that in general subjects' timing variability is significantly affected when playing on the Lightning and Radio Baton when compared to the more familiar tom drum and VDrum. Possible explanations and other observations are also presented. },
address = {Pittsburgh, PA, United States},
author = {Collicutt, Mike and Casciato, Carmine and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Buchla Lightning II,Evaluation of Input Devices,Motion Capture,Radio Baton.},
pages = {1--6},
title = {{From Real to Virtual : A Comparison of Input Devices for Percussion Tasks}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}001.pdf},
year = {2009}
}
@inproceedings{DArcangelo2002,
address = {Dublin, Ireland},
author = {D'Arcangelo, Gideon},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {creative expression,input devices,musical controllers},
pages = {46--49},
title = {{Creating a Context for Musical Innovation: A NIME Curriculum}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}046.pdf},
year = {2002}
}
@inproceedings{Berdahl2009,
address = {Pittsburgh, PA, United States},
author = {Berdahl, Edgar and Niemeyer, G{\"{u}}nter and Smith, Julius O},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {but,com-,currently chai 3d is,figure 1,fully supported only on,grip,haptic musical instrument,haptics,hsp,musician holding the falcon,nime09,novint,pd,physical modeling,pure data,puter music,windows},
pages = {262--263},
title = {{HSP : A Simple and Effective Open-Source Platform for Implementing Haptic Musical Instruments}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}262.pdf},
year = {2009}
}
@inproceedings{Oliver:2012,
abstract = {There is some evidence that structured training can benefit cochlear implant (CI) users' appraisal of music as well as their music perception abilities. There are currently very limited music training resources available for CI users to explore. This demonstration will introduce delegates to the `Interactive Music Awareness Program' (IMAP) for cochlear implant users, which was developed in response to the need for a client-centered, structured, interactive, creative, open-ended, educational and challenging music (re)habilitation resource.},
address = {Ann Arbor, Michigan},
author = {Oliver, Benjamin R and van Besouw, Rachel M and Nicholls, David R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {auditory training,client-centred software,cochlear implants,interactive learning,music,perception,rehabilitation},
publisher = {University of Michigan},
title = {{The `Interactive Music Awareness Program' (IMAP) for Cochlear Implant Users}},
year = {2012}
}
@inproceedings{Gadd2002,
address = {Dublin, Ireland},
author = {Gadd, Ashley and Fels, Sidney S},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {expressive interface,granular synthesis,metaphor,prop-based con-,transparency,troller},
pages = {65--70},
title = {{MetaMuse: Metaphors for Expressive Instruments}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}065.pdf},
year = {2002}
}
@inproceedings{Lock2006,
address = {Paris, France},
author = {Lock, Damien and Schiemer, Greg},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {array,immersive sound,loud-speaker,multi-channel sound,multimedia,performance,real-time media,sound installation,streaming media},
pages = {89--92},
title = {{Orbophone: a New Interface for Radiating Sound and Image}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}089.pdf},
year = {2006}
}
@inproceedings{Ferguson2009,
address = {Pittsburgh, PA, United States},
author = {Ferguson, Sam and Beilharz, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {35--36},
title = {{An Interface for Live Interactive Sonification}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}035.pdf},
year = {2009}
}
@inproceedings{Zappi2011,
address = {Oslo, Norway},
author = {Zappi, Victor and Mazzanti, Dario and Brogni, Andrea and Caldwell, Darwin},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {hybrid choreographies,interactive performance,music control,reality,virtual},
number = {June},
pages = {355--360},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Design and Evaluation of a Hybrid Reality Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}355.pdf},
year = {2011}
}
@inproceedings{Hamilton2006,
address = {Paris, France},
author = {Hamilton, Robert},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {bioinformatics,composition,real-time score generation},
pages = {338--341},
title = {{Bioinformatic Feedback: Performer Bio-data as a Driver for Real-time Composition}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}338.pdf},
year = {2006}
}
@inproceedings{Eaton2005,
abstract = {In this presentation, we discuss and demonstrate a multiple touch sensitive (MTS) keyboard developed by Robert Moog for John Eaton. Each key of the keyboard is equipped with sensors that detect the three-dimensional position of the performer's finger. The presentation includes some of Eaton's performances for certain earlier prototypes as well as this keyboard. },
address = {Vancouver, BC, Canada},
author = {Eaton, John and Moog, Robert},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {MTS,Multiple touch sensitive,key sensor design,keyboard,upgrading to present-day computers},
pages = {258--259},
title = {{Multiple-Touch-Sensitive Keyboard}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}258.pdf},
year = {2005}
}
@inproceedings{Brandtsegg2011,
abstract = {The article describes a flexible mapping technique realized as a many-to-many dynamic mapping matrix. Digital sound generation is typically controlled by a large number of parameters and efficient and flexible mapping is necessary to provide expressive control over the instrument. The proposed modulation matrix technique may be seen as a generic and selfmodifying mapping mechanism integrated in a dynamic interpolation scheme. It is implemented efficiently by taking advantage of its inherent sparse matrix structure. The modulation matrix is used within the Hadron Particle Synthesizer, a complex granular module with 200 synthesis parameters and a simplified performance control structure with 4 expression parameters. },
address = {Oslo, Norway},
author = {Brandtsegg, {\O}yvind and Saue, Sigurd and Johansen, Thom},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Mapping,granular synthesis,live performance,modulation},
pages = {316--319},
title = {{A Modulation Matrix for Complex Parameter Sets}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}316.pdf},
year = {2011}
}
@inproceedings{Carlson:2012,
abstract = {Borderlands is a new interface for composing and performing with granular synthesis. The software enables flexible, realtime improvisation and is designed to allow users to engage with sonic material on a fundamental level, breaking free of traditional paradigms for interaction with this technique. The user is envisioned as an organizer of sound, simultaneously assuming the roles of curator, performer, and listener. This paper places the software within the context of painterly interfaces and describes the user interaction design and synthesis methodology.},
address = {Ann Arbor, Michigan},
author = {Carlson, Chris and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {CCRMA,Granular synthesis,NIME,improvisation,organized sound,painterly interfaces},
month = {may},
publisher = {University of Michigan},
title = {{Borderlands -An Audiovisual Interface for Granular Synthesis}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}152.pdf},
year = {2012}
}
@inproceedings{Perez2007,
address = {New York City, NY, United States},
author = {P{\'{e}}rez, Miguel A and Knapp, Benjamin and Alcorn, Michael},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {composition,emotion measurement,integral music controller,nime07,physiological measurement,spatialisation},
pages = {289--292},
title = {{D{\'{i}}amair : Composing for Choir and Integral Music Controller}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}289.pdf},
year = {2007}
}
@inproceedings{nime2014:pdahlstedt1,
abstract = {The idea behind the YouHero was two-fold. First, to make an expressive instrument out of the computer game toy guitar controller from the famous game GuitarHero. With its limited amount of control parameters, this was a challenge. Second, through this instrument we wanted to provide an alternative to the view that you become a hero by perfect imitation of your idols. Instead, play yourself. You are the hero. In this paper, we describe the design of the instrument, including its novel mapping approach based on switched timbre vectors scaled by accellerometer data, unconventional sound engines and the sound and mapping editing features, including manual editing of individual vectors. The instrument is evaluated through its practical applications during the whole project, with workshops with teenagers, a set of state-funded commissions from professional composers, and the development of considerable skill by the key performers. We have also submitted a performance proposal for this project.},
address = {London, United Kingdom},
author = {Dahlstedt, Palle and Karlsson, Patrik and Widell, Katarina and Blomdahl, Tony},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {403--406},
publisher = {Goldsmiths, University of London},
title = {{YouHero - Making an Expressive Concert Instrument from the GuitarHero Controller}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}513.pdf},
year = {2014}
}
@inproceedings{Bozzolan2008,
address = {Genoa, Italy},
author = {Bozzolan, Matteo and Cospito, Giovanni},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {eyesweb,interaction,interfaces,multimodal interaction,nime08,pure data,sound spatialization},
pages = {24--27},
title = {{SMuSIM : a Prototype of Multichannel Spatialization System with Multimodal Interaction Interface}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}024.pdf},
year = {2008}
}
@inproceedings{Schedel2011,
address = {Oslo, Norway},
author = {Schedel, Margaret and Perry, Phoenix and Fiebrink, Rebecca},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
number = {June},
pages = {453--456},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Wekinating 000000Swan : Using Machine Learning to Create and Control Complex Artistic Systems}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}453.pdf},
year = {2011}
}
@inproceedings{Johnston:2013,
abstract = {This paper describes an audio-visual performance system based on real-timefluid simulation. The aim is to provide a rich environment for works whichblur the boundaries between dance and instrumental performance -- and sound andvisuals -- while maintaining transparency for audiences and new performers. The system uses infra-red motion tracking to allow performers to manipulate areal-time fluid simulation, which in turn provides control data forcomputer-generated audio and visuals. It also provides a control andconfiguration system which allows the behaviour of the interactive system to bechanged over time, enabling the structure within which interactions take placeto be `composed'.},
address = {Daejeon, Republic of Korea},
author = {Johnston, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {composition,dance,fluid simulation,performance},
month = {may},
pages = {132--135},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Fluid Simulation as Full Body Audio-Visual Instrument}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Trento2013,
abstract = {This paper describes the development of a prototype of a sonic toy forpre-scholar kids. The device, which is a mod- ified version of a footballratchet, is based on the spinning gesture and it allows to experience fourdifferent types of auditory feedback. These algorithms let a kid play withmusic rhythm, generate a continuous sound feedback and control the pitch of apiece of music. An evaluation test of the device has been performed withfourteen kids in a kindergarten. Results and observations showed that kidspreferred the algorithms based on the exploration of the music rhythm and onpitch shifting.},
address = {Daejeon, Republic of Korea},
author = {Trento, Stefano and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Sonic toy,auditory feedback.,children},
pages = {456--459},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Flag beat: a novel interface for rhythmic musical expression for kids}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Davidson2006,
address = {Paris, France},
author = {Davidson, Philip L and Han, Jefferson Y},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {bi-manual,multi-touch,multi-user,synthesis,tactile,touch},
pages = {216--219},
title = {{Synthesis and Control on Large Scale Multi-Touch Sensing Displays}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}216.pdf},
year = {2006}
}
@inproceedings{Udell2014,
abstract = {In our current era, where smartphones are commonplace and buzzwords like ``the internet of things," ``wearable tech," and ``augmented reality" are ubiquitous, translating performance gestures into data and intuitively mapping it to control musical/visual parameters in the realm of computing should be trivial; but it isn't. Technical barriers still persist that limit this activity to exclusive groups capable of learning skillsets far removed from one's musical craft. These skills include programming, soldering, microprocessors, wireless protocols, and circuit design. Those of us whose creative activity is centered in NIME have to become polyglots of many disciplines to achieve our work. In the NIME community, it's unclear that we should even draw distinctions between 'artist' and 'technician', because these skillsets have become integral to our creative practice. However, what about the vast communities of musicians, composers, and artists who want to leverage sensing to take their craft into new territory with no background in circuits, soldering, embedded programming, and sensor function? eMersion, a plug-and-play, modular, wireless alternative solution for creating NIMEs will be presented. It enables one to bypass the technical hurdles listed above in favor of immediate experimentation with musical practice and wireless sensing. A unique software architecture will also be unveiled that enables one to quickly and intuitively process and map unpredictable numbers and types of wireless data streams, the Digital Data Workstation.},
address = {London, United Kingdom},
author = {Udell, Chet and Sain, James Paul},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {130--133},
publisher = {Goldsmiths, University of London},
title = {{eMersion | Sensor-controlled Electronic Music Modules {\&} Digital Data Workstation}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}272.pdf},
year = {2014}
}
@inproceedings{Havryliv2010,
address = {Sydney, Australia},
author = {Havryliv, Mark},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {audio de-,chaos and music,chaotic dynamics and oscillators,dif-,ferential equations and music,mathematica,nime10,scriptors and mpeg-7},
number = {Nime},
pages = {94--99},
title = {{Composing For Improvisation with Chaotic Oscillators}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}094.pdf},
year = {2010}
}
@inproceedings{McLean:2013,
abstract = {The Human vocal tract is considered for its sonorous qualities incarrying prosodic information, which implicates vision in theperceptual processes of speech. These considerations are put in thecontext of previous work in NIME, forming background for theintroduction of two sound installations; ``Microphone'', which uses acamera and computer vision to translate mouth shapes to sounds, and``Microphone II'', a work-in-progress, which adds physical modellingsynthesis as a sound source, and visualisation of mouth movements.},
address = {Daejeon, Republic of Korea},
author = {McLean, Alex and Shin, EunJoo and Ng, Kia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {computer vision,face tracking,installation,microphone},
pages = {381--384},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Paralinguistic Microphone}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Pelletier2005,
abstract = {This paper describes DspMap, a graphical user interface (GUI)designed to assist the dynamic routing of signal generators andmodifiers currently being developed at the International Academyof Media Arts {\&} Sciences. Instead of relying on traditional boxand-line approaches, DspMap proposes a design paradigm whereconnections are determined by the relative positions of the variouselements in a single virtual space.},
address = {Vancouver, BC, Canada},
author = {Pelletier, Jean-Marc},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Graphical user interface,dynamic routing,map,real-time performance},
pages = {89--92},
title = {{A Graphical Interface for Real-Time Signal Routing}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}089.pdf},
year = {2005}
}
@inproceedings{nime2014:mmainsbridge,
abstract = {This paper explores the challenge of achieving nuanced control and physical engagement with gestural interfaces in performance. Performances with a prototype gestural performance system, Gestate, provide the basis for insights into the application of gestural systems in live contexts. These reflections stem from a performer's perspective, outlining the experience of prototyping and performing with augmented instruments that extend vocal or instrumental technique through ancillary gestures. Successful implementation of rapidly evolving gestural technologies in real-time performance calls for new approaches to performing and musicianship, centred around a growing understanding of the body's physical and creative potential. For musicians hoping to incorporate gestural control seamlessly into their performance practice a balance of technical mastery and kinaesthetic awareness is needed to adapt existing systems to their own purposes. Within non-tactile systems, visual feedback mechanisms can support this process by providing explicit visual cues that compensate for the absence of haptic or tangible feedback. Experience gained through prototyping and performance can yield a deeper understanding of the broader nature of gestural control and the way in which performers inhabit their own bodies.},
address = {London, United Kingdom},
author = {Mainsbridge, Mary and Beilharz, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {110--113},
publisher = {Goldsmiths, University of London},
title = {{Body As Instrument: Performing with Gestural Interfaces}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}393.pdf},
year = {2014}
}
@inproceedings{Bosi:2012,
abstract = {Tangible tabletop musical interfaces allowing for a collabo- rative real-time interaction in live music performances are one of the promising fields in NIMEs. At present, this kind of interfaces present at least some of the following charac- teristics that limit their musical use: latency in the inter- action, and partial or complete lack of responsiveness to gestures such as tapping, scrubbing or pressing force. Our current research is exploring ways of improving the quality of interaction with this kind of interfaces, and in particular with the tangible tabletop instrument Reactable . In this paper we present a system based on a circular array of me- chanically intercoupled force sensing resistors used to obtain a low-latency, affordable, and easily embeddable hardware system able to detect surface impacts and pressures on the tabletop perimeter. We also consider the option of com- pleting this detected gestural information with the sound information coming from a contact microphone attached to the mechanical coupling layer, to control physical modelling synthesis of percussion instruments.},
address = {Ann Arbor, Michigan},
author = {Bosi, Mathieu and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {complementary sensing.,fast low-noise analog to digital conversion,force sensing resistor,low-latency sensing,mechanical coupling,micro controller,multimodal systems,tangible tabletop interfaces},
publisher = {University of Michigan},
title = {{Towards fast multi-point force and hit detection in tabletops using mechanically intercoupled force sensing resisors}},
year = {2012}
}
@inproceedings{Han:2013a,
abstract = {This paper presents a framework that transforms fingerprint patterns intoaudio. We describe Digiti Sonus, an interactive installation performingfingerprint sonification and visualization, including novel techniques forrepresenting user-intended fingerprint expression as audio parameters. In orderto enable personalized sonification and broaden timbre of sound, theinstallation employs sound synthesis based on various visual feature analysissuch as minutiae extraction, area, angle, and push pressure of fingerprints.The sonification results are discussed and the diverse timbres of soundretrieved from different fingerprints are compared.},
address = {Daejeon, Republic of Korea},
author = {Han, Yoon Chung and Han, Byeong-jun and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Fingerprint,Fingerprint sonification,biometric data,interactive sonification,sound synthesis},
pages = {136--141},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Digiti Sonus: Advanced Interactive Fingerprint Sonification Using Visual Feature Analysis}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{KimBoyle2008,
address = {Genoa, Italy},
author = {Kim-Boyle, David},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {collaborative,interface,networks,nime08,open-form,play},
pages = {3--8},
title = {{Network Musics - Play , Engagement and the Democratization of Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}003.pdf},
year = {2008}
}
@inproceedings{Momeni2003,
abstract = {In this paper, we examine the use of spatial layouts of musicalmaterial for live performance control. Emphasis is given tosoftware tools that provide for the simple and intuitivegeometric organization of sound material, sound processingparameters, and higher-level musical structures.},
address = {Montreal},
author = {Momeni, Ali and Wessel, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
pages = {54--62},
title = {{Characterizing and Controlling Musical Material Intuitively with Geometric Models}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}054.pdf},
year = {2003}
}
@inproceedings{Weinberg2005,
address = {Vancouver, BC, Canada},
author = {Weinberg, Gil and Driscoll, Scott},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {17--22},
title = {{iltur  Connecting Novices and Experts Through Collaborative Improvisation}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}017.pdf},
year = {2005}
}
@inproceedings{Tang:2013,
abstract = {CalliMusic, is a system developed for users to generate traditional Chinesemusic by writing Chinese ink brush calligraphy, turning the long-believedstrong linkage between the two art forms with rich histories into reality. Inaddition to traditional calligraphy writing instruments (brush, ink and paper),a camera is the only addition needed to convert the motion of the ink brushinto musical notes through a variety of mappings such as human-inspired,statistical and a hybrid. The design of the system, including details of eachmapping and research issues encountered are discussed. A user study of systemperformance suggests that the result is quite encouraging. The technique is,obviously, applicable to other related art forms with a wide range ofapplications.},
address = {Daejeon, Republic of Korea},
author = {Tang, Will W W and Chan, Stephen and Ngai, Grace and Leong, Hong-va},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Assisted Music Generation,Chinese Calligraphy,Chinese Music},
pages = {84--89},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Computer Assisted Melo-rhythmic Generation of Traditional Chinese Music from Ink Brush Calligraphy}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Mclean2009,
abstract = {Phonetic symbols describe movements of the vocal tract,tongue and lips, and are combined into complex movementsforming the words of language. In music, vocables are wordsthat describe musical sounds, by relating vocal movementsto articulations of a musical instrument. We posit that vocable words allow the composers and listeners to engageclosely with dimensions of timbre, and that vocables couldsee greater use in electronic music interfaces. A preliminarysystem for controlling percussive physical modelling synthesis with textual words is introduced, with particular application in expressive specification of timbre during computer music performances.},
address = {Pittsburgh, PA, United States},
author = {Mclean, Alex and Wiggins, Geraint},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09,timbre,vocable synthesis},
pages = {276--279},
title = {{Words , Movement and Timbre}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}276.pdf},
year = {2009}
}
@inproceedings{Diakopoulos2011,
abstract = {This paper presents a series of open-source firmwares for the latest iteration of the popular Arduino microcontroller platform. A portmanteau of Human Interface Device and Arduino, the HIDUINO project tackles a major problem in designing NIMEs: easily and reliably communicating with a host computer using standard MIDI over USB. HIDUINO was developed in conjunction with a class at the California Institute of the Arts intended to teach introductory-level human-computer and human-robot interaction within the context of musical controllers. We describe our frustration with existing microcontroller platforms and our experiences using the new firmware to facilitate the development and prototyping of new music controllers. },
address = {Oslo, Norway},
author = {Diakopoulos, Dimitri and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Arduino,HCI,HID,MIDI,USB,controllers,microcontrollers},
pages = {405--408},
title = {{HIDUINO : A firmware for building driverless {\{}USB{\}}-MIDI devices using the Arduino microcontroller}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}405.pdf},
year = {2011}
}
@inproceedings{Murphy:2012,
abstract = {A problem with many contemporary musical robotic percussion systems lies in the fact that solenoids fail to respond lin- early to linear increases in input velocity. This nonlinearity forces performers to individually tailor their compositions to specific robotic drummers. To address this problem, we introduce a method of pre-performance calibration using metaheuristic search techniques. A variety of such techniques are introduced and evaluated and the results of the optimized solenoid-based percussion systems are presented and compared with output from non-calibrated systems.},
address = {Ann Arbor, Michigan},
author = {Murphy, Jim and Kapur, Ajay and Carnegie, Dale},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {human-robot interaction,musical robotics},
publisher = {University of Michigan},
title = {{Better Drumming Through Calibration: Techniques for Pre-Performance Robotic Percussion Optimization}},
year = {2012}
}
@inproceedings{Allen2005,
address = {Vancouver, BC, Canada},
author = {Allen, Jamie},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {168--171},
title = {{boomBox}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}168.pdf},
year = {2005}
}
@inproceedings{Fan:2013a,
abstract = {The BioSync interface presented in this paper merges the heart-rate basedparadigm with the brain-wave based paradigm into one mobile unit which isscalable for large audience real-time applications. The goal of BioSync is toprovide a hybrid interface, which uses audience biometric responses foraudience participation techniques. To provide an affordable and scalablesolution, BioSync collects the user's heart rate via mobile phone pulseoximetry and the EEG data via Bluetooth communication with the off-the-shelfMindWave Mobile hardware. Various interfaces have been designed and implementedin the development of audience participation techniques and systems. In thedesign and concept of BioSync, we first summarize recent interface research foraudience participation within the NIME-related context, followed by the outlineof the BioSync methodology and interface design. We then present a techniquefor dynamic tempo control based on the audience biometric responses and anearly prototype of a mobile dual-channel pulse oximetry and EEG bi-directionalinterface for iOS device (BioSync). Finally, we present discussions and ideasfor future applications, as well as plans for a series of experiments, whichinvestigate if temporal parameters of an audience's physiological metricsencourage crowd synchronization during a live event or performance, acharacteristic, which we see as having great potential in the creation offuture live musical and audiovisual performance applications.},
address = {Daejeon, Republic of Korea},
author = {Fan, Yuan-Yi and Sciotto, Myles},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Audience,Biometrics,Experience,Mobile,Social,Synchronous Interaction},
pages = {248--251},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{BioSync: An Informed Participatory Interface for Audience Dynamics and Audiovisual Content Co-creation using Mobile PPG and EEG}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Nakra2003,
abstract = {This paper describes the artistic projects undertaken at ImmersionMusic, Inc. (www.immersionmusic.org) during its three-yearexistence. We detail work in interactive performance systems,computer-based training systems, and concert production.},
address = {Montreal},
author = {Nakra, Teresa M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
keywords = {Conductor's Jacket,Digital Baton,Interactive computer music systems,gestural interaction},
pages = {151--152},
title = {{Immersion Music: a Progress Report}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}151.pdf},
year = {2003}
}
@inproceedings{Hsu2006,
address = {Paris, France},
author = {Hsu, William},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {instrument control,interactive music systems,timbre analysis},
pages = {376--379},
title = {{Managing Gesture and Timbre for Analysis and Instrument Control in an Interactive Environment}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}376.pdf},
year = {2006}
}
@inproceedings{Toenjes2009,
address = {Pittsburgh, PA, United States},
author = {Toenjes, John},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {control surface,interactive dance,interface,natural,nime09,organic,tactile},
pages = {52--53},
title = {{Natural Materials on Stage : Custom Controllers for Aesthetic Effect}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}052.pdf},
year = {2009}
}
@inproceedings{Greenlee:2013,
abstract = {In the design of recent systems, I have advanced techniques that positiongraphic synthesis methods in the context of solo, improvisational performance.Here, the primary interfaces for musical action are prepared works on paper,scanned by digital video cameras which in turn pass image data on to softwarefor analysis and interpretation as sound synthesis and signal processingprocedures. The focus of this paper is on one of these techniques, a process Idescribe as graphic waveshaping. A discussion of graphic waveshaping in basicform and as utilized in my performance work, (title omitted), is offered. Inthe latter case, the performer's objective is to guide the interpretation ofimages as sound, constantly tuning and retuning the conversion while selectingand scanning images from a large catalog. Due to the erratic nature of thesystem and the precondition that image to sound relationships are unfixed, theperformance situation is replete with the discovery of new sounds and thecircumstances that bring them into play. Graphic waveshaping may be understood as non-linear distortion synthesis withtime-varying transfer functions stemming from visual scan lines. As a form ofgraphic synthesis, visual images function as motivations for sound generation.There is a strategy applied for creating one out of the other. However, counterto compositionally oriented forms of graphic synthesis where one may assignimage characteristics to musical parameters such as pitches, durations,dynamics, etc., graphic waveshaping is foremost a processing technique, as itdistorts incoming signals according to graphically derived transfer functions.As such, it may also be understood as an audio effect; one that in myimplementations is particularly feedback dependent, oriented towards shapingthe erratic behavior of synthesis patches written in Max/MSP/Jitter. Used inthis manner, graphic waveshaping elicits an emergent system behaviorconditioned by visual features.},
address = {Daejeon, Republic of Korea},
author = {Greenlee, Shawn},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Graphic waveshaping,drawn sound,graphic sound,graphic synthesis,waveshaping synthesis},
pages = {287--290},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Graphic Waveshaping}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Berdahl2009a,
address = {Pittsburgh, PA, United States},
author = {Berdahl, Edgar and Niemeyer, G{\"{u}}nter and Smith, Julius O},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {control junction,digital waveguide,explicit,figure 1,haptic musical instrument,implicit,musical instrument,musician playing a haptic,nime09,teleoperation},
pages = {183--186},
title = {{Using Haptic Devices to Interface Directly with Digital Waveguide-Based Musical Instruments}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}183.pdf},
year = {2009}
}
@inproceedings{Schiemer2005,
abstract = {This paper describes software tools used to create java applications for performing music using mobile phones. The tools provide a means for composers working in the Pure Data composition environment to design and audition performances using ensembles of mobile phones. These tools were developed as part of a larger project motivated by the desire to allow large groups of non-expert players to perform music based on just intonation using ubiquitous technology. The paper discusses the process that replicates a Pure Data patch so that it will operate within the hardware and software constraints of the Java 2 Micro Edition. It also describes development of objects that will enable mobile phone performances to be simulated accurately in PD and to audition microtonal tuning implemented using MIDI in the j2me environment. These tools eliminate the need for composers to compose for mobile phones by writing java code. In a single desktop application, they offer the composer the flexibility to write music for multiple phones. },
address = {Vancouver, BC, Canada},
author = {Schiemer, Greg and Havryliv, Mark},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Java 2 Micro Edition; j2me; Pure Data; PD; Real-Ti},
pages = {156--159},
title = {{Pocket Gamelan: a Pure Data interface for mobile phones}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}156.pdf},
year = {2005}
}
@inproceedings{nime2014:ckiefer,
abstract = {Echo State Networks (ESNs), a form of recurrent neural network developed in the field of Reservoir Computing, show significant potential for use as a tool in the design of mappings for digital musical instruments. They have, however, seldom been used in this area, so this paper explores their possible uses. This project contributes a new open source library, which was developed to allow ESNs to run in the Pure Data dataflow environment. Several use cases were explored, focusing on addressing current issues in mapping research. ESNs were found to work successfully in scenarios of pattern classification, multiparametric control, explorative mapping and the design of nonlinearities and uncontrol. $\backslash$emph{\{}Un-trained{\}} behaviours are proposed, as augmentations to the conventional reservoir system that allow the player to introduce potentially interesting non-linearities and uncontrol into the reservoir. Interactive evolution style controls are proposed as strategies to help design these behaviours, which are otherwise dependent on arbitrary parameters. A study on sound classification shows that ESNs can reliably differentiate between two drum sounds, and also generalise to other similar input. Following evaluation of the use cases, heuristics are proposed to aid the use of ESNs in computer music scenarios.},
address = {London, United Kingdom},
author = {Kiefer, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {293--298},
publisher = {Goldsmiths, University of London},
title = {{Musical Instrument Mapping Design with Echo State Networks}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}530.pdf},
year = {2014}
}
@inproceedings{Huott2002,
address = {Dublin, Ireland},
author = {Huott, Robert},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {musical controller,tactex,tactile interface,tuning sys-},
pages = {94--98},
title = {{An Interface for Precise Musical Control}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}094.pdf},
year = {2002}
}
@inproceedings{Pirro2011,
address = {Oslo, Norway},
author = {Pirr{\`{o}}, David and Eckel, Gerhard},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {embod-,enactive interfaces,has been ap-,iment,interaction,motion tracking,of sound and music,physical modelling,to movement and gesture},
pages = {461--464},
title = {{Physical Modelling Enabling Enaction: an Example}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}461.pdf},
year = {2011}
}
@inproceedings{Collins2010,
address = {Sydney, Australia},
author = {Collins, Nick},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {beat tracking,contrary,musical agent,nime10,stream analysis},
number = {Nime},
pages = {125--129},
title = {{Contrary Motion : An Oppositional Interactive Music System}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}125.pdf},
year = {2010}
}
@inproceedings{nime2014:jsokolovskis,
abstract = {This paper presents a method for locating the position of a strike on an acoustic drumhead. Near-field optical sensors were installed underneath the drumhead of a commercially available snare drum. By implementing time difference of arrival (TDOA) algorithm accuracy within 2cm was achieved in approximating the location of strikes. The system can be used for drum performance analysis, timbre analysis and can form a basis for an augmented drum performance system.},
address = {London, United Kingdom},
author = {Sokolovskis, Janis and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {70--73},
publisher = {Goldsmiths, University of London},
title = {{Optical Measurement of Acoustic Drum Strike Locations}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}436.pdf},
year = {2014}
}
@inproceedings{nime2014:rkleinberger,
abstract = {Vocal Vibrations is a new project by the Opera of the Future group at the MIT Media Lab that seeks to engage the public in thoughtful singing and vocalizing, while exploring the relationship between human physiology and the resonant vibrations of the voice. This paper describes the motivations, the technical implementation, and the experience design of the Vocal Vibrations public installation. This installation consists of a space for reflective listening to a vocal composition (the Chapel) and an interactive space for personal vocal exploration (the Cocoon). In the interactive experience, the participant also experiences a tangible exteriorization of his voice by holding the ORB, a handheld device that translates his voice and singing into tactile vibrations. This installation encourages visitors to explore the physicality and expressivity of their voices in a rich musical context.},
address = {London, United Kingdom},
author = {Holbrow, Charles and Jessop, Elena and Kleinberger, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {431--434},
publisher = {Goldsmiths, University of London},
title = {{Vocal Vibrations: A Multisensory Experience of the Voice}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}378.pdf},
year = {2014}
}
@inproceedings{Essl2010a,
address = {Sydney, Australia},
author = {Essl, Georg and Rohs, Michael and Kratz, Sven},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {force,impact,mobile,mobile phone,multi-touch,nime10,pressure},
number = {Nime},
pages = {182--185},
title = {{Use the Force (or something) - Pressure and Pressure - Like Input for Mobile Music Performance}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}182.pdf},
year = {2010}
}
@inproceedings{nime2014:fheller,
abstract = {Although an analog technology, many DJs still value the turntable as an irreplaceable performance tool. Digital vinyl systems combine the distinct haptic nature of the analog turntable with the advantages of digital media. They use special records containing a digital timecode which is then processed by a computer and mapped to properties like playback speed and direction. These records, however, are generic and, in contrast to traditional vinyl, do not provide visual cues representing the structure of the track. We present a system that augments the timecode record with a visualization of song information such as artist, title, and track length, but also with a waveform that allows to visually navigate to a certain beat. We conducted a survey examining the acceptance of such tools in the DJ community and conducted a user study with professional DJs. The system was widely accepted as a tool in the DJ community and received very positive feedback during observational mixing sessions with four professional DJs.},
address = {London, United Kingdom},
author = {Heller, Florian and Borchers, Jan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {66--69},
publisher = {Goldsmiths, University of London},
title = {{Visualizing Song Structure on Timecode Vinyls}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}290.pdf},
year = {2014}
}
@inproceedings{Estrada2007,
address = {New York City, NY, United States},
author = {Pereira, Rui},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {interface,live-visuals,loop,nime07,real-time,video},
pages = {411--414},
title = {{Loop-R : Real-Time Video Interface}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}411.pdf},
year = {2007}
}
@inproceedings{Patricio:2012,
abstract = {This article proposes a wireless handheld multimedia digital instrument, which allows one to compose and perform digital music for films in real-time. Not only does it allow the performer and the audience to follow the film images in question, but also the relationship between the gestures performed and the sound generated. Furthermore, it allows one to have an effective control over the sound, and consequently achieve great musical expression. In addition, a method for calibrating the multimedia digital instrument, devised to overcome the lack of a reliable reference point of the accelerometer and a process to obtain a video score are presented. This instrument has been used in a number of concerts (Portugal and Brazil) so as to test its robustness.},
address = {Ann Arbor, Michigan},
author = {Patr{\'{i}}cio, Pedro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
publisher = {University of Michigan},
title = {{MuDI - Multimedia Digital Instrument for Composing and Performing Digital Music for Films in Real-time}},
year = {2012}
}
@inproceedings{Freed2008,
address = {Genoa, Italy},
author = {Freed, Adrian},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {agile development,augmented,conductive fabric,conductive heatshrink tubing,nime08,piezoresistive fabric,rapid prototyping},
pages = {107--112},
title = {{Application of new Fiber and Malleable Materials for Agile Development of Augmented Instruments and Controllers}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}107.pdf},
year = {2008}
}
@inproceedings{D'Alessandro2011,
address = {Oslo, Norway},
author = {D'Alessandro, Nicolas and Calderon, Roberto and M{\"{u}}ller, Stefanie},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {agent,architecture,collaboration,figure 1,installation,instrument,interactive fabric,light,mo-,movements in the installation,space and,tion,voice synthesis},
number = {June},
pages = {132--135},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{ROOM {\#} 81 - Agent-Based Instrument for Experiencing Architectural and Vocal Cues}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}132.pdf},
year = {2011}
}
@inproceedings{Fasciani:2013,
abstract = {ABSTRACT: Mapping gestures to digital musical instrument parameters is nottrivial when the dimensionality of the sensor-captured data is high and themodel relating the gesture to sensor data is unknown. In these cases, afront-end processing system for extracting gestural information embedded in thesensor data is essential. In this paper we propose an unsupervised offlinemethod that learns how to reduce and map the gestural data to a genericinstrument parameter control space. We make an unconventional use of theSelf-Organizing Maps to obtain only a geometrical transformation of thegestural data, while dimensionality reduction is handled separately. Weintroduce a novel training procedure to overcome two main Self- Organizing Mapslimitations which otherwise corrupt the interface usability. As evaluation, weapply this method to our existing Voice-Controlled Interface for musicalinstruments, obtaining sensible usability improvements.},
address = {Daejeon, Republic of Korea},
author = {Fasciani, Stefano and Wyse, Lonce},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Gestural Controller,Multi Dimensional Control,Self-Organizing Maps,Unsupervised Gesture Mapping,Voice Control},
pages = {507--512},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Self-Organizing Gesture Map for a Voice-Controlled Instrument Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Platz2008,
address = {Genoa, Italy},
author = {Ananya, Misra and Essl, Georg and Rohs, Michael},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {microphone,mobile music making,mobile-stk,nime08},
pages = {185--188},
title = {{Microphone as Sensor in Mobile Phone Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}185.pdf},
year = {2008}
}
@inproceedings{Cook2001,
address = {Seattle, WA, United States},
author = {Cook, Perry R},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
file = {::},
keywords = {artistic interfaces,musical control},
pages = {3--6},
title = {{Principles for Designing Computer Music Controllers}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}003.pdf},
year = {2001}
}
@inproceedings{Verplank2001,
address = {Seattle, WA, United States},
author = {Verplank, Bill and Sapp, Craig and Mathews, Max},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
file = {::},
keywords = {chi technology,courses,input devices,music controllers},
pages = {7--10},
title = {{A Course on Controllers}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}007.pdf},
year = {2001}
}
@inproceedings{Hindle:2013,
abstract = {Audience participation in computer music has long been limited byresources such as sensor technology or the material goods necessary toshare such an instrument. A recent paradigm is to take advantageof the incredible popularity of the smart-phone, a pocket sizedcomputer, and other mobile devices, to provide the audience aninterface into a computer music instrument. In this paper we discuss amethod of sharing a computer music instrument's interface with anaudience to allow them to interact via their smartphone. We propose amethod that is relatively cross-platform and device-agnostic, yetstill allows for a rich user-interactive experience. By emulating acaptive-portal or hotspot we reduce the adoptability issues and configurationproblems facing performers and their audience. We share ourexperiences with this system, as well as an implementation of thesystem itself.},
address = {Daejeon, Republic of Korea},
author = {Hindle, Abram},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Adoption,Audience Interaction,Captive Portal,Hotspot,Multi-User,Smartphone,Wifi},
pages = {174--179},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{SWARMED: Captive Portals, Mobile Devices, and Audience Participation in Multi-User Music Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Tokunaga2013,
abstract = {We are exploring the design and implementation of artificial expressions,kinetic audio-visual representations of real-time physiological data whichreflect emotional and cognitive state. In this work we demonstrate a prototype,the Enactive Mandala, which maps real-time EEG signals to modulate ambientmusic and animated visual music. The design draws inspiration from the visualmusic of the Whitney brothers as well as traditional meditative practices.Transparent real-time audio-visual feedback ofbrainwave qualities supports intuitive insight into the connection betweenthoughts and physiological states. Our method is constructive: by linkingphysiology with an dynamic a/v display, and embedding the human-machine systemin the social contexts that arise in real-time play, we hope to seed new, andas yet unknown forms, of non-verbal communication, or ``artificialexpressions''.},
address = {Daejeon},
author = {Tokunaga, Tomohiro and Lyons, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Artificial Expressions,BCI,Brain-computer Interfaces,EEG,NIME,Sonification,Visual Music,Visualization},
pages = {118--119},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Enactive Mandala: Audio-visualizing Brain Waves}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Morris2007,
address = {New York City, NY, United States},
author = {Morris, Simon},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {425},
title = {{Musique Concrete : Transforming Space , Sound and the City Through Skateboarding}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}425.pdf},
year = {2007}
}
@inproceedings{Nishimoto2003,
address = {Montreal, QC, Canada},
author = {Nishimoto, Kazushi and Oshima, Chika and Miyagawa, Yohei},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {164--169},
title = {{Why Always Versatile? Dynamically Customizable Musical Instruments Facilitate Expressive Performances}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}164.pdf},
year = {2003}
}
@inproceedings{Pritchard2006,
abstract = {We describe the implementation of an environment for Gesturally-Realized Audio, Speech and Song Performance (GRASSP), which includes a glove-based interface, a mapping/training interface, and a collection of Max/MSP/Jitter bpatchers that allow the user to improvise speech, song, sound synthesis, sound processing, sound localization, and video processing. The mapping/training interface provides a framework for performers to specify by example the mapping between gesture and sound or video controls. We demonstrate the effectiveness of the GRASSP environment for gestural control of musical expression by creating a gesture-to-voice system that is currently being used by performers. },
address = {Paris, France},
author = {Pritchard, Bob and Fels, Sidney S},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Cyberglove,Glove-Talk,Jitter,Max/MSP,Polhemus,Speech synthesis,UBC Toolbox,gesture control,parallel formant speech synthesizer,sound diffusion},
pages = {272--276},
title = {{GRASSP: Gesturally-Realized Audio, Speech and Song Performance}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}272.pdf},
year = {2006}
}
@inproceedings{Park:2012,
abstract = {This paper describes an interactive gestural microphone for vocal performance named Voicon. Voicon is a non-invasive and gesture-sensitive microphone which allows vocal performers to use natural gestures to create vocal augmentations and modifications by using embedded sensors in a microphone. Through vocal augmentation and modulation, the performers can easily generate desired amount of the vibrato and achieve wider vocal range. These vocal en-hancements will deliberately enrich the vocal performance both in its expressiveness and the dynamics. Using Voicon, singers can generate additional vibrato, control the pitch and activate customizable vocal effect by simple and intuitive gestures in live and recording context.},
address = {Ann Arbor, Michigan},
author = {Park, Yongki and Heo, Hoon and Lee, Kyogu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Gesture,Microphone,Performance In-terface,Vocal Performance},
month = {may},
publisher = {University of Michigan},
title = {{Voicon: An Interactive Gestural Microphone For Vocal Performance}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}199.pdf},
year = {2012}
}
@inproceedings{Demey2008,
abstract = {The Musical Synchrotron is a software interface that connects wireless motion sensors to a real-time interactive environment (Pure Data, Max/MSP). In addition to the measurement of movement, the system provides audio playback and visual feedback. The Musical Synchrotron outputs a score with the degree in which synchronization with the presented music is successful. The interface has been used to measure how people move in response to music. The system was used for experiments at public events. },
address = {Genoa, Italy},
author = {Demey, Michiel and Leman, Marc and Bossuyt, Frederick and Vanfleteren, Jan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Wireless sensors,embodied music cognition,music and movement,social interaction,tempo perception},
pages = {372--373},
title = {{The Musical Synchrotron : Using Wireless Motion Sensors to Study How Social Interaction Affects Synchronization with Musical Tempo}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}372.pdf},
year = {2008}
}
@inproceedings{Park:2013a,
abstract = {In this paper, we designed a sound effect device, which was applicable forspray paint art process. For the applicability research of the device, wedesigned a prototype which had a form not far off the traditional spray cans,using Arduino and various sensors. Through the test process of the prototype,we verified the elements that would be necessary to apply our newly designeddevice to real spray paint art activities. Thus we checked the possibility ofvarious musical expressions by expanding the functions of the designed device.},
address = {Daejeon, Republic of Korea},
author = {Park, Gibeom and Lee, Kyogu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Arduino,Pure Data,Sound effect device,Spray paint art},
pages = {65--68},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Sound Spray - can-shaped sound effect device}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{nime2014:knymoen1,
abstract = {We present Funky Sole Music, a musical interface employing a sole embedded with three force sensitive resistors in combination with a novel algorithm for continuous movement classification. A heuristics-based music engine has been implemented, allowing users to control high-level parameters of the musical output. This provides a greater degree of control to users without musical expertise compared to what they get with traditional media playes. By using the movement classification result not as a direct control action in itself, but as a way to change mapping spaces and musical sections, the control possibilities offered by the simple interface are greatly increased.},
address = {London, United Kingdom},
author = {Nymoen, Kristian and Song, Sichao and Hafting, Yngve and Torresen, Jim},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {299--302},
publisher = {Goldsmiths, University of London},
title = {{Funky Sole Music: Gait Recognition and Adaptive Mapping}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}289.pdf},
year = {2014}
}
@inproceedings{Orio2001,
address = {Seattle, WA, United States},
author = {Orio, Nicola and Schnell, Norbert and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
file = {::},
keywords = {gestural control,input device design,interactive systems},
pages = {15--18},
title = {{Input Devices for Musical Expression : Borrowing Tools from HCI}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}015.pdf},
year = {2001}
}
@inproceedings{Batula2013,
abstract = {We present a system which allows an adult-sized humanoid to determine whetheror not it is correctly playing a pitched percussive instrument to produce adesired sound. As hu- man musicians utilize sensory feedback to determine ifthey are successfully using their instruments to generate certain pitches,robot performers should be capable of the same feat. We present a noteclassification algorithm that uses auditory and haptic feedback to decide if anote was well- or poorly-struck. This system is demonstrated using Hubo, anadult-sized humanoid, which has been enabled to actu- ate pitched pipes usingmallets. We show that, with this system, Hubo is able to determine whether ornot a note was played correctly.},
address = {Daejeon, Republic of Korea},
author = {Batula, Alyssa and Colacot, Manu and Grunberg, David and Kim, Youngmoo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Musical robots,auditory feedback,haptic feedback,humanoids},
pages = {295--300},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Using Audio and Haptic Feedback to Improve Pitched Percussive Instrument Performance in Humanoids}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Shear:2012,
abstract = {The Electromagnetically Sustained Rhodes Piano is an orig- inal Rhodes Piano modified to provide control over the amplitude envelope of individual notes through aftertouch pressure. Although there are many opportunities to shape the amplitude envelope before loudspeaker amplification, they are all governed by the ever-decaying physical vibra- tions of the tone generating mechanism. A single-note proof of concept for electromagnetic control over this vibrating mechanism was presented at NIME 2011.
In the past year, virtually every aspect of the system has been improved. We use a different vibration sensor that is immune to electromagnetic interference, thus eliminat- ing troublesome feedback. For control, we both reduce cost and gain continuous position sensing throughout the entire range of key motion in addition to aftertouch pressure. Finally, the entire system now fits within the space constraints presented by the original piano, allowing it to be installed on adjacent notes.},
address = {Ann Arbor, Michigan},
author = {Shear, Greg and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Rhodes,electromagnetic,feedback,mechanical synthesizer,piano,sustain},
publisher = {University of Michigan},
title = {{Further Developments in the Electromagnetically Sustained Rhodes Piano}},
year = {2012}
}
@inproceedings{Moody2007,
abstract = {This paper describes the Ashitaka audiovisual instrumentand the process used to develop it. The main idea guidingthe design of the instrument is that motion can be used toconnect audio and visuals, and the first part of the paperconsists of an exploration of this idea. The issue of mappings is raised, discussing both audio-visual mappings andthe mappings between the interface and synthesis methods.The paper concludes with a detailed look at the instrumentitself, including the interface, synthesis methods, and mappings used.},
address = {New York City, NY, United States},
author = {Moody, Niall and Fells, Nick and Bailey, Nicholas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {audiovisual,instrument,mappings,nime07,synchresis,x3d},
pages = {148--153},
title = {{Ashitaka : An Audiovisual Instrument}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}148.pdf},
year = {2007}
}
@inproceedings{nime2014:jcomajuncosas,
abstract = {According to the tradition, music ensembles are usually lead by a conductor who is the responsible to coordinate and guide the group under a specific musical criteria. Similarly, computer ensembles resort to a conductor to keep the synchronization and structural coordination of the performance, often with the assistance of software. Achieving integration and coherence in a networked performance, however, can be challenging in certain scenarios. This is the case for configurations with a high degree of mutual interdependence and shared control. This paper focuses on the design strategies for developing a software based conductor assistant for collective instruments. We propose a novel conductor dimension space representation for collective instruments, which takes into account both its social and structural features. We present a case study of a collective instrument implementing a software conductor. Finally, we discuss the implications of human and machine conduction schemes in the context of the proposed dimension space.},
address = {London, United Kingdom},
author = {Comajuncosas, Josep and Guaus, Enric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {513--516},
publisher = {Goldsmiths, University of London},
title = {{Conducting Collective Instruments : A Case Study}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}448.pdf},
year = {2014}
}
@inproceedings{Malloch2007,
address = {New York City, NY, United States},
author = {Malloch, Joseph and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {digital musical instrument,families of,gestural controller,nime07},
pages = {66--69},
title = {{The T-Stick : From Musical Interface to Musical Instrument}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}066.pdf},
year = {2007}
}
@inproceedings{nime2014:lfyfe,
abstract = {The Nexus Data Exchange Format (NDEF) is an Open Sound Control (OSC) namespace specification designed to make connection and message management tasks easier for OSC-based networked performance systems. New extensions to the NDEF namespace improve both connection and message management between OSC client and server nodes. Connection management between nodes now features human-readable labels for connections and a new message exchange for pinging connections to determine their status. Message management now has improved namespace synchronization via a message count exchange and by the ability to add, remove, and replace messages on connected nodes.},
address = {London, United Kingdom},
author = {Fyfe, Lawrence and Tindale, Adam and Carpendale, Sheelagh},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {343--346},
publisher = {Goldsmiths, University of London},
title = {{Extending the Nexus Data Exchange Format (NDEF) Specification}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}368.pdf},
year = {2014}
}
@inproceedings{nime2014:slui,
abstract = {Pop music jamming on the keyboard requires massive music knowledge. Musician needs to understand and memorize the behavior of each chord in different keys. However, most simple pop music follows a common chord progression pattern. This pattern applies to most simple pop music on all the 12 keys. We designed an app that can reduce the difficulty of music jamming on the keyboard by using this pattern. The app displays the current chord in the Roman numeral and suggests the expected next chord in an easy to understand way on a smartphone. This work investigates into the human computer interaction perspective of music performance. We use a smartphone app as a bridge, which assists musician to react faster in music jamming by transforming the complex music knowledge into a simple, unified and easy to understand format. Experiment result shows that this app can help the non-keyboardist musician to learn pop music jamming. It also shows that the app is useful to assist keyboardist in making key transpose and playing music in the key with many sharps and flats. We will use the same interface design to guide user on playing other chord progressions such as the jazz chord progression.},
address = {London, United Kingdom},
author = {Lui, Simon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {98--101},
publisher = {Goldsmiths, University of London},
title = {{A Real Time Common Chord Progression Guide on the Smartphone for Jamming Pop Song on the Music Keyboard}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}275.pdf},
year = {2014}
}
@inproceedings{Fyans2010,
abstract = {Drawing on a model of spectator understanding of error inperformance in the literature, we document a qualitativeexperiment that explores the relationships between domainknowledge, mental models, intention and error recognitionby spectators of performances with electronic instruments.Participants saw two performances with contrasting instruments, with controls on their mental model and understanding of intention. Based on data from a subsequent structured interview, we identify themes in participants' judgements and understanding of performance and explanationsof their spectator experience. These reveal both elementsof similarity and difference between the two performances,instruments and between domain knowledge groups. Fromthese, we suggest and discuss implications for the design ofnovel performative interactions with technology.},
address = {Sydney, Australia},
author = {Fyans, A Cavan and Gurevich, Michael and Stapleton, Paul},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {error,intention,mental model,nime10,qualitative,spectator},
pages = {451--454},
title = {{Examining the Spectator Experience}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}451.pdf},
year = {2010}
}
@inproceedings{McGlynn:2012,
abstract = {This paper contends that the development of expressive performance interfaces using multi-touch technology has been hindered by an over-reliance upon GUI paradigms. Despite offering rich and robust data output and multiple ways to interpret it, approaches towards using multi-touch technology in digit al musical inst rument design have been markedly conservative, showing a strong tendency towards modeling existing hardware. This not only negates many of the benefits of multi-touch technology but also creates specific difficulties in the context of live music performance. A case study of two other interface types that have seen considerable musical use -- the XY pad and button grid -- illustrates the manner in which the implicit characteristics of a device determine the conditions under which it will favorably perform. Accordingly, this paper proposes an alternative approach to multi-touch which emp hasizes the imp licit strengths of the technology and establishes a philosophy of design around them. Finally, we introduce two toolkits currently being used to assess the validity of this approach.},
address = {Ann Arbor, Michigan},
author = {McGlynn, Patrick and Lazzarini, Victor and Delap, Gordon and Chen, Xiaoyu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {GUIs,Multi-touch,controllers,gesture,mapping,perceptual {\&} cognitive issues,physical interfaces},
publisher = {University of Michigan},
title = {{Recontextualizing the Multi-touch Surface}},
year = {2012}
}
@inproceedings{Berthaut:2013,
abstract = {Digital musical instruments bring new possibilities for musical performance.They are also more complex for the audience to understand, due to the diversityof their components and the magical aspect of the musicians' actions whencompared to acoustic instruments. This complexity results in a loss of livenessand possibly a poor experience for the audience. Our approach, called Rouages,is based on a mixed-reality display system and a 3D visualization application.It reveals the mechanisms of digital musical instruments by amplifyingmusicians' gestures with virtual extensions of the sensors, by representingthe sound components with 3D shapes and specific behaviors and by showing theimpact ofmusicians gestures on these components. We believe that Rouages opens up newperspectives to help instrument makers and musicians improve audienceexperience with their digital musical instruments.},
address = {Daejeon, Republic of Korea},
author = {Berthaut, Florent and Marshall, Mark T and Subramanian, Sriram and Hachet, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {3D interface,digital musical instruments,mappings,mixed-reality,rouages},
month = {may},
pages = {164--169},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Rouages: Revealing the Mechanisms of Digital Musical Instruments to the Audience}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{fmorreale:2014,
abstract = {This paper presents MINUET, a framework for musical interface design grounded in the experience of the player. MINUET aims to provide new perspectives on the design of musical interfaces, referred to as a general term that comprises digital musical instruments and interactive installations. The ultimate purpose is to reduce the complexity of the design space emphasizing the experience of the player. MINUET is structured as a design process consisting of two stages: goal and specifications. The reliability of MINUET is tested through a systematic comparison with the related work and through a case study. To this end, we present the design and prototyping of Hexagon, a new musical interface with learning purposes.},
address = {London, United Kingdom},
author = {Morreale, Fabio and Angeli, Antonella De and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Morreale, Angeli, O'Modhrain - 2014 - Musical Interface Design An Experience-oriented Framework.pdf:pdf},
month = {jun},
pages = {467--472},
publisher = {Goldsmiths, University of London},
title = {{Musical Interface Design: An Experience-oriented Framework}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}437.pdf},
year = {2014}
}
@inproceedings{Freed2008,
abstract = {The paper introduces new fiber and malleable materials,including piezoresistive fabric and conductive heat-shrinktubing, and shows techniques and examples of how they maybe used for rapid prototyping and agile development of musicalinstrument controllers. New implementations of well-knowndesigns are covered as well as enhancements of existingcontrollers. Finally, two new controllers are introduced that aremade possible by these recently available materials andconstruction techniques.},
address = {Genoa, Italy},
author = {Freed, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Agile Development,Conductive fabric,Piezoresistive fabric,Rapid Prototyping,augmented instruments.,conductive heatshrink tubing},
pages = {107--112},
title = {{Application of new Fiber and Malleable Materials for Agile Development of Augmented Instruments and Controllers}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}107.pdf},
year = {2008}
}
@inproceedings{Baird2003,
address = {Montreal, QC, Canada},
author = {Baird, Kevin C},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {211--212},
title = {{Multi-Conductor: An Onscreen Polymetrical Conducting and Notation Display System}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}211.pdf},
year = {2003}
}
@inproceedings{Weinberg2006,
address = {Paris, France},
author = {Weinberg, Gil and Thatcher, Travis},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {1,background and motivations,biological research,interactive auditory display,neural patterns,scholars are,sonification,with new developments in},
pages = {246--249},
title = {{Interactive Sonification of Neural Activity}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}246.pdf},
year = {2006}
}
@inproceedings{Hsu2007,
abstract = {In previous publications (see for example [2] and [3]), we described an interactive music system, designed to improvise with saxophonist John Butcher; our system analyzes timbral and gestural features in real-time, and uses this information to guide response generation. This paper overviews our recent work with the system's interaction management component (IMC). We explore several options for characterizing improvisation at a higher level, and managing decisions for interactive performance in a rich timbral environment. We developed a simple, efficient framework using a small number of features suggested by recent work in mood modeling in music. We describe and evaluate the first version of the IMC, which was used in performance at the Live Algorithms for Music (LAM) conference in December 2006. We touch on developments on the system since LAM, and discuss future plans to address perceived shortcomings in responsiveness, and the ability of the system to make long-term adaptations. },
address = {New York City, NY, United States},
author = {Hsu, William},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Interactive music systems,free improvisation.,timbral analysis},
pages = {367--370},
title = {{Design Issues in Interaction Modeling for Free Improvisation}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}367.pdf},
year = {2007}
}
@inproceedings{Hayafuchi2008,
abstract = {This research aims to develop a wearable musical interfacewhich enables to control audio and video signals by usinghand gestures and human body motions. We have beendeveloping an audio-visual manipulation system that realizes tracks control, time-based operations and searching fortracks from massive music library. It aims to build an emotional and affecting musical interaction, and will providea better method of music listening to people. A sophisticated glove-like device with an acceleration sensor and several strain sensors has been developed. A realtime signalprocessing and musical control are executed as a result ofgesture recognition. We also developed a stand-alone device that performs as a musical controller and player at thesame time. In this paper, we describe the development of acompact and sophisticated sensor device, and demonstrateits performance of audio and video signals control.},
address = {Genoa, Italy},
author = {Hayafuchi, Kouki and Suzuki, Kenji},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Body Motion,Embodied Sound Media,Gestures,Music Controller,Musical Interface},
pages = {241--244},
title = {{MusicGlove : A Wearable Musical Controller for Massive Media Library}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}241.pdf},
year = {2008}
}
@inproceedings{Overholt2001,
address = {Seattle, WA, United States},
author = {Overholt, Dan},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
file = {::},
keywords = {3-d,audio synthesis,effects algorithms,interface,musical controller,real-time expression,sculptable surface,signal processing,tangible interface},
pages = {38--41},
title = {{The MATRIX : A Novel Controller for Musical Expression}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}038.pdf},
year = {2001}
}
@inproceedings{Zappi2010,
address = {Sydney, Australia},
author = {Zappi, Victor and Brogni, Andrea and Caldwell, Darwin},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {con-,glove device,music controller,nime10,osc,virtual reality},
number = {Nime},
pages = {297--302},
title = {{OSC Virtual Controller}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}297.pdf},
year = {2010}
}
@inproceedings{Parson:2012,
abstract = {With the advent of high resolution digital video projection and high quality spatial sound systems in modern planetariums, the planetarium can become the basis for a unique set of virtual musical instrument capabilities that go well beyond packaged multimedia shows. The dome, circular speaker and circular seating arrangements provide means for skilled composers and performers to create a virtual reality in which attendees are immersed in the composite instrument. This initial foray into designing an audio-visual computerbased instrument for improvisational performance in a planetarium builds on prior, successful work in mapping the rules and state of two-dimensional computer board games to improvised computer music. The unique visual and audio geometries of the planetarium present challenges and opportunities. The game tessellates the dome in mobile, colored hexagons that emulate both atoms and musical scale intervals in an expanding universe. Spatial activity in the game maps to spatial locale and instrument voices in the speakers, in essence creating a virtual orchestra with a string section, percussion section, etc. on the dome. Future work includes distribution of game play via mobile devices to permit attendees to participate in a performance. This environment is open-ended, with great educational and aesthetic potential.},
address = {Ann Arbor, Michigan},
author = {Parson, Dale and Reed, Phillip},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {aleatory music,algorithmic improvisation,computer game,planetarium},
publisher = {University of Michigan},
title = {{The Planetarium as a Musical Instrument}},
year = {2012}
}
@inproceedings{Kessous2003,
address = {Montreal, QC, Canada},
author = {Kessous, Lo{\"{i}}c and Arfib, Daniel},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {140--145},
title = {{Bimanuality in Alternate Musical Instruments}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}140.pdf},
year = {2003}
}
@inproceedings{Levin2005a,
address = {Vancouver, BC, Canada},
author = {Levin, Golan and Lieberman, Zachary},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
number = {2},
pages = {115--120},
title = {{Sounds from Shapes: Audiovisual Performance with Hand Silhouette Contours in The Manual Input Sessions}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}115.pdf},
year = {2005}
}
@inproceedings{Sinyor2005,
address = {Vancouver, BC, Canada},
author = {Sinyor, Elliot and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {42--45},
title = {{Gyrotyre : A dynamic hand-held computer-music controller based on a spinning wheel}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}042.pdf},
year = {2005}
}
@inproceedings{Bukvic:2012,
abstract = {In the following paper we propose a new tiered granularity approach to developing modules or abstractions in the Pd- L2Ork visual multimedia programming environment with the specific goal of devising creative environments that scale their educational scope and difficulty to encompass several stages within the context of primary and secondary (K-12) education. As part of a preliminary study, the team designed modules targeting 4th and 5th grade students, the primary focus being exploration of creativity and collaborative learning. The resulting environment infrastructure - coupled with the Boys {\&} Girls Club of Southwest Virginia Satellite Linux Laptop Orchestra - offers opportunities for students to design and build original instruments, master them through a series of rehearsals, and ultimately utilize them as part of an ensemble in a performance of a predetermined piece whose parameters are coordinated by instructor through an embedded networked module. The ensuing model will serve for the assessment and development of a stronger connection with content-area standards and the development of creative thinking and collaboration skills.},
address = {Ann Arbor, Michigan},
author = {Bukvic, Ivica and Baum, Liesl and Layman, Bennett and Woodard, Kendall},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Education,Granular,K-12,L2Ork,Learning Objects,PdL2Ork},
publisher = {University of Michigan},
title = {{Granular Learning Objects for Instrument Design and Collaborative Performance in K-12 Education}},
year = {2012}
}
@inproceedings{Schmeder2010,
address = {Sydney, Australia},
author = {Schmeder, Andrew and Freed, Adrian},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {gesture signal processing,nime10,support vector,touch sensor},
number = {Nime},
pages = {244--249},
title = {{Support Vector Machine Learning for Gesture Signal Estimation with a Piezo-Resistive Fabric Touch Surface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}244.pdf},
year = {2010}
}
@inproceedings{Roma2008,
address = {Genoa, Italy},
author = {Roma, Gerard and Xamb{\'{o}}, Anna},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {interaction techniques,musical performance,nime08,tabletop interface,tangible interface},
pages = {249--252},
title = {{A Tabletop Waveform Editor for Live Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}249.pdf},
year = {2008}
}
@inproceedings{Soria:2013,
abstract = {This work presents a general framework method for cre-ating spatialization systems focused on electroacoustic andacousmatic music performance and creation. Although weused the logistic equation as orbit generator, any dynami-cal system could be suitable. The main idea lies on generating vectors of Rn with entriesfrom data series of di{\_}x000B{\_}erent orbits from an speci{\_}x000C{\_}c dynami-cal system. Such vectors will be called system vectors. Ourproposal is to create ordered paths between those pointsor system vectors using the Splines Quark library by Felix,1which allow us to generate smooth curves joining the points.Finally, interpolating that result with a {\_}x000C{\_}xed sample value,we are able to obtain speci{\_}x000C{\_}c and independent multidimen-sional panning trajectories for each speaker array and forany number of sound sources.Our contribution is intended to be at the very root of the compositionalprocess giving to the creator a method for exploring new ways for spatialsound placement over time for a wide range of speakers ar-rangements. The advantage of using controlled chaotic dy-namical systems like the logistic equation, lies on the factthat the composer can freely and consciously choose be-tween stable or irregular behaviour for the orbits that willgenerate his/her panning trajectories. Besides, with the useof isometries, it is possible to generate di{\_}x000B{\_}erent related or-bits with one single evaluation of the system. The use ofthe spline method in SuperCollider allows the possibilityof joining and relating those values from orbits into a wellde{\_}x000C{\_}ned and coherent general system. Further research willinclude controlling synthesis parameters in the same waywe created panning trajectories.},
address = {Daejeon, Republic of Korea},
author = {Soria, Edmar and Morales-Manzanares, Roberto},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {NIME,chaos,dynamical systems,spatialization},
pages = {79--83},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Multidimensional sound spatialization by means of chaotic dynamical systems}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Headlee2010,
address = {Sydney, Australia},
author = {Headlee, Kimberlee and Koziupa, Tatyana and Siwiak, Diana},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {biomusic,collaborative,expressive,hci,interactive,interactivity design,interface for musical expression,multimodal,musical mapping strategies,nime10,performance,sonification},
number = {Nime},
pages = {423--426},
title = {{Sonic Virtual Reality Game : How Does Your Body Sound ?}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}423.pdf},
year = {2010}
}
@inproceedings{Senturk:2012,
abstract = {Meaning crossword of sound, Crossole is a musical meta- instrument where the music is visualized as a set of virtual blocks that resemble a crossword puzzle. In Crossole, the chord progressions are visually presented as a set of virtual blocks. With the aid of the Kinect sensing technology, a per- former controls music by manipulating the crossword blocks using hand movements. The performer can build chords in the high level, traverse over the blocks, step into the low level to control the chord arpeggiations note by note, loop a chord progression or map gestures to various processing algorithms to enhance the timbral scenery.},
address = {Ann Arbor, Michigan},
author = {Sent{\"{u}}rk, Sertan and Lee, Sang Won and Sastry, Avinash and Daruwalla, Anosh and Weinberg, Gil},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Kinect,body gesture,chord progression,meta-instrument},
publisher = {University of Michigan},
title = {{Crossole: A Gestural Interface for Composition, Improvisation and Performance using Kinect}},
year = {2012}
}
@inproceedings{Poepel2005,
address = {Vancouver, BC, Canada},
author = {Poepel, Cornelius},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {228--231},
title = {{On Interface Expressivity: A Player-Based Study}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}228.pdf},
year = {2005}
}
@inproceedings{Schacher2009,
address = {Pittsburgh, PA, United States},
author = {Schacher, Jan C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {adaptive mapping,audience engagement,interaction,learning,machine,nime09},
pages = {286--289},
title = {{Action and Perception in Interactive Sound Installations : An Ecological Approach}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}286.pdf},
year = {2009}
}
@inproceedings{Cerqueira:2013,
abstract = {SoundCraft is a framework that enables real-time data gathering from aStarCraft 2 game to external software applications, allowing for musicalinterpretation of the game's internal structure and strategies in novel ways.While players battle each other for victory within the game world, a customStarCraft 2 map collects and writes out data about players' decision-making,performance, and current focus on the map. This data is parsed and transmittedover Open Sound Control (OSC) in real-time, becoming the source for thesoundscape that accompanies the player's game. Using SoundCraft, we havecomposed a musical work for two em StarCraft 2 players, entitled GG Music. Thispaper details the technical and aesthetic development of SoundCraft, includingdata collection and sonic mapping. Please see the attached video file for a performance of GG Music using theSoundCraft framework.},
address = {Daejeon, Republic of Korea},
author = {Cerqueira, Mark and Salazar, Spencer and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {StarCraft 2,interactive game music,interactive sonification},
pages = {243--247},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{SoundCraft: Transducing StarCraft 2}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Yerkes2010,
address = {Sydney, Australia},
author = {Yerkes, Karl and Shear, Greg and Wright, Matthew},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {dial,diy,encoder,hard drive,ing,inherent dynamics,nime10,re-purposed,scratch-,turntable},
number = {Nime},
pages = {150--155},
title = {{Disky : a DIY Rotational Interface with Inherent Dynamics}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}150.pdf},
year = {2010}
}
@inproceedings{Burt2010,
address = {Sydney, Australia},
author = {Burt, Warren},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {nime10},
number = {June},
pages = {15--18},
title = {{Packages for ArtWonk : New Mathematical Tools for Composers}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}493.pdf},
year = {2010}
}
@inproceedings{Webster2014,
abstract = {This paper introduces the OWL stage effects pedal and aims to present the device within the context of Human Computer Interaction (HCI) research. The OWL is a dedicated, programmable audio device designed to provide an alternative to the use of laptop computers for bespoke audio processing on stage for music performance. By creating a software framework that allows the user to program their own code for the hardware in C++, the OWL project makes it possible to use homemade audio processing on stage without the need for a laptop running a computer music environment such as Pure Data or Supercollider. Moving away from the general-purpose computer to a dedicated audio device means that some of the potential problems and technical complexity of performing with a laptop computer onstage can be avoided, allowing the user to focus more of their attention on the musical performance. Within the format of a traditional guitar 'stomp box', the OWL aims to integrate seamlessly into a guitarist's existing pedal board setup, and in this way presents as an example of a ubiquitous and tangible computing device -a programmable computer designed to fit into an existing mode of musical performance whilst being transparent in use.},
address = {London, United Kingdom},
author = {Webster, Thomas and LeNost, Guillaume and Klang, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {621--624},
publisher = {Goldsmiths, University of London},
title = {{The OWL programmable stage effects pedal: Revising the concept of the on-stage computer for live music performance}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}399.pdf},
year = {2014}
}
@inproceedings{Price2008,
address = {Genoa, Italy},
author = {Price, Robin and Rebelo, Pedro},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {12,an audiovisual installation,as part of the,featuring a prepared,fix 07 festival,hacked,nime08,piece was to engage,radio set and accompanying,the objective of the,users in a,visuals,was created},
pages = {311--314},
title = {{Database and Mapping Design for Audiovisual Prepared Radio Set Installation}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}311.pdf},
year = {2008}
}
@inproceedings{Crevoisier2005,
address = {Vancouver, BC, Canada},
author = {Crevoisier, Alain and Polotti, Pietro},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {97--100},
title = {{Tangible Acoustic Interfaces and their Applications for the Design of New Musical Instruments}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}097.pdf},
year = {2005}
}
@inproceedings{Siwiak2009,
address = {Pittsburgh, PA, United States},
author = {Siwiak, Diana and Berger, Jonathan and Yang, Yao},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {auditory display,music,nime09,sensor},
pages = {153--154},
title = {{Catch Your Breath - Musical Biofeedback for Breathing Regulation}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}153.pdf},
year = {2009}
}
@inproceedings{nime2014:ajensenius,
abstract = {The term 'gesture' has represented a buzzword in the NIME community since the beginning of its conference series. But how often is it actually used, what is it used to describe, and how does its usage here differ from its usage in other fields of study? This paper presents a linguistic analysis of the motion-related terminology used in all of the papers published in the NIME conference proceedings to date (2001-- 2013). The results show that 'gesture' is in fact used in 62 {\%} of all NIME papers, which is a significantly higher percentage than in other music conferences (ICMC and SMC), and much more frequently than it is used in the HCI and biomechanics communities. The results from a collocation analysis support the claim that 'gesture' is used broadly in the NIME community, and indicate that it ranges from the description of concrete human motion and system control to quite metaphorical applications.},
address = {London, United Kingdom},
author = {Jensenius, Alexander Refsum},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {217--220},
publisher = {Goldsmiths, University of London},
title = {{To gesture or Not? An Analysis of Terminology in NIME Proceedings 2001--2013}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}351.pdf},
year = {2014}
}
@inproceedings{Castellano2007,
abstract = {In this paper we describe a system which allows users to use their full-body for controlling in real-time the generation of an expressive audio-visual feedback. The system extracts expressive motion features from the user's full-body movements and gestures. The values of these motion features are mapped both onto acoustic parameters for the real-time expressive rendering of a piece of music, and onto real-time generated visual feedback projected on a screen in front of the user. },
address = {New York City, NY, United States},
author = {Castellano, Ginevra and Bresin, Roberto and Camurri, Antonio and Volpe, Gualtiero},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Expressive interaction; multimodal environments; i},
pages = {390--391},
title = {{Expressive Control of Music and Visual Media by Full-Body Movement}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}390.pdf},
year = {2007}
}
@inproceedings{Deleflie2010,
abstract = {The tools for spatial composition typically model just a small subset of the spatial audio cues known to researchers. As composers explore this medium it has become evident that the nature of spatial sound perception is complex. Yet interfaces for spatial composition are often simplistic and the end results can be disappointing. This paper presents an interface that is designed to liberate the composer from thinking of spatialised sound as points in space. Instead, visual images are used to define sound in terms of shape, size and location. Images can be sequenced into video, thereby creating rich and complex temporal soundscapes. The interface offers both the ability to craft soundscapes and also compose their evolution in time. },
address = {Sydney, Australia},
author = {Deleflie, Etienne and Schiemer, Greg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Spatial audio,ambisonics,decorrelation,diffusion.,granular synthesis,surround sound},
pages = {130--135},
title = {{Images as Spatial Sound Maps}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}130.pdf},
year = {2010}
}
@inproceedings{Kamiyama2008,
address = {Genoa, Italy},
author = {Kamiyama, Yusuke and Tanaka, Mai and Tanaka, Hiroya},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {3d sound,musical expression,nime08,sound generating device,sound-field arrangement,system,umbrella},
pages = {352--353},
title = {{Oto-Shigure : An Umbrella-Shaped Sound Generator for Musical Expression}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}352.pdf},
year = {2008}
}
@inproceedings{Mattek2010,
address = {Sydney, Australia},
author = {Mattek, Alison and Freeman, Mark and Humphrey, Eric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {computer-assisted composition,multi-touch interfaces,nime10},
number = {Nime},
pages = {479--480},
title = {{Revisiting Cagean Composition Methodology with a Modern Computational Implementation}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}479.pdf},
year = {2010}
}
@inproceedings{Polfreman2009,
address = {Pittsburgh, PA, United States},
author = {Polfreman, Richard},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {3d graphics,3d graphics are becoming,and scientific,bio-medical,digital audio workstation,from,graphical user-,interfaces,java,max,msp,nime09,once limited to games,pervasive,visualisation},
pages = {226--229},
title = {{FrameWorks 3D : Composition in the Third Dimension}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}226.pdf},
year = {2009}
}
@inproceedings{Vigoda2007,
address = {New York City, NY, United States},
author = {Vigoda, Benjamin and Merrill, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {JamiOki,PureJoy,collaborative performance,electronically-mediated performance,found sound,structured im- provisation},
pages = {321--326},
title = {{JamiOki-PureJoy : A Game Engine and Instrument for Electronically-Mediated Musical Improvisation}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}321.pdf},
year = {2007}
}
@inproceedings{nime2014:jsnyder1,
abstract = {This paper discusses the Birl, an electronic wind instrument developed by the authors. It uses artificial neural nets to apply machine learning to the mapping of fingering systems and embouchure position. The design features of the instrument are described, and the machine learning mapping strategy is discussed.},
address = {London, United Kingdom},
author = {Snyder, Jeff and Ryan, Danny},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {585--588},
publisher = {Goldsmiths, University of London},
title = {{The Birl: An Electronic Wind Instrument Based on an Artificial Neural Network Parameter Mapping Structure}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}540.pdf},
year = {2014}
}
@inproceedings{Clay2007,
abstract = {The starting point for this project is the want to produce amusic controller that could be employed in such a manner thateven lay public could enjoy the possibilities of mobile art. Allof the works that are discussed here are in relation to a newGPS-based controller, the Wrist-Conductor. The works aretechnically based around the synchronizing possibilitiesusing the GPS Time Mark and are aesthetically rooted in worksthat function in an open public space such as a city or a forest.One of the works intended for the controller, China Gates, i sdiscussed here in detail in order to describe how the GPSWrist-Controller is actually used in a public art context. Theother works, CitySonics, The Enchanted Forest and Get a Pot{\&} a Spoon are described briefly in order to demonstrate thateven a simple controller can be used to create a body of works.This paper also addresses the breaking of the media bubblevia the concept of the "open audience", or how mobile art canengage pedestrians as viewers or listeners within public spaceand not remain an isolated experience for performers only.},
address = {New York City, NY, United States},
author = {Clay, Arthur and Majoe, Dennis},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Collaborative Performance,Controller,GPS,Mobile Music},
pages = {242--245},
title = {{The Wrist-Conductor}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}242.pdf},
year = {2007}
}
@inproceedings{Mandelis:2002,
address = {Dublin},
author = {Mandelis, James},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {adaptive interfaces,artificial life,expressivity,hyperinstruments,live performance,motion-to-sound mapping,selective breeding,sound meta-synthesis},
pages = {116--117},
title = {{Adaptive Hyperinstruments: Applying Evolutionary Techniques to Sound Synthesis and Performance}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}116.pdf},
year = {2002}
}
@inproceedings{Everett:2013,
abstract = {This presentation-demonstration discusses the creation of FIRST LIFE, a75-minute mixed media performance for string quartet, live audio processing,live motion capture video, and audience participation utilizing stochasticmodels of chemical data provided by Martha Grover's Research Group at theSchool of Chemical and Biomolecular Engineering at Georgia Institute ofTechnology. Each section of this work is constructed from contingent outcomesdrawn from biochemical research exploring possible early Earth formations oforganic compounds. Audio-video excerpts of the composition will be played during the presentation.Max patches for sonification and for generating stochastic processes will bedemonstrated as well.},
address = {Daejeon, Republic of Korea},
author = {Everett, Steve},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Data-driven composition,live electronics-video,sonification},
pages = {277--278},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Sonifying Chemical Evolution}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{McMillen2008,
address = {Genoa, Italy},
author = {McMillen, Keith A},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {a 40 db dynamic,and balanced,bluetooth sensor bows for,cello and bass,nime08,rapid signal with over,series of properly weighted,the demonstration of a,this provides a robust,viola,violin},
pages = {347--348},
title = {{Stage-Worthy Sensor Bows for Stringed Instruments}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}347.pdf},
year = {2008}
}
@inproceedings{Bryan2011,
address = {Oslo, Norway},
author = {Bryan, Nicholas J and Wang, Ge},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {accelerometer,digital dj,digital scratching,gyro-,mobile music,record player,scope,smartphone,turntable,turntablism,vinyl emulation software},
number = {June},
pages = {179--184},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Two Turntables and a Mobile Phone}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}179.pdf},
year = {2011}
}
@inproceedings{Krzyzaniak2014,
abstract = {This paper describes the implementation of a digital audio / visual feedback system for an extemporaneous dance performance. The system was designed to automatically synchronize aesthetically with the dancers. The performance was premiered at the Slingshot festival in Athens Georgia on March 9, 2013.},
address = {London},
author = {Krzyzaniak, Michael and Akerly, Julie and Mosher, Matthew and Yildirim, Muharrem and Paine, Garth},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {303--306},
publisher = {Goldsmiths, University of London},
title = {{Separation: Short Range Repulsion. Implementation of an Automated Aesthetic Synchronization System for a Dance Performance.}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}279.pdf},
year = {2014}
}
@inproceedings{Pedrosa2009,
address = {Pittsburgh, PA, United States},
author = {Pedrosa, Ricardo and Maclean, Karon E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {comfort,force feedback,music interfaces,nime09,target acquisition,tempo},
pages = {19--24},
title = {{Evaluation of 3D Haptic Target Rendering to Support Timing in Music Tasks}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}019.pdf},
year = {2009}
}
@inproceedings{nime2014:jallison,
abstract = {Mobile music applications are typically quite limiting to musicians, as they either attempt to mimic non-touch screen interfaces or do not offer enough control. Pitch Canvas is a musical interface that was built specifically for the touchscreen. Pitches are laid out in a hexagonal pattern that allow for easy scale, chord, and arpeggiation patterns. Notes are played by touch, but are sustained through continuous movement. Pitch bends can be achieved by passing through the space between the notes. Its current implementation runs only on Apple iPad tablet computers using a libPd to convert user interaction into audio. An iPad overlay offers physical feedback for the circles as well as the pitch bend area between the circles. A performable version of the application has been built, though several active developments allow alternative sonic interpretation of the gestures, enhanced visual response to user interaction, and the ability to control the instrument with multiple devices.},
address = {London, United Kingdom},
author = {Strylowski, Bradley and Allison, Jesse and Guessford, Jesse},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {171--174},
publisher = {Goldsmiths, University of London},
title = {{Pitch Canvas: Touchscreen Based Mobile Music Instrument}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}533.pdf},
year = {2014}
}
@inproceedings{Hornof2007,
address = {New York City, NY, United States},
author = {Hornof, Anthony J and Rogers, Troy and Halverson, Tim},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {299--300},
title = {{EyeMusic : Performing Live Music and Multimedia Compositions with Eye Movements}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}299.pdf},
year = {2007}
}
@inproceedings{Chacin:2012,
abstract = {This paper is an in depth exploration of the fashion object and device, the Play-A-Grill. It details inspirations, socio-cultural implications, technical function and operation, and potential applications for the Play-A-Grill system.},
address = {Ann Arbor, Michigan},
author = {Chacin, Aisen Caro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Digital Music Players,Grills,Hip Hop,Mouth Controllers,Mouth Jewelry,Music Fashion,Rap,and Bone Conduction Hearing.},
publisher = {University of Michigan},
title = {{Play-A-Grill: Music To Your Teeth}},
year = {2012}
}
@inproceedings{Kiser2006,
abstract = {This report presents an interface for musical performance called the spinCycle. spinCycle enables performers to make visual patterns with brightly colored objects on a spinning turntable platter that get translated into musical arrangements in realtime. I will briefly describe the hardware implementation and the sound generation logic used, as well as provide a historical background for the project.},
address = {Paris, France},
author = {Kiser, Spencer},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Color-tracking,interactivity,synesthesia,turntable,visualization},
pages = {75--76},
title = {{spinCycle: a Color-Tracking Turntable Sequencer}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}075.pdf},
year = {2006}
}
@inproceedings{Roberts:2013a,
abstract = {Web technologies provide an incredible opportunity to present new musicalinterfaces to new audiences. Applications written in JavaScript and designed torun in the browser offer remarkable performance, mobile/desktop portability andlongevity due to standardization. Our research examines the use and potentialof native web technologies for musical expression. We introduce two librariestowards this end: Gibberish.js, a heavily optimized audio DSP library, andInterface.js, a GUI toolkit that works with mouse, touch and motion events.Together these libraries provide a complete system for defining musicalinstruments that can be used in both desktop and mobile browsers. Interface.jsalso enables control of remote synthesis applications by including anapplication that translates the socket protocol used by browsers into both MIDIand OSC messages.},
address = {Daejeon, Republic of Korea},
author = {Roberts, Charles and Wakefield, Graham and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {browser-based NIMEs,javascript,mobile devices,web audio,websockets},
month = {may},
pages = {313--318},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{The Web Browser As Synthesizer And Interface}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Roberts:2013a,
abstract = {Web technologies provide an incredible opportunity to present new musicalinterfaces to new audiences. Applications written in JavaScript and designed torun in the browser offer remarkable performance, mobile/desktop portability andlongevity due to standardization. Our research examines the use and potentialof native web technologies for musical expression. We introduce two librariestowards this end: Gibberish.js, a heavily optimized audio DSP library, andInterface.js, a GUI toolkit that works with mouse, touch and motion events.Together these libraries provide a complete system for defining musicalinstruments that can be used in both desktop and mobile browsers. Interface.jsalso enables control of remote synthesis applications by including anapplication that translates the socket protocol used by browsers into both MIDIand OSC messages.},
address = {Daejeon, Republic of Korea},
author = {Roberts, Charles and Wakefield, Graham and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {browser-based NIMEs,javascript,mobile devices,web audio,websockets},
pages = {313--318},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{The Web Browser As Synthesizer And Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Frounberg2010,
address = {Sydney, Australia},
author = {Frounberg, Ivar and Innervik, Kjell Tore and Jensenius, Alexander R},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {glass instruments,nime,nime10,performance practice},
number = {Nime},
pages = {287--290},
title = {{Glass Instruments  From Pitch to Timbre}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}287.pdf},
year = {2010}
}
@inproceedings{Fan:2013a,
abstract = {The BioSync interface presented in this paper merges the heart-rate basedparadigm with the brain-wave based paradigm into one mobile unit which isscalable for large audience real-time applications. The goal of BioSync is toprovide a hybrid interface, which uses audience biometric responses foraudience participation techniques. To provide an affordable and scalablesolution, BioSync collects the user's heart rate via mobile phone pulseoximetry and the EEG data via Bluetooth communication with the off-the-shelfMindWave Mobile hardware. Various interfaces have been designed and implementedin the development of audience participation techniques and systems. In thedesign and concept of BioSync, we first summarize recent interface research foraudience participation within the NIME-related context, followed by the outlineof the BioSync methodology and interface design. We then present a techniquefor dynamic tempo control based on the audience biometric responses and anearly prototype of a mobile dual-channel pulse oximetry and EEG bi-directionalinterface for iOS device (BioSync). Finally, we present discussions and ideasfor future applications, as well as plans for a series of experiments, whichinvestigate if temporal parameters of an audience's physiological metricsencourage crowd synchronization during a live event or performance, acharacteristic, which we see as having great potential in the creation offuture live musical and audiovisual performance applications.},
address = {Daejeon, Republic of Korea},
author = {Fan, Yuan-Yi and Sciotto, Myles},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Audience,Biometrics,Experience,Mobile,Social,Synchronous Interaction},
pages = {248--251},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{BioSync: An Informed Participatory Interface for Audience Dynamics and Audiovisual Content Co-creation using Mobile PPG and EEG}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Kartadinata2003,
address = {Montreal, QC, Canada},
author = {Kartadinata, Sukandar},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {180--183},
title = {{The Gluiph: a Nucleus for Integrated Instruments}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}180.pdf},
year = {2003}
}
@inproceedings{Schedel2008,
abstract = {The Color of Waiting is an interactive theater workwith music, dance, and video which was developed atSTEIM in Amsterdam and further refined at CMMASin Morelia Mexico with funding from Meet theComposer. Using Max/MSP/ Jitter a cellist is able tocontrol sound and video during the performancewhile performing a structured improvisation inresponse to the dancer's movement. In order toensure. repeated performances of The Color o fWaiting , Kinesthetech Sense created the scorecontained in this paper. Performance is essential tothe practice of time-based art as a living form, buthas been complicated by the unique challenges ininterpretation and re-creation posed by worksincorporating technology. Creating a detailed scoreis one of the ways artists working with technologycan combat obsolescence.},
address = {Genoa, Italy},
author = {Schedel, Margaret and Rootberg, Alison and de Martelly, Elizabeth},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {nime08},
pages = {339--342},
title = {{Scoring an Interactive, Multimedia Performance Work}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}339.pdf},
year = {2008}
}
@inproceedings{nime2014:cgeiger,
abstract = {We present a system that allows users to experience singing without singing using gesture-based interaction techniques. We designed a set of body-related interaction and multi-modal feedback techniques and developed a singing voice synthesizer system that is controlled by the user's mouth shapes and arm gestures. Based on the adaption of a number of digital media-related techniques such as face and body tracking, 3D rendering, singing voice synthesis and physical computing, we developed a media installation that allows users to perform an aria without real singing and provide the look and feel from a 20th century performance of an opera singer. We evaluated this system preliminarily with users.},
address = {London, United Kingdom},
author = {Poepel, Cornelius and Feitsch, Jochen and Strobel, Marco and Geiger, Christian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {359--362},
publisher = {Goldsmiths, University of London},
title = {{Design and Evaluation of a Gesture Controlled Singing Voice Installation}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}439.pdf},
year = {2014}
}
@inproceedings{Hamano:2013,
abstract = {Electroencephalography (EEG) has been used to generate music for over 40 years,but the most recent developments in brain--computer interfaces (BCI) allowgreater control and more flexible expression for using new musical instrumentswith EEG. We developed a real-time musical performance system using BCItechnology and sonification techniques to generate imagined musical chords withorganically fluctuating timbre. We aim to emulate the expressivity oftraditional acoustic instruments. The BCI part of the system extracts patternsfrom the neural activity while a performer imagines a score of music. Thesonification part of the system captures non-stationary changes in the brainwaves and reflects them in the timbre by additive synthesis. In this paper, wediscuss the conceptual design, system development, and the performance of thisinstrument.},
address = {Daejeon, Republic of Korea},
author = {Hamano, Takayuki and Rutkowski, Tomasz and Terasawa, Hiroko and Okanoya, Kazuo and Furukawa, Kiyoshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Brain-computer interface (BCI),classification,qualitative and quantitative information,sonification},
pages = {49--54},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Generating an Integrated Musical Expression with a Brain--Computer Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Scott:2013,
abstract = {Digital music technology has transformed the listener experience and creatednew avenues for creative interaction and expression within the musical domain.The barrier to music creation, distribution and collaboration has been reduced,leading to entirely new ecosystems of musical experience. Software editingtools such as digital audio workstations (DAW) allow nearly limitlessmanipulation of source audio into new sonic elements and textures and havepromoted a culture of recycling and repurposing of content via mashups andremixes. We present a multi-touch application that allows a user to customizetheir listening experience by blending various versions of a song in real time.},
address = {Daejeon, Republic of Korea},
author = {Scott, Jeffrey and Moorhead, Mickey and Chapman, Justin and Schwabe, Ryan and Kim, Youngmoo E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Interactive media,Mobile devices,Multi-touch,Multi-track},
pages = {417--420},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Personalized Song Interaction Using a Multi Touch Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Reboursiere:2012,
abstract = {In this paper we present a series of algorithms developed to detect the following guitar playing techniques : bend, hammer-on, pull-off, slide, palm muting and harmonic. Detection of playing techniques can be used to control exter-nal content (i.e audio loops and effects, videos, light events, etc.), as well as to write real-time score or to assist guitar novices in their learning process. The guitar used is a Godin Multiac with an under-saddle RMC hexaphonic piezo pickup (one pickup per string, i.e six mono signals).},
address = {Ann Arbor, Michigan},
author = {Reboursi{\`{e}}re, Lo{\"{i}}c and L{\"{a}}hdeoja, Otso and Drugman, Thomas and Dupont, St{\'{e}}phane and Picard-Limpens, C{\'{e}}cile and Riche, Nicolas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Guitar audio analysis,augmented guitar,controller,hexaphonic pickup,playing techniques},
month = {may},
publisher = {University of Michigan},
title = {{Left and right-hand guitar playing techniques detection}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}213.pdf},
year = {2012}
}
@inproceedings{Nagashima2009,
address = {Pittsburgh, PA, United States},
author = {Nagashima, Yoichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {midi,nime09,parallel processing,propeller,sensor},
pages = {169--170},
title = {{Parallel Processing System Design with "Propeller" Processor}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}169.pdf},
year = {2009}
}
@inproceedings{Jaimovich2010a,
address = {Sydney, Australia},
author = {Jaimovich, Javier and Knapp, Benjamin},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {multimodal signals,nime10,sensor data acquisition,synchronization},
number = {Nime},
pages = {372--374},
title = {{Synchronization of Multimodal Recordings for Musical Performance Research}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}372.pdf},
year = {2010}
}
@inproceedings{nime2014:oizmirli,
abstract = {We present a framework for imitation of percussion performances with parameter-based learning for accurate reproduction. We constructed a robotic setup involving pull-solenoids attached to drum sticks which communicate with a computer through an Arduino microcontroller. The imitation framework allows for parameter adaptation to different mechanical constructions by learning the capabilities of the overall system being used. For the rhythmic vocabulary, we have considered regular stroke, flam and drag styles. A learning and calibration system was developed to efficiently perform grace notes for the drag rudiment as well as the single stroke and the flam rudiment. A second pre-performance process is introduced to minimize the latency difference between individual drum sticks in our mechanical setup. We also developed an off-line onset detection method to reliably recognize onsets from the microphone input. Once these pre-performance steps are taken, our setup will then listen to a human drummer's performance pattern, analyze for onsets, loudness, and rudiment pattern, and then play back using the learned parameters for the particular system. We conducted three different evaluations of our constructed system.},
address = {London, United Kingdom},
author = {Izmirli, Ozgur and Faris, Jake},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {483--486},
publisher = {Goldsmiths, University of London},
title = {{Imitation Framework for Percussion}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}360.pdf},
year = {2014}
}
@inproceedings{Birnbaum2005,
address = {Vancouver, BC, Canada},
author = {Birnbaum, David and Fiebrink, Rebecca and Malloch, Joseph and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Birnbaum et al. - 2005 - Towards a Dimension Space for Musical Devices.pdf:pdf},
keywords = {design space analysis,human-computer interaction,interfaces for musical expression,new},
pages = {192--195},
title = {{Towards a Dimension Space for Musical Devices}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}192.pdf},
year = {2005}
}
@inproceedings{Gelineck2009,
abstract = {This paper presents a HCI inspired evaluation of simple physical interfaces used to control physical models. Specifically knobs and sliders are compared in a creative and exploratory framework, which simulates the natural environment in which an electronic musician would normally explore a new instrument. No significant difference was measured between using knobs and sliders for controlling parameters of a physical modeling electronic instrument. Thereported difference between the tested instruments were mostlydue to the sound synthesis models.},
address = {Pittsburgh, PA, United States},
author = {Gelineck, Steven and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Affordances.,Creativ- ity,Electronic Musicians,Evaluation,Exploration,Interfaces,Knobs,Physi- cal Modeling,Sliders},
pages = {13--18},
title = {{A Quantitative Evaluation of the Differences between Knobs and Sliders}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}013.pdf},
year = {2009}
}
@inproceedings{nime2014:jmurphy,
abstract = {This paper presents a method for a self-tuning procedure for musical robots capable of continuous pitch-shifting. Such a technique is useful for robots consisting of many strings: the ability to self-tune allows for long-term installation without human intervention as well as on-the-fly tuning scheme changes. The presented method consists of comparing a detuned string's pitch at runtime to a pre-compiled table of string responses at varying tensions. The behavior of the current detuned string is interpolated from the two nearest pre-characterized neighbors, and the desired virtual fret positions are added to the interpolated model. This method allows for rapid tuning at runtime, requiring only a single string actuation to determine the pitch. After a detailed description of the self-tuning technique and implementation, the results will be evaluated on the new Swivel 2 robotic slide guitar. The paper concludes with a discussion of performance applications and ideas for subsequent work on self-tuning musical robotic systems.},
address = {London, United Kingdom},
author = {Murphy, Jim and Mathews, Paul and Kapur, Ajay and Carnegie, Dale},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {565--568},
publisher = {Goldsmiths, University of London},
title = {{Robot: Tune Yourself! Automatic Tuning for Musical Robotics}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}345.pdf},
year = {2014}
}
@inproceedings{Papetti2011,
address = {Oslo, Norway},
author = {Papetti, Stefano and Civolani, Marco and Fontana, Federico},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {audio,embodiment,foot tapping,footwear,interface,mobile,tactile,wearable,wireless},
number = {June},
pages = {473--476},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Rhythm'n'Shoes : a Wearable Foot Tapping Interface with Audio-Tactile Feedback}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}473.pdf},
year = {2011}
}
@inproceedings{Newton-Dunn2003,
address = {Montreal, QC, Canada},
author = {Newton-Dunn, Henry and Nakano, Hiroaki and Gibson, James},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {170--177},
title = {{Block Jam: A Tangible Interface for Interactive Music}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}170.pdf},
year = {2003}
}
@inproceedings{Chun2010,
address = {Sydney, Australia},
author = {Chun, Sungkuk and Hawryshkewich, Andrew and Jung, Keechul and Pasquier, Philippe},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {collision detection,computer vision,custom interface,form recognition,midi,mixed-,nime10,reality},
number = {Nime},
pages = {31--36},
title = {{Freepad : A Custom Paper-based MIDI Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}031.pdf},
year = {2010}
}
@inproceedings{Cassinelli2010,
address = {Sydney, Australia},
author = {Cassinelli, Alavaro and Kuribara, Yusaku and Zerroug, Alexis and Ishikawa, Masatoshi and Manabe, Daito},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {2,5,and humanities,arts,h,h5,interaction styles,j,methodologies and techniques,music computing,nime10,performing arts,sound and,user interfaces},
number = {Nime},
pages = {144--149},
title = {{scoreLight : Playing with a Human-Sized Laser Pick-Up}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}144.pdf},
year = {2010}
}
@inproceedings{Berthaut2010,
address = {Sydney, Australia},
author = {Berthaut, Florent and Desainte-Catherine, Myriam and Hachet, Martin},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {3d interac-,drile,hierarchical live-looping,immersive instrument,nime10},
number = {Nime},
pages = {192--197},
title = {{DRILE : An Immersive Environment for Hierarchical Live-Looping}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}192.pdf},
year = {2010}
}
@inproceedings{Fyans2011,
address = {Oslo, Norway},
author = {Fyans, A Cavan and Gurevich, Michael},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {control,effort,embodiment,perception,skill,spectator},
number = {June},
pages = {495--498},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Perceptions of Skill in Performances with Acoustic and Electronic Instruments}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}495.pdf},
year = {2011}
}
@inproceedings{Seldess2011,
address = {Oslo, Norway},
author = {Seldess, Zachary and Yamada, Toshiro},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {audio control systems,audio for vr,max,msp,spatial},
number = {June},
pages = {161--166},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Tahakum: A Multi-Purpose Audio Control Framework}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}161.pdf},
year = {2011}
}
@inproceedings{McDonald2009,
address = {Pittsburgh, PA, United States},
author = {McDonald, Kyle and Kouttron, Dane and Bahn, Curtis and Braasch, Jonas and Oliveros, Pauline},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {haptics,interface,nime09,performance,telematic},
pages = {41--42},
title = {{The Vibrobyte : A Haptic Interface for Co-Located Performance}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}041.pdf},
year = {2009}
}
@inproceedings{Mclean2009,
address = {Pittsburgh, PA, United States},
author = {Mclean, Alex and Wiggins, Geraint},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09,timbre,vocable synthesis},
pages = {276--279},
title = {{Words , Movement and Timbre}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}276.pdf},
year = {2009}
}
@inproceedings{Overholt2011,
address = {Oslo, Norway},
author = {Overholt, Dan},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Overholt - 2011 - The Overtone Fiddle an Actuated Acoustic Instrument(2).pdf:pdf},
keywords = {acoustics,active,actuated musical instruments,electronic violin,hybrid instruments},
number = {June},
pages = {30--33},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{The Overtone Fiddle: an Actuated Acoustic Instrument}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}004.pdf},
year = {2011}
}
@inproceedings{Frisson:2012,
abstract = {This paper presents the LoopJam installation which allows participants to interact with a sound map using a 3D com- puter vision tracking system. The sound map results from similarity-based clustering of sounds. The playback of these sounds is controlled by the positions or gestures of partic- ipants tracked with a Kinect depth-sensing camera. The beat-inclined bodily movements of participants in the in- stallation are mapped to the tempo of played sounds, while the playback speed is synchronized by default among all sounds. We presented and tested an early version of the in- stallation to three exhibitions in Belgium, Italy and France. The reactions among participants ranged between curiosity and amusement.},
address = {Ann Arbor, Michigan},
author = {Frisson, Christian and Dupont, St{\'{e}}phane and Leroy, Julien and Moinet, Alexis and Ravet, Thierry and Siebert, Xavier and Dutoit, Thierry},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Interactive music systems and retrieval,audio similarity,depth sensors,user interaction and interfaces},
publisher = {University of Michigan},
title = {{LoopJam: turning the dance floor into a collaborative instrumental map}},
year = {2012}
}
@inproceedings{Andersson:2013,
abstract = {Our voice and body are important parts of our self-experience, andcommunication and relational possibilities. They gradually become moreimportant for Interaction Design, due to increased development of tangibleinteraction and mobile communication. In this paper we present and discuss ourwork with voice and tangible interaction in our ongoing research project XXXXX.The goal is to improve health for families, adults and children withdisabilities through use of collaborative, musical, tangible media. We build onuse of voice in Music Therapy and on a humanistic health approach. Ourchallenge is to design vocal and tangible interactive media that through usereduce isolation and passivity and increase empowerment for the users. We usesound recognition, generative sound synthesis, vibrations and cross-mediatechniques, to create rhythms, melodies and harmonic chords to stimulatebody-voice connections, positive emotions and structures for actions.},
address = {Daejeon, Republic of Korea},
author = {Andersson, Anders-Petter and Cappelen, Birgitta},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Empowerment,Music {\&} Health,Music Therapy,Resource-Oriented,Tangible Interaction,Vocal Interaction,Voice},
pages = {406--412},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Designing Empowering Vocal and Tangible Interaction}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Feng2010,
abstract = { Irregular Incurve is a MIDI controllable robotic string instrument. The twelve independent string-units compose the complete musical scale of 12 units. Each string can be plucked by a motor control guitar pick. A MIDI keyboard is attached to the instrument and serves as an interface for real-time interactions between the instrument and the audience. Irregular Incurve can also play preprogrammed music by itself. This paper presents the design concept and the technical solutions to realizing the functionality of Irregular Incurve. The future features are also discussed. },
address = {Sydney, Australia},
author = {Feng, Xiaoyang},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Acoustic,Arduino,Interactive,MIDI,Motor Control,NIME,Real time Performance,Robotics,Servo,String Instrument},
pages = {377--379},
title = {{Irregular Incurve}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}377.pdf},
year = {2010}
}
@inproceedings{nime2014:ldonovan,
abstract = {This paper presents the Talking Guitar, an electric guitar augmented with a system which tracks the position of the headstock in real time and uses that data to control the parameters of a formant-filtering effect which impresses upon the guitar sound a sense of speech. A user study is conducted with the device to establish an indication of the practicality of using headstock tracking to control effect parameters and to suggest natural and useful mapping strategies. Individual movements and gestures are evaluated in order to guide further development of the system.},
address = {London, United Kingdom},
author = {Donovan, Liam and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {351--354},
publisher = {Goldsmiths, University of London},
title = {{The Talking Guitar: Headstock Tracking and Mapping Strategies}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}407.pdf},
year = {2014}
}
@inproceedings{Lyons2003,
address = {Montreal, QC, Canada},
author = {Lyons, Michael J and Haehnel, Michael and Tetsutani, Nobuji},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {116--121},
title = {{Designing, Playing, and Performing with a Vision-based Mouth Interface}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}116.pdf},
year = {2003}
}
@inproceedings{Sinyor2005,
abstract = {This paper presents a novel controller built to exploit thephysical behaviour of a simple dynamical system, namely aspinning wheel. The phenomenon of gyroscopic precessioncauses the instrument to slowly oscillate when it is spunquickly, providing the performer with proprioceptive feedback. Also, due to the mass of the wheel and tire and theresulting rotational inertia, it maintains a relatively constant angular velocity once it is set in motion. Various sensors were used to measure continuous and discrete quantitiessuch as the the angular frequency of the wheel, its spatialorientation, and the performer's finger pressure. In addition, optical and hall-effect sensors detect the passing of aspoke-mounted photodiode and two magnets. A base software layer was developed in Max/MSP and various patcheswere written with the goal of mapping the dynamic behaviorof the wheel to varied musical processes.},
address = {Vancouver, BC, Canada},
author = {Sinyor, Elliot and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Digital Musical Instruments,Gyroscopic Precession,HCI,Open Sound Control,Rotational Inertia},
pages = {42--45},
title = {{Gyrotyre : A dynamic hand-held computer-music controller based on a spinning wheel}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}042.pdf},
year = {2005}
}
@inproceedings{Mandelis2004,
address = {Hamamatsu, Japan},
author = {Mandelis, James and Husbands, Phil},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {musical interaction,performance mapping,sound synthesis},
number = {1994},
pages = {47--50},
title = {{Don't Just Play it, Grow it! : Breeding Sound Synthesis and Performance Mappings}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}047.pdf},
year = {2004}
}
@inproceedings{Barenca2011,
address = {Oslo, Norway},
author = {Barenca, Adri{\'{a}}n and Torre, Giuseppe},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {1,and force sensors within,force sensing,gestural,gestural controller,manipulation,strings,strings and force sensing,the integration of strings},
number = {June},
pages = {232--235},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{The Manipuller : Strings Manipulation and Multi- Dimensional Force Sensing}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}232.pdf},
year = {2011}
}
@inproceedings{sserafin:2014,
abstract = {In this paper we propose an empirical method to develop mapping strategies between a gestural based interface (the Gloves) and physically based sound synthesis models. An experiment was performed in order to investigate which kind of gestures listeners associate to synthesised sounds produced using physical models, corresponding to three categories of sound: sustained, iterative and impulsive. The results of the experiment show that listeners perform similar gestures when controlling sounds from the different categories. We used such gestures in order to create the mapping strategy between the Gloves and the physically based synthesis engine.},
address = {London, United Kingdom},
author = {Serafin, Stefania and Trento, Stefano and Grani, Francesco and Perner-Wilson, Hannah and Madgwick, Seb and Mitchell, Tom},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {521--524},
publisher = {Goldsmiths, University of London},
title = {{Controlling Physically Based Virtual Musical Instruments Using The Gloves}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}307.pdf},
year = {2014}
}
@inproceedings{Ramakrishnan2006,
address = {Paris, France},
author = {Ramakrishnan, Chandrasekhar and Go{\ss}man, Joachim and Br{\"{u}}mmer, Ludger},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Acousmatic Music,Ambisonics,Sound Spatialization,Vector Based Additive Panning (VBAP),Wave Field Synthesis},
pages = {140--143},
title = {{The ZKM Klangdom}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}140.pdf},
year = {2006}
}
@inproceedings{fhashimoto:2014,
abstract = {In recent years, there has been an increase in the number of artists who make use of automated music performances in their music and live concerts. Automated music performance is a form of music production using programmed musical notes. Some artists who introduce automated music performance operate parameters of the sound in their performance for production of their music. In this paper, we focus on the music production aspects and describe a method that realizes operation of the sound parameters via computer. Further, in this study, the probability distribution of the action (i.e., variation of parameters) is obtained within the music, using Bayesian filters. The probability distribution of each piece of music is transformed by passing through a Markov model. After the probability distribution is obtained, sound parameters can be automatically controlled. We have developed a system to reproduce the musical expressions of humans and confirmed the possibilities of our method.},
address = {London, United Kingdom},
author = {Hashimoto, Fumito and Miura, Motoki},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {347--350},
publisher = {Goldsmiths, University of London},
title = {{Operating Sound Parameters Using {\{}Markov{\}} Model and {\{}Bayes{\}}ian Filters in Automated Music Performance}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}380.pdf},
year = {2014}
}
@inproceedings{Burns2006,
address = {Paris, France},
author = {Burns, Anne-Marie and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {finger-tracking,gesture,guitar fingering,hough transform},
pages = {196--199},
title = {{Visual Methods for the Retrieval of Guitarist Fingering}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}196.pdf},
year = {2006}
}
@inproceedings{Hansen:2012,
abstract = {This paper presents the results of user interaction with two explorative music environments (sound system A and B) that were inspired from the Banda Linda music tradition in two different ways. The sound systems adapted to how a team of two players improvised and made a melody together in an interleaved fashion: Systems A and B used a fuzzy logic algorithm and pattern recognition to respond with modifications of a background rhythms. In an experiment with a pen tablet interface as the music instrument, users aged 10-13 were to tap tones and continue each other's melody. The sound systems rewarded users sonically, if they managed to add tones to their mutual melody in a rapid turn taking manner with rhythmical patterns. Videos of experiment sessions show that user teams contributed to a melody in ways that resemble conversation. Interaction data show that each sound system made player teams play in different ways, but players in general had a hard time adjusting to a non-Western music tradition. The paper concludes with a comparison and evaluation of the two sound systems. Finally it proposes a new approach to the design of collaborative and shared music environments that is based on ''listening applications''.},
address = {Ann Arbor, Michigan},
author = {Hansen, Anne-Marie and Andersen, Hans J{\o}rgen and Raudaskoski, Pirkko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Music improvisation,interaction design.,interaction studies,novices,social learning},
publisher = {University of Michigan},
title = {{Two Shared Rapid Turn Taking Sound Interfaces for Novices}},
year = {2012}
}
@inproceedings{nime2014:obown,
abstract = {Audiences of live laptop music frequently express dismay at the opacity of performer activity and question how ``live" performances actually are. Yet motionless laptop performers endure as musical spectacles from clubs to concert halls, suggesting that for many this is a non-issue. Understanding these perceptions might help performers better achieve their intentions, inform interface design within the NIME field and help develop theories of liveness and performance. To this end, a study of listeners' perception of liveness and performer control in laptop performance was carried out, in which listeners examined several short audio-only excerpts of laptop performances and answered questions about their perception of the performance: what they thought was happening and its sense of liveness. Our results suggest that audiences are likely to associate liveness with perceived performer activity such as improvisation and the audibility of gestures, whereas perceptions of generative material, backing tracks, or other preconceived material do not appear to inhibit perceptions of liveness.},
address = {London, United Kingdom},
author = {Bown, Oliver and Bell, Renick and Parkinson, Adam},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {13--18},
publisher = {Goldsmiths, University of London},
title = {{Examining the Perception of Liveness and Activity in Laptop Music: Listeners' Inference about what the Performer is Doing from the Audio Alone}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}538.pdf},
year = {2014}
}
@inproceedings{Schoonderwaldt2006,
address = {Paris, France},
author = {Schoonderwaldt, Erwin and Rasamimanana, Nicolas and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
pages = {200--203},
title = {{Combining Accelerometer and Video Camera: Reconstruction of Bow Velocity Profiles}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}200.pdf},
year = {2006}
}
@inproceedings{Choi2003,
abstract = {This paper suggests that there is a need for formalizing acomponent model of gestural primitive throughput in musicinstrument design. The purpose of this model is to construct acoherent and meaningful interaction between performer andinstrument. Such a model has been implicit in previous researchfor interactive performance systems. The model presented heredistinguishes gestural primitives from units of measure ofgestures. The throughput model identifies symmetry betweenperformance gestures and musical gestures, and indicates a rolefor gestural primitives when a performer navigates regions ofstable oscillations in a musical instrument. The use of a highdimensional interface tool is proposed for instrument design, forfine-tuning the mapping between movement sensor data andsound synthesis control data.},
address = {Montreal},
author = {Choi, Insook},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
keywords = {Performance gestures,affordances,instrument design,mapping,musical gestures,stability.,tuning},
pages = {201--204},
title = {{A Component Model of Gestural Primitive Throughput}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}201.pdf},
year = {2003}
}
@inproceedings{Franinovic2007,
address = {New York City, NY, United States},
author = {Franinovic, Karmen and Visell, Yon},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {architecture,interaction,music,nime07,sound in-,urban design},
pages = {191--196},
title = {{New Musical Interfaces in Context : Sonic Interaction Design in the Urban Setting}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}191.pdf},
year = {2007}
}
@inproceedings{mmarier:2014,
abstract = {The development of the cushion-like musical interface called the sponge started about seven years ago. Since then, it was extensively used to perform in various settings. The sponge itself is described, but the main focus is on the evolution of the mapping strategies that are used. The author reviews the guidelines proposed by other researchers and explains how they were concretely applied with the sponge. He concludes that no single strategy constitutes a solution to the issue of mapping and that musical compositions are complex entities that require the use of a multitude of mapping strategies in parallel. It is hoped that the mappings described combined with new strategies will eventually lead to the emergence of a musical language that is idiomatic to the sponge.},
address = {London, United Kingdom},
author = {Marier, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {525--528},
publisher = {Goldsmiths, University of London},
title = {{Designing Mappings for the Sponge: Towards Spongistic Music}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}292.pdf},
year = {2014}
}
@inproceedings{Poepel2006,
address = {Paris, France},
author = {Poepel, Cornelius and Overholt, Dan},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {bass,cello,digital,electronic,synthesis,viola,violin},
pages = {390--395},
title = {{Recent Developments in Violin-related Digital Musical Instruments: Where Are We and Where Are We Going?}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}390.pdf},
year = {2006}
}
@inproceedings{Lee2006,
address = {Paris, France},
author = {Lee, Eric and Gr{\"{u}}ll, Ingo and Keil, Henning and Borchers, Jan},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {conducting,gesture recognition,software gesture frameworks},
pages = {260--265},
title = {{conga: A Framework for Adaptive Conducting Gesture Analysis}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}260.pdf},
year = {2006}
}
@inproceedings{Smith2011,
address = {Oslo, Norway},
author = {Smith, Benjamin D and Garnett, Guy E},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {adaptive resonance theory,chine listening,improvisation,interactive computer music,ma-,machine learning,nime},
number = {June},
pages = {108--111},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{The Self-Supervising Machine}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}108.pdf},
year = {2011}
}
@inproceedings{DeLaubier2006,
address = {Paris, France},
author = {de Laubier, Serge and Goudard, Vincent},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
keywords = {1,audio-graphic portable instrument,ethernet,from 1983 to 1988,genesis of the project,on,puce muse studios,r{\'{e}}pertoire,we worked at the,wifi},
pages = {288--291},
title = {{Meta-Instrument 3: a Look over 17 Years of Practice}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}288.pdf},
year = {2006}
}
@inproceedings{Neill2009,
address = {Pittsburgh, PA, United States},
author = {Neill, Ben and Singer, Eric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {331},
title = {{Ben Neill and LEMUR}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}331.pdf},
year = {2009}
}
@inproceedings{Menzies2008,
abstract = {Phya is an open source C++ library originally designed foradding physically modeled contact sounds into computergame environments equipped with physics engines. We review some aspects of this system, and also consider it fromthe purely aesthetic perspective of musical expression.},
address = {Genoa, Italy},
author = {Menzies, Dylan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {NIME,audio synthesis,musical expression,physical model- ing,virtual reality},
pages = {71--76},
title = {{Virtual Intimacy : Phya as an Instrument}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}071.pdf},
year = {2008}
}
@inproceedings{Hasan2002,
address = {Dublin, Ireland},
author = {Hasan, Leila and Yu, Nicholas and Paradiso, Joseph A},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {capacitive sensing,gesture interface,harp,laser,musical,optical proximity sensing,servo control,theremin},
pages = {82--87},
title = {{The Termenova : A Hybrid Free-Gesture Interface}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}082.pdf},
year = {2002}
}
@inproceedings{Kim2011b,
address = {Oslo, Norway},
author = {Kim, Tae Hun and Fukayama, Satoru and Nishimoto, Takuya and Sagayama, Shigeki},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {conditional random fields,modeling,performance rendering,polyphonic expression,statistical},
number = {June},
pages = {96--99},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Polyhymnia : An Automatic Piano Performance System with Statistical Modeling of Polyphonic Expression and Musical Symbol Interpretation}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}096.pdf},
year = {2011}
}
@inproceedings{Berger2010,
address = {Sydney, Australia},
author = {Berger, Michael},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {emotive gesture and music,hall effect,human-controller interaction,musical mapping strategies,nime10,novel musical instrument,passive haptic feedback,sensor-augmented hand-exerciser},
number = {Nime},
pages = {419--422},
title = {{The GRIP MAESTRO : Idiomatic Mappings of Emotive Gestures for Control of Live Electroacoustic Music}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}419.pdf},
year = {2010}
}
@inproceedings{Jensenius2006a,
address = {Paris, France},
author = {Jensenius, Alexander R and Kvifte, Tellef and God{\o}y, Rolf I},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {gesture analysis,gesture description,standards},
pages = {176--179},
title = {{Towards a Gesture Description Interchange Format}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}176.pdf},
year = {2006}
}
@inproceedings{Endo2008,
address = {Genoa, Italy},
author = {Endo, Ayaka and Kuhara, Yasuo},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {nime08},
pages = {345--346},
title = {{Rhythmic Instruments Ensemble Simulator Generating Animation Movies Using Bluetooth Game Controller}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}345.pdf},
year = {2008}
}
@inproceedings{Jessop2009,
address = {Pittsburgh, PA, United States},
author = {Jessop, Elena},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {conducting,gestural control,musical expressivity,nime09,vocal performance},
pages = {256--259},
title = {{The Vocal Augmentation and Manipulation Prosthesis (VAMP): A Conducting-Based Gestural Controller for Vocal Performance}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}256.pdf},
year = {2009}
}
@inproceedings{Lee2007,
abstract = {We present a system for rhythmic analysis of human motion inreal-time. Using a combination of both spectral (Fourier) andspatial analysis of onsets, we are able to extract repeating rhythmic patterns from data collected using accelerometers. These extracted rhythmic patterns show the relative magnitudes of accentuated movements and their spacing in time. Inspired by previouswork in automatic beat detection of audio recordings, we designedour algorithms to be robust to changes in timing using multipleanalysis techniques and methods for sensor fusion, filtering andclustering. We tested our system using a limited set of movements,as well as dance movements collected from a professional, bothwith promising results.},
address = {New York City, NY, United States},
author = {Lee, Eric and Enke, Urs and Borchers, Jan and de Jong, Leo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {dance movement analysis,onset analysis,rhythm analysis},
pages = {136--141},
title = {{Towards Rhythmic Analysis of Human Motion Using Acceleration-Onset Times}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}136.pdf},
year = {2007}
}
@inproceedings{Spowage2009,
address = {Pittsburgh, PA, United States},
author = {Spowage, Neal},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {29--30},
title = {{The Ghetto Bastard : A Portable Noise Instrument}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}029.pdf},
year = {2009}
}
@inproceedings{Carrillo2010,
address = {Sydney, Australia},
author = {Carrillo, Alfonso P and Bonada, Jordi},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {control,nime10,spectral,synthesis,violin,virtual},
number = {Nime},
pages = {229--232},
title = {{The Bowed Tube : a Virtual Violin}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}229.pdf},
year = {2010}
}
@inproceedings{Kamiyama2008,
address = {Genoa, Italy},
author = {Kamiyama, Yusuke and Tanaka, Mai and Tanaka, Hiroya},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {3D sound system,musical expression,sound generating device,sound-field arrangement.,umbrella},
pages = {352--353},
title = {{Oto-Shigure : An Umbrella-Shaped Sound Generator for Musical Expression}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}352.pdf},
year = {2008}
}
@inproceedings{Fuhrmann:2013,
abstract = {This paper demonstrates how to use multiple Kinect(TM) sensors to map aperformers motion to music. We merge skeleton data streams from multiplesensors to compensate for occlusions of the performer. The skeleton jointpositions drive the performance via open sound control data. We discuss how toregister the different sensors to each other and how to smoothly merge theresulting data streams and how to map position data in a general framework tothe live electronics applied to a chamber music ensemble.},
address = {Daejeon, Republic of Korea},
author = {Fuhrmann, Anton and Kretz, Johannes and Burwik, Peter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {kinect,live electronics,motion tracking,multi sensor,open sound control,parameter mapping,sensor fusion},
pages = {358--362},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Multi Sensor Tracking for Live Sound Transformation}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Berdahl2009b,
address = {Pittsburgh, PA, United States},
author = {Berdahl, Edgar and Niemeyer, G{\"{u}}nter and Smith, Julius O},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {cellist assisted by two,detent,feedback control,figure 1,gravity well,haptic,haptic devices,human motor,nime09,pitch selection,response time,system},
pages = {177--182},
title = {{Using Haptics to Assist Performers in Making Gestures to a Musical Instrument}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}177.pdf},
year = {2009}
}
@inproceedings{Ramakrishnan2004,
address = {Hamamatsu, Japan},
author = {Ramakrishnan, Chandrasekhar and Freeman, Jason and Varnik, Kristjan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {and speech analysis,auracle,diction,interactive music systems,jsyn,linear pre-,networking and control,neural networks,open sound con-,transjam,voice,voice interface},
pages = {100--103},
title = {{The Architecture of Auracle: a Real-Time, Distributed, Collaborative Instrument}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}100.pdf},
year = {2004}
}
@inproceedings{Sosnick2011,
abstract = {In this paper, we describe an implementation of a real-time sound synthesizer using Finite Difference-based simulation of a two-dimensional membrane. Finite Difference (FD) methods can be the basis for physics-based music instrument models that generate realistic audio output. However, such methods are compute-intensive; large simulations cannot run in real time on current CPUs. Many current systems now include powerful Graphics Processing Units (GPUs), which are a good fit for FD methods. We demonstrate that it is possible to use this method to create a usable real-time audio synthesizer. },
address = {Oslo, Norway},
author = {Sosnick, Marc and Hsu, William},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {CUDA,Finite Difference,GPU,Synthesis},
pages = {264--267},
title = {{Implementing a Finite Difference-Based Real-time Sound Synthesizer using {\{}GPU{\}}s}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}264.pdf},
year = {2011}
}
@inproceedings{Humphrey2010,
address = {Sydney, Australia},
author = {Humphrey, Tim and Flynn, Madeleine and Stevens, Jesse},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {nime10,ready-mades morphology stillness,sonification installation spectator-choreography m},
number = {Nime},
pages = {69--71},
title = {{Epi-thet : A Musical Performance Installation and a Choreography of Stillness}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}069.pdf},
year = {2010}
}
@inproceedings{nime2014:eberdahl,
abstract = {An embedded acoustic instrument is an embedded musical instrument that provides a direct acoustic output. This pa- per describes how to make embedded acoustic instruments using laser cutting for digital fabrication. Several tips are given for improving the acoustic quality including: employing maximally stiff material, placing loudspeaker drivers in the corners of enclosure faces, increasing the stiffness of ``loudspeaker" faces by doubling their thickness, choosing side-lengths with non-integer ratios, and incorporating bracing. Various versions of an open design of the ``LapBox" are provided to help community members replicate and ex- tend the work. A procedure is suggested for testing and optimizing the acoustic quality.},
address = {London, United Kingdom},
author = {Berdahl, Edgar},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Berdahl - 2014 - How to Make Embedded Acoustic Instruments.pdf:pdf},
pages = {140--143},
publisher = {Goldsmiths, University of London},
title = {{How to Make Embedded Acoustic Instruments}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}551.pdf},
year = {2014}
}
@inproceedings{Barbosa2012,
abstract = {The authors propose the development of a more complete Digital Music Instrument (DMI) evaluation methodology, which provides structured tools for the incremental development of prototypes based on user feedback. This paper emphasizes an important but often ignored stakeholder present in the context of musical performance: the audience. We demonstrate the practical application of an audience focused methodology through a case study (`Illusio'), discuss the obtained results and possible improvements for future works.},
address = {Ann Arbor, Michigan},
author = {Barbosa, Jer{\^{o}}nimo and Calegario, Filipe and Teichrieb, Ver{\^{o}}nica and Ramalho, Geber and McGlynn, Patrick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Empirical methods,Illusio,digital musical instruments,evaluation methodology,quantitative,usability testing and evaluation},
publisher = {University of Michigan},
title = {{Considering Audience's View Towards an Evaluation Methodology for Digital Musical Instruments}},
year = {2012}
}
@inproceedings{Ferguson:2013,
abstract = {Feedback created by guitars and amplifiers is difficult to use in musicalsettings -- parameters such as pitch and loudness are hard to specify preciselyby fretting a string or by holding the guitar near an amplifier. This researchinvestigates methods for controlling the level and pitch of the feedbackproduced by a guitar and amplifier, which are based on incorporatingcorpus-based control into the system. Two parameters are used to define thecontrol parameter space -- a simple automatic gain control system to controlthe output level, and a band-pass filter frequency for controlling the pitch ofthe feedback. This control parameter space is mapped to a corpus of soundscreated by these parameters and recorded, and these sounds are analysed usingsoftware created for concatenative synthesis. Following this process, thedescriptors taken from the analysis can be used to select control parametersfrom the feedback system.},
address = {Daejeon, Republic of Korea},
author = {Ferguson, Sam and Martin, Aengus and Johnston, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
pages = {541--546},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A corpus-based method for controlling guitar feedback}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Guedes2007,
abstract = {In this demonstration, I exemplify how a musical channel ofcommunication can be established in computer-mediatedinteraction between musicians and dancers in real time. Thischannel of communication uses a software libraryimplemented as a library of external objects for Max/MSP[1],that processes data from an object or library that performsframe-differencing analysis of a video stream in real time inthis programming environment.},
address = {New York City, NY, United States},
author = {Guedes, Carlos},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {dance,in dance,interaction between music and,interactive,interactive dance,interactive performance,musical rhythm and rhythm,nime07,performance systems},
pages = {417--419},
title = {{Establishing a Musical Channel of Communication between Dancers and Musicians in Computer-Mediated Collaborations in Dance Performance}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}417.pdf},
year = {2007}
}
@inproceedings{Martin:2013,
abstract = {This paper describes the development of an Apple iPhone based mobile computersystem for vibraphone and its use in a series of the author's performanceprojects in 2011 and 2012.This artistic research was motivated by a desire to develop an alternative tolaptop computers for the author's existing percussion and computer performancepractice. The aims were to develop a light, compact and flexible system usingmobile devices that would allow computer music to infiltrate solo and ensembleperformance situations where it is difficult to use a laptop computer.The project began with a system that brought computer elements to NordligVinter, a suite of percussion duos, using an iPhone, RjDj, Pure Data and ahome-made pickup system. This process was documented with video recordings andanalysed using ethnographic methods.The mobile computer music setup proved to be elegant and convenient inperformance situations with very little time and space to set up, as well as inperformance classes and workshops. The simple mobile system encouragedexperimentation and the platforms used enabled sharing with a wider audience.},
address = {Daejeon, Republic of Korea},
author = {Martin, Charles},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Apple iOS,artistic research,collaborative performance practice,ethnography,mobile computer music,percussion},
pages = {377--380},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Performing with a Mobile Computer System for Vibraphone}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{nime2014:sserafin,
abstract = {In this paper we propose an empirical method to develop mapping strategies between a gestural based interface (the Gloves) and physically based sound synthesis models. An experiment was performed in order to investigate which kind of gestures listeners associate to synthesised sounds produced using physical models, corresponding to three categories of sound: sustained, iterative and impulsive. The results of the experiment show that listeners perform similar gestures when controlling sounds from the different categories. We used such gestures in order to create the mapping strategy between the Gloves and the physically based synthesis engine.},
address = {London, United Kingdom},
author = {Serafin, Stefania and Trento, Stefano and Grani, Francesco and Perner-Wilson, Hannah and Madgwick, Seb and Mitchell, Tom},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {521--524},
publisher = {Goldsmiths, University of London},
title = {{Controlling Physically Based Virtual Musical Instruments Using The Gloves}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}307.pdf},
year = {2014}
}
@inproceedings{MakiPatola2006,
address = {Paris, France},
author = {Maki-Patola, Teemu and H{\"{a}}m{\"{a}}l{\"{a}}inen, Perttu and Kanerva, Aki},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {1,2,2 9,3897,39,425,43,7,8,9},
pages = {364--369},
title = {{The Augmented Djembe Drum - Sculpting Rhythms}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}364.pdf},
volume = {73},
year = {2006}
}
@inproceedings{Bahn2001,
address = {Seattle, WA, United States},
author = {Bahn, Curtis and Trueman, Dan},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
keywords = {afue lqwhuidfh h,composed instruments,dpsohv kwpo,hohfwurqlf pxvlf,kwws pxvlf sulqfhwrq hgx,lv dq lpsurylvdwru,lqwhuidfh},
pages = {19--23},
title = {{interface : Electronic Chamber Ensemble}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}019.pdf},
year = {2001}
}
@inproceedings{Wyse:2012,
abstract = {The upper limit of frequency sensitivity for vibrotactile stimulation of the fingers and hand is commonly accepted as 1 kHz. However, during the course of our research to develop a full-hand vibrotactile musical communication device for the hearing-impaired, we repeatedly found evidence suggesting sensitivity to higher frequencies. Most of the studies on which vibrotactile sensitivity are based have been conducted using sine tones delivered by point-contact actuators. The current study was designed to investigate vibrotactile sensitivity using complex signals and full, open-hand contact with a flat vibrating surface representing more natural environmental conditions. Sensitivity to frequencies considerably higher than previously reported was demonstrated for all the signal types tested. Furthermore, complex signals seem to be more easily detected than sine tones, especially at low frequencies. Our findings are applicable to a general understanding of sensory physiology, and to the development of new vibrotactile display devices for music and other applications.},
address = {Ann Arbor, Michigan},
author = {Wyse, Lonce and Nanayakkara, Suranga and Seekings, Paul and Ong, Sim Heng and Taylor, Elizabeth},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Haptic Sensitivity,Hearing-impaired,Vibrotactile Threshold},
publisher = {University of Michigan},
title = {{Palm-area sensitivity to vibrotactile stimuli above 1 kHz}},
year = {2012}
}
@inproceedings{Lai:2013,
abstract = {This paper presents observations from investigating audience experience of apractice-based research in live sound performance with electronics. In seekingto understand the communication flow and the engagement between performer andaudience in this particular performance context, we designed an experiment thatinvolved the following steps: (a) performing WOSAWIP at a new media festival,(b) conducting a qualitative research study with audience members and (c)analyzing the data for new insights.},
address = {Daejeon, Republic of Korea},
author = {Lai, Chi-Hsia and Bovermann, Till},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Audience Experience Study,Evaluation,Live Performance,Research Methods},
pages = {170--173},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Audience Experience in Sound Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Lyon2009,
address = {Pittsburgh, PA, United States},
author = {Lyon, Eric and Knapp, Benjamin and Ouzounian, Gascia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {327},
title = {{Biomuse Trio}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}327.pdf},
year = {2009}
}
@inproceedings{Schacher2011,
address = {Oslo, Norway},
author = {Schacher, Jan C and Stoecklin, Angela},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {interactive dance,motion,motion and gesture,sonification},
number = {June},
pages = {292--295},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Traces  Body, Motion and Sound}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}292.pdf},
year = {2011}
}
@inproceedings{Fan:2013,
abstract = {We show how body-centric sensing can be integrated in musical interface toenable more flexible gestural control. We present a barehanded body-centricinteraction paradigm where users are able to interact in a spontaneous waythroughperforming gestures. The paradigm employs a wearable camera and see-throughdisplay to enable flexible interaction in the 3D space. We designed andimplemented a prototype called Air Violin, a virtual musical instrument usingdepth camera, to demonstrate the proposed interaction paradigm. We describedthe design and implementation details.},
address = {Daejeon, Republic of Korea},
author = {Fan, Xin and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Kinect,NIME,gesture,interaction,musical instrument},
pages = {122--123},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Air Violin: A Body-centric Style Musical Instrument}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Fyfe2011,
address = {Oslo, Norway},
author = {Fyfe, Lawrence and Tindale, Adam and Carpendale, Sheelagh},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {multi-touch,open sound control,toolkit,tuio},
number = {June},
pages = {276--279},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{JunctionBox : A Toolkit for Creating Multi-touch Sound Control Interfaces}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}276.pdf},
year = {2011}
}
@inproceedings{Resch:2013,
abstract = {"note{\~{}} for Max - An extension for Max/MSP for Media Arts {\&} musicShort Paper/Poster {\&} Workshop proposal for the NIME 2013At the Electronic Studio/Research Department of the University of Music Baselwe have been developing the software note{\~{}}, which consists of 4 in theC-Programming language developed objects for Max/MSP. The essential concept ofnote{\~{}} is that of a classic MIDI sequencer: Recording, editing and playing backcontrol data. It provides a GUI, a scripting interface and feedback for everyinteraction so its functionality can be extended by common Max/MSP objects. Instead of using the limited MIDI format , note{\~{}} allows storing floating pointlists up to 1024 elements plus text within one event.In order to get a quick overview about note{\~{}}'s capabilities I recommendwatching the video ""noteForMax{\_}IntroductionToNote{\~{}}"".Instead of doing a demonstration for the short paper I would like to propose a3 hours workshop for note{\~{}}:The first half of the workshop would be an introduction to note{\~{}}, its basicfeatures and how to integrate it into the Max/MSP environment as it is shown inthe videos""noteForMax{\_}Webcast1"" and ""noteForMax{\_}Webcast2"".In the second half, some of the examples shown in the videos""noteForMax{\_}IntroductionToNote{\~{}}"", ""noteForMax{\_}theMakingOfWings{\&}Halos"" and""noteForMax{\_}WhatsNewInTheUpcomingRelease""would be explained in detail:- Extending note{\~{}}'s functionality with Max Programming- Synchronizing several note{\~{}} objects in order to create complex polyrhythmicstructures- Synchronizing audio-, event- and video- playback with note{\~{}}- Using the note.score object for live score generation- Importing data from SDIF- and Excel-Files and work with them algorithmically"},
address = {Daejeon, Republic of Korea},
author = {Resch, Thomas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {GUI,Max/MSP,composing,notation.,score,sequencing,timeline},
pages = {210--212},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{note{\~{}} for Max - An extension for Max/MSP for Media Arts {\&} music}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{nime2014:asarasua,
abstract = {Many musical interfaces have used the musical conductor metaphor, allowing users to control the expressive aspects of a performance by imitating the gestures of conductors. In most of them, the rules to control these expressive aspects are predefined and users have to adapt to them. Other works have studied conductors' gestures in relation to the performance of the orchestra. The goal of this study is to analyze, following the path initiated by this latter kind of works, how simple motion capture descriptors can explain the relationship between the loudness of a given performance and the way in which different subjects move when asked to impersonate the conductor of that performance. Twenty-five subjects were asked to impersonate the conductor of three classical music fragments while listening to them. The results of different linear regression models with motion capture descriptors as explanatory variables show that, by studying how descriptors correlate to loudness differently among subjects, different tendencies can be found and exploited to design models that better adjust to their expectations.},
address = {London, United Kingdom},
author = {Saras{\'{u}}a, Alvaro and Guaus, Enric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {195--200},
publisher = {Goldsmiths, University of London},
title = {{Dynamics in Music Conducting: A Computational Comparative Study Among Subjects}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}464.pdf},
year = {2014}
}
@inproceedings{Hochenbaum:2012,
abstract = {In this paper we present a multimodal system for analyzing drum performance. In the first example we perform automatic drum hand recognition utilizing a technique for automatic labeling of training data using direct sensors, and only indirect sensors (e.g. a microphone) for testing. Left/Right drum hand recognition is achieved with an average accuracy of 84.95{\%} for two performers. Secondly we provide a study investigating multimodality dependent performance metrics analysis.},
address = {Ann Arbor, Michigan},
author = {Hochenbaum, Jordan and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Drum stroke identification,Multimodality,machine learning,music information retrieval,performance metrics,surrogate data training,surrogate sensors},
publisher = {University of Michigan},
title = {{Drum Stroke Computing: Multimodal Signal Processing for Drum Stroke Identification and Performance Metrics}},
year = {2012}
}
@inproceedings{nime2014:arenaud,
abstract = {This paper provides an overview of a proposed demonstration of 3DinMotion, a system using real time motion capture of one or several subjects, which can be used in interactive audiovisual pieces and network performances. The skeleton of a subject is analyzed in real time and displayed as an abstract avatar as well as sonified based on mappings and rules to make the interplay experience lively and rewarding. A series of musical pieces have been composed for the interface following cueing strategies. In addition a second display, ``the prompter" guides the users through the piece. 3DinMotion has been developed from scratch and natively, leading to a system with a very low latency, making it suitable for real time music interactions. In addition, 3DinMotion is fully compatible with the OpenSoundControl (OSC) protocol, allowing expansion to commonly used musical and sound design applications.},
address = {London, United Kingdom},
author = {Renaud, Alain and Charbonnier, Caecilia and Chagu{\'{e}}, Sylvain},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {495--496},
publisher = {Goldsmiths, University of London},
title = {{3DinMotion - A Mocap Based Interface for Real Time Visualisation and Sonification of Multi-User Interactions}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}423.pdf},
year = {2014}
}
@inproceedings{Till2007,
address = {New York City, NY, United States},
author = {Till, Bernie C and Benning, Manjinder S and Livingston, Nigel},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {human-computer,inertial sensing,music controller,nime07,sensing},
pages = {403--404},
title = {{Wireless Inertial Sensor Package (WISP)}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}403.pdf},
year = {2007}
}
@inproceedings{Lee2011,
address = {Oslo, Norway},
author = {Lee, Jeong-seob and Yeo, Woon Seung},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {audible sound,doppler effect,free interface,hand-,interactive performance,musical instrument,stereotypical transducers},
number = {June},
pages = {24--27},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Sonicstrument : A Musical Interface with Stereotypical Acoustic Transducers}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}024.pdf},
year = {2011}
}
@inproceedings{Lee:2013,
abstract = {This work aims at a music piece for large-scale audience participation usingmobile phones as musical instruments at a music performance. Utilizing theubiquity of smart phones, we attempted to accomplish audience engagement bycrafting an accessible musical instrument with which audience can be a part ofthe performance. Drawing lessons learnt from the creative works of mobilemusic, audience participation, and the networked instrument a mobile musicalinstrument application is developed so that audience can download the app atthe concert, play the instrument instantly, interact with other audiencemembers, and contribute to the music by sound generated from their mobilephones. The post-survey results indicate that the instrument was easy to use,and the audience felt connected to the music and other musicians.},
address = {Daejeon, Republic of Korea},
author = {Lee, Sang Won and Freeman, Jason},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {audience participation,mobile music,networked instrument},
pages = {450--455},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{echobo : Audience Participation Using The Mobile Music Instrument}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{chonigman:2014,
abstract = {This paper introduces a new technique for creating Swept Frequency Capacitive Sensing with open source technology for use in creating richer and more complex musical gestures. This new style of capacitive touch sensing is extremely robust compared to older versions and will allow greater implementation of gesture recognition and touch control in the development of NIMEs. Inspired by the Touch{\{}{\'{e}}{\}} project, this paper discusses how to implement this technique using the community standard hardware Arduino instead of custom designed electronics. The technique requires only passive components and can be used to enhance the touch sensitivity of many everyday objects and even biological materials and substances such as plants, which this paper will focus on as a case study through the project known as Cultivating Frequencies. This paper will discuss different techniques of filtering data captured by this system, different methods for creating gesture recognition unique to the object being used, and the implications of this technology as it pertains to the goal of ubiquitous sensing. Furthermore, this paper will introduce a new Arduino Library, SweepingCapSense, which simplifies the coding required to implement this technique.},
address = {London, United Kingdom},
author = {Honigman, Colin and Hochenbaum, Jordan and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {74--77},
publisher = {Goldsmiths, University of London},
title = {{Techniques in Swept Frequency Capacitive Sensing: An Open Source Approach}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}515.pdf},
year = {2014}
}
@inproceedings{Astrinaki:2012,
abstract = {In this paper, we describe our pioneering work in developing speech synthesis beyond the Text-To-Speech paradigm. We introduce tangible speech synthesis as an alternate way of envisioning how artificial speech content can be produced. Tangible speech synthesis refers to the ability, for a given system, to provide some physicality and interactivity to important speech production parameters. We present MAGE, our new software platform for high-quality reactive speech synthesis, based on statistical parametric modeling and more particularly hidden Markov models. We also introduce a new HandSketch-based musical instrument. This instrument brings pen and posture based interaction on the top of MAGE, and demonstrates a first proof of concept.},
address = {Ann Arbor, Michigan},
author = {Astrinaki, Maria and D'Alessandro, Nicolas and Dutoit, Thierry},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {HTS,Hidden Markov Models,MAGE,performative,software library,speech synthesis,tangible interaction},
publisher = {University of Michigan},
title = {{MAGE -- A Platform for Tangible Speech Synthesis}},
year = {2012}
}
@inproceedings{Cassinelli2010,
address = {Sydney, Australia},
author = {Cassinelli, Alavaro and Kuribara, Yusaku and Zerroug, Alexis and Ishikawa, Masatoshi and Manabe, Daito},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {2,5,and humanities,arts,h,h5,interaction styles,j,methodologies and techniques,music computing,nime10,performing arts,sound and,user interfaces},
number = {Nime},
pages = {144--149},
title = {{scoreLight : Playing with a Human-Sized Laser Pick-Up}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}144.pdf},
year = {2010}
}
@inproceedings{Keating2007,
abstract = {In this paper, design scenarios made possible by the use of an interactive illuminated floor as the basis of an audiovisual environment are presented. By interfacing a network of pressure sensitive, light-emitting tiles with a 7.1 channel speaker system and requisite audio software, many avenues for collaborative expression emerge, as do heretofore unexplored modes of multiplayer music and dance gaming. By giving users light and sound cues that both guide and respond to their movement, a rich environment is created that playfully integrates the auditory, the visual, and the kinesthetic into a unified interactive experience.},
address = {New York City, NY, United States},
author = {Keating, Noah H},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Audiovisual Play,Immersive Dance,Kinetic Games,Movement Rich Game Play,Responsive Environments,Smart Floor},
pages = {338--343},
title = {{The Lambent Reactive : An Audiovisual Environment for Kinesthetic Playforms}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}338.pdf},
year = {2007}
}
@inproceedings{Oh:2013,
abstract = {"Significant progress in the domains of speech- and singing-synthesis hasenhanced communicative potential of machines. To make computers more vocallyexpressive, however, we need a deeper understanding of how nonlinguistic socialsignals are patterned and perceived. In this paper, we focus on laughterexpressions: how a phrase of vocalized notes that we call ""laughter"" may bemodeled and performed to implicate nuanced meaning imbued in the acousticsignal. In designing our model, we emphasize (1) using high-level descriptorsas control parameters, (2) enabling real-time performable laughter, and (3)prioritizing expressiveness over realism. We present an interactive systemimplemented in ChucK that allows users to systematically play with the musicalingredients of laughter. A crowdsourced study on the perception of synthesizedlaughter showed that our model is capable of generating a range of laughtertypes, suggesting an exciting potential for expressive laughter synthesis."},
address = {Daejeon, Republic of Korea},
author = {Oh, Jieun and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {interface for musical expression,laughter,real-time controller,synthesis model,vocalization},
pages = {190--195},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{LOLOL: Laugh Out Loud On Laptop}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{jo:2013,
abstract = {In this paper, we present a method to produce analog records with standardvector graphics software (i.e. Adobe Illustrator) and two different types ofcutting machines: laser cutter, and paper cutter. The method enables us toengrave wave forms on a surface of diverse materials such as paper, wood,acrylic, and leather without or with prior acoustic information (i.e. digitalaudio data). The results could be played with standard record players. Wepresent the method with its technical specification and explain our initialtrials with two performances and a workshop. The work examines the role ofmusical reproduction in the age of personal fabrication. ---p.s. If it's possible, we also would like to submit the work for performanceand workshop.A video of performance {\textless} it contains information on the authorshttp://www.youtube.com/watch?v=vbCLe06P7j0},
address = {Daejeon, Republic of Korea},
author = {Kazuhiro jo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Analog Record,Media Archaeology,Personal Fabrication},
pages = {283--286},
publisher = {Graduate School of Culture Technology, KAIST},
title = {cutting record - a record without (or with) prior acoustic information},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Levin2005a,
abstract = {We report on The Manual Input Sessions, a series of audiovisual vignettes which probe the expressive possibilities of free-form hand gestures. Performed on a hybrid projection system which combines a traditional analog overhead projector and a digital PC video projector, our vision-based software instruments generate dynamic sounds and graphics solely in response to the forms and movements of the silhouette contours of the user's hands. Interactions and audiovisual mappings which make use of both positive (exterior) and negative (interior) contours are discussed. },
address = {Vancouver, BC, Canada},
author = {Levin, Golan and Lieberman, Zachary},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Audiovisual performance,augmented reality.,computer vision,contour analysis,hand silhouettes,sound-image relationships},
pages = {115--120},
title = {{Sounds from Shapes: Audiovisual Performance with Hand Silhouette Contours in The Manual Input Sessions}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}115.pdf},
year = {2005}
}
@inproceedings{Hauert2007,
address = {New York City, NY, United States},
author = {Hauert, Sibylle and Reichmuth, Daniel and B{\"{o}}hm, Volker},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {4057},
title = {{Instant City, a Music Building Game Table}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}422.pdf},
year = {2007}
}
@inproceedings{Solis2010,
address = {Sydney, Australia},
author = {Solis, Jorge and Petersen, Klaus and Yamamoto, Tetsuro and Takeuchi, Masaki and Ishikawa, Shimpei and Takanishi, Atsuo and Hashimoto, Kunimatsu},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {nime10},
number = {Nime},
pages = {156--161},
title = {{Development of the Waseda Saxophonist Robot and Implementation of an Auditory Feedback Control}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}156.pdf},
year = {2010}
}
@inproceedings{Dahl2010,
address = {Sydney, Australia},
author = {Dahl, Luke and Wang, Ge},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {design,gameplay,metaphor,mobile music,nime10,performance},
number = {Nime},
pages = {178--181},
title = {{Sound Bounce : Physical Metaphors in Designing Mobile Music Performance}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}178.pdf},
year = {2010}
}
@inproceedings{Morris2004,
address = {Hamamatsu, Japan},
author = {Morris, Geoffrey C and Leitman, Sasha and Kassianidou, Marina},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
pages = {201--202},
title = {{SillyTone Squish Factory}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}201.pdf},
year = {2004}
}
@inproceedings{Hoste:2013,
abstract = {Nowadays many music artists rely on visualisations and light shows to enhanceand augment their live performances. However, the visualisation and triggeringof lights is normally scripted in advance and synchronised with the concert,severely limiting the artist's freedom for improvisation, expression and ad-hocadaptation of their show. These scripts result in performances where thetechnology enforces the artist and their music to stay in synchronisation withthe pre-programmed environment. We argue that these limitations can be overcomebased on emerging non-invasive tracking technologies in combination with anadvanced gesture recognition engine.We present a solution that uses explicit gestures and implicit dance moves tocontrol the visual augmentation of a live music performance. We furtherillustrate how our framework overcomes existing limitations of gestureclassification systems by delivering a precise recognition solution based on asingle gesture sample in combination with expert knowledge. The presentedsolution enables a more dynamic and spontaneous performance and, when combinedwith indirect augmented reality, results in a more intense interaction betweenthe artist and their audience.},
address = {Daejeon, Republic of Korea},
author = {Hoste, Lode and Signer, Beat},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {3D gesture recognition,Expressive control,Kinect,augmented reality,declarative language,live music performance},
pages = {13--18},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Expressive Control of Indirect Augmented Reality During Live Music Performances}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Tache:2012,
abstract = {Force-feedback and physical modeling technologies now allow to achieve the same kind of relation with virtual instruments as with acoustic instruments, but the design of such elaborate models needs guidelines based on the study of the human sensory-motor system and behaviour. This article presents a qualitative study of a simulated instrumental interaction in the case of the virtual bowed string, using both waveguide and mass-interaction models. Subjects were invited to explore the possibilities of the simulations and to express themselves verbally at the same time, allowing us to identify key qualities of the proposed systems that determine the construction of an intimate and rich relationship with the users.},
address = {Ann Arbor, Michigan},
author = {Tache, Olivier and Sinclair, Stephen and Florens, Jean-Loup and Wanderley, Marcelo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Instrumental interaction,bowed string.,force-feedback,haptics,physical modeling,presence,simulation},
publisher = {University of Michigan},
title = {{Exploring audio and tactile qualities of instrumentality with bowed string simulations}},
year = {2012}
}
@inproceedings{Farwell2006,
address = {Paris, France},
author = {Farwell, Neal},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
keywords = {composition,electro-acoustic adaptation,emulation,illusion,improvisation,mapping,mute,trombone,ultrasonic},
pages = {358--363},
title = {{Adapting the Trombone: a Suite of Electro-acoustic Interventions for the Piece}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}358.pdf},
year = {2006}
}
@inproceedings{Hashida2007a,
abstract = {This paper introduces a system for improvisational musicalexpression that enables all users, novice and experienced, toperform intuitively and expressively. Users can generate musically consistent results through intuitive action, inputtingrhythm in a decent tempo. We demonstrate novel mappingways that reect user's input information more interactivelyand eectively in generating the music. We also present various input devices that allow users more creative liberty.},
address = {New York City, NY, United States},
author = {Hashida, Tomoko and Naemura, Takeshi and Sato, Takao},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Improvisation,a sense of tempo,interactive music},
pages = {407--408},
title = {{A System for Improvisational Musical Expression Based on Player {\{}$\backslash$textquoteright{\}} s Sense of Tempo}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}407.pdf},
year = {2007}
}
@inproceedings{nime2014:jherrera,
abstract = {A recently developed system that uses pitched sounds to discover relative 3D positions of a group of devices located in the same physical space is described. The measurements are coordinated over an IP network in a decentralized manner, while the actual measurements are carried out measuring the time-of-flight of the notes played by different devices. Approaches to sonify the discovery process are discussed. A specific instantiation of the system is described in detail. The melody is specified in the form of a score, available to every device in the network. The system performs the melody by playing different notes consecutively on different devices, keeping a consistent timing, while carrying out the inter-device measurements necessary to discover the geometrical configuration of the devices in the physical space.},
address = {London, United Kingdom},
author = {Kim, Hyung Suk and Herrera, Jorge and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {273--276},
publisher = {Goldsmiths, University of London},
title = {{Ping-Pong: Musically Discovering Locations}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}550.pdf},
year = {2014}
}
@inproceedings{Tidemann2011,
address = {Oslo, Norway},
author = {Tidemann, Axel},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {artificial intelli-,drumming,modeling human behaviour},
number = {June},
pages = {268--271},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{An Artificial Intelligence Architecture for Musical Expressiveness that Learns by Imitation}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}268.pdf},
year = {2011}
}
@inproceedings{Sheehan2004,
address = {Hamamatsu, Japan},
author = {Sheehan, Brian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {digital musical instruments,mapping,pd,real-time performance,scanned,sensors,shapetape,synthesis,tactile interfaces},
pages = {92--95},
title = {{The Squiggle: A Digital Musical Instrument}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}092.pdf},
year = {2004}
}
@inproceedings{Dahl:2012,
abstract = {Composing music for ensembles of computer-based instruments, such as laptop orchestra or mobile phone orchestra, is a multi-faceted and challenging endeavor whose parameters and criteria for success are ill-defined. In the design community, tasks with these qualities are known as wicked problems. This paper frames composing for computer-based ensemble as a design task, shows how Buchanan's four domains of design are present in the task, and discusses its wicked properties. The themes of visibility, risk, and embodiment, as formulated by Klemmer, are shown to be implicitly present in this design task. Composers are encouraged to address them explicitly and to take advantage of the practices of prototyping and iteration.},
address = {Ann Arbor, Michigan},
author = {Dahl, Luke},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Design,composition,instrument design,interaction design,laptop orchestra,mobile phone orchestra},
month = {may},
publisher = {University of Michigan},
title = {{Wicked Problems and Design Considerations in Composing for Laptop Orchestra}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}259.pdf},
year = {2012}
}
@inproceedings{Dimitrov2008,
address = {Genoa, Italy},
author = {Dimitrov, Smilen and Alonso, Marcos and Serafin, Stefania},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {contact fric-,motion sonification,nime08,physical model,reactable},
pages = {211--214},
title = {{Developing Block-Movement, Physical-Model Based Objects for the Reactable}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}211.pdf},
year = {2008}
}
@inproceedings{Dubrau2010,
address = {Sydney, Australia},
author = {Dubrau, Josh M and Havryliv, Mark},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {nime10},
pages = {467--468},
title = {{P[a]ra[pra]xis : Towards Genuine Realtime 'Audiopoetry'}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}467.pdf},
year = {2010}
}
@inproceedings{Kellum2009,
address = {Pittsburgh, PA, United States},
author = {Kellum, Greg and Crevoisier, Alain},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {multi-modal interface,multi-touch,nime,nime09,sonic interaction design},
pages = {242--245},
title = {{A Flexible Mapping Editor for Multi-touch Musical Instruments}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}242.pdf},
year = {2009}
}
@inproceedings{Waadeland2011,
address = {Oslo, Norway},
author = {Waadeland, Carl H},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {gesture,movement,rhythm performance,spectral analysis},
number = {June},
pages = {248--251},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Rhythm Performance from a Spectral Point of View}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}248.pdf},
year = {2011}
}
@inproceedings{Stead:2012,
abstract = {We describe a system that allows non-programmers to specify the grammar for a novel graphic score notation of theirown design, defining performance notations suitable for drawing in live situations on a surface such as a whiteboard. Thescore can be interpreted via the camera of a smartphone,interactively scanned over the whiteboard to control theparameters of synthesisers implemented in Overtone. Thevisual grammar of the score, and its correspondence to thesound parameters, can be defined by the user with a simple visual condition-action language. This language can beedited on the touchscreen of an Android phone, allowingthe grammar to be modified live in performance situations.Interactive scanning of the score is visible to the audience asa performance interface, with a colour classifier and visualfeature recogniser causing the grammar-specified events tobe sent using OSC messages via Wi-Fi from the hand-heldsmartphone to an audio workstation.},
address = {Ann Arbor, Michigan},
author = {Stead, Alistair G and Blackwell, Alan F and Aaron, Samual},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {Com- puter Vision,Disposable Notation,Graphic Notation,Live Coding,Mobile Music},
publisher = {University of Michigan},
title = {{Graphic Score Grammars for End-Users}},
year = {2012}
}
@inproceedings{Fuhrmann:2013,
abstract = {This paper demonstrates how to use multiple Kinect(TM) sensors to map aperformers motion to music. We merge skeleton data streams from multiplesensors to compensate for occlusions of the performer. The skeleton jointpositions drive the performance via open sound control data. We discuss how toregister the different sensors to each other and how to smoothly merge theresulting data streams and how to map position data in a general framework tothe live electronics applied to a chamber music ensemble.},
address = {Daejeon, Republic of Korea},
author = {Fuhrmann, Anton and Kretz, Johannes and Burwik, Peter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {kinect,live electronics,motion tracking,multi sensor,open sound control,parameter mapping,sensor fusion},
pages = {358--362},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Multi Sensor Tracking for Live Sound Transformation}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Havel2004,
address = {Hamamatsu, Japan},
author = {Havel, Christophe and Desainte-Catherine, Myriam},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {gesture analysis,strike recognition,virtual percussion},
pages = {31--34},
title = {{Modeling an Air Percussion for Composition and Performance}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}031.pdf},
year = {2004}
}
@inproceedings{nime2014:mzareei,
abstract = {Over the past few decades, there has been an increasing number of musical instruments and works of sound art that incorporate robotics and mechatronics. This paper proposes a new approach in classification of such works and focuses on those whose ideological roots can be sought in Luigi Russolo's noise-intoners (intonarumori). It presents a discussion on works in which mechatronics is used to investigate new - and traditionally perceived as ``extra-musical" - sonic territories, and introduces Rasper: a new mechatronic noise-intoner that features an electromechanical apparatus to create noise physically, while regulating it rhythmically and timbrally.},
address = {London, United Kingdom},
author = {Zareei, Mo and Kapur, Ajay and Carnegie, Dale A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {473--478},
publisher = {Goldsmiths, University of London},
title = {{Rasper: a Mechatronic Noise-Intoner}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}268.pdf},
year = {2014}
}
@inproceedings{Maki-patola2005b,
abstract = {An experimental study comparing different user interfaces for a virtual drum is reported. Virtual here means that the drum is not a physical object. 16 subjects played the drum on five different interfaces and two metronome patterns trying to match their hits to the metronome clicks. Temporal accuracy of the playing was evaluated. The subjects also rated the interfaces subjectively. The results show that hitting the drum alternately from both sides with motion going through the drum plate was less accurate than the traditional one sided hitting. A physical stick was more accurate than a virtual computer graphic stick. Visual feedback of the drum slightly increased accuracy compared to receiving only auditory feedback. Most subjects evaluated the physical stick to offer a better feeling and to be more pleasant than the virtual stick. },
address = {Vancouver, BC, Canada},
author = {Maki-patola, Teemu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Virtual drum,feedback,musical instrument design,percussion instrument.,sound control,user interface,virtual reality},
pages = {144--147},
title = {{User Interface Comparison for Virtual Drums}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}144.pdf},
year = {2005}
}
@inproceedings{dAlessandro2011,
address = {Oslo, Norway},
author = {D'Alessandro, Nicolas and Calderon, Roberto and M{\"{u}}ller, Stefanie},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {agent,architecture,collaboration,figure 1,installation,instrument,interactive fabric,light,mo-,movements in the installation,space and,tion,voice synthesis},
pages = {132--135},
title = {{ROOM {\#} 81 -- Agent-Based Instrument for Experiencing Architectural and Vocal Cues}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}132.pdf},
year = {2011}
}
@inproceedings{Donnarumma:2013,
abstract = {We present the first combined use of the electromyogram (EMG) andmechanomyogram (MMG), two biosignals that result from muscular activity, forinteractive music applications. We exploit differences between these twosignals, as reported in the biomedical literature, to create bi-modalsonification and sound synthesis mappings that allow performers to distinguishthe two components in a single complex arm gesture. We study non-expertplayers' ability to articulate the different modalities. Results show thatpurposely designed gestures and mapping techniques enable novices to rapidlylearn to independently control the two biosignals.},
address = {Daejeon, Republic of Korea},
author = {Donnarumma, Marco and Caramiaux, Baptiste and Tanaka, Atau},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {EMG,MMG,NIME,biosignal,mapping,multimodal,sensorimotor system},
month = {may},
pages = {128--131},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Muscular Interactions. Combining {\{}EMG{\}} and MMG sensing for musical practice}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Oboe2002,
address = {Dublin, Ireland},
author = {Oboe, Roberto and {De Poli}, Giovanni},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {dynamic simulation,virtual mechanisms},
pages = {137--142},
title = {{Multi-instrument Virtual Keyboard  The MIKEY Project}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}137.pdf},
year = {2002}
}
@inproceedings{Schacher:2013a,
abstract = {This article describes the motivations and reflections that led to thedevelopment of a gestural sensor instrument called the Quarterstaff. In aniterative design and fabrication process, several versions of this interfacewere build, tested and evaluated in performances. A detailed explanation of thedesign choices concerning the shape but also the sensing capabilities of theinstrument illustrates the emphasis on establishing an `enactive'instrumental relationship. A musical practice for this type of instrument isshown by discussing the methods used in the exploration of the gesturalpotential of the interface and the strategies deployed for the development ofmappings and compositions. Finally, to gain more information about how thisinstrument compares with similar designs, two dimension-space analyses are madethat show a clear positioning in relation to instruments that precede theQuarterstaff.},
address = {Daejeon, Republic of Korea},
author = {Schacher, Jan C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Gestural sensor interface,body-object relation,composition and performance practice,dimension space analysis,instrument design},
pages = {535--540},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{The Quarterstaff, a Gestural Sensor Instrument}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Perrotin:2013,
abstract = {Touch user interfaces such as touchpad or pen tablet are often used forcontinuous pitch control in synthesis devices. Usually, pitch is set at thecontact point on the interface, thus introducing possible pitch inaccuracies atthe note onset. This paper proposes a new algorithm, based on an adaptiveattraction mapping, for improving initial pitch accuracy with touch userinterfaces with continuous control. At each new contact on the interface, thealgorithm adjusts the mapping to produce the most likely targeted note of thescale in the vicinity of the contact point. Then, pitch remains continuouslyadjustable as long as the contact is maintained, allowing for vibrato,portamento and other subtle melodic control. The results of experimentscomparing the users' pitch accuracy with and without the help of the algorithmshow that such a correction enables to play sharply in tune at the contact withthe interface, regardless the musical background of the player. Therefore, thedynamic mapping algorithm allows for a clean and accurate attack when playing touch user interfaces for controlling continuous pitch instruments like voicesynthesizers.},
address = {Daejeon, Republic of Korea},
author = {Perrotin, Olivier and D'Alessandro, Christophe},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Sound synthesis control,accuracy,automatic correction,pen tablet,precision,touch user interfaces},
pages = {186--189},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Adaptive mapping for improved pitch accuracy on touch user interfaces}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Lui:2013,
abstract = {Music is expressive and hard to be described by words. Learning music istherefore not a straightforward task especially for vocal music such as humanbeatboxing. People usually learn beatboxing in the traditional way of imitatingaudio sample without steps and instructions. Spectrogram contains a lot ofinformation about audio, but it is too complicated to be understood inreal-time. Reinforcement learning is a psychological method, which makes use ofreward and/or punishment as stimulus to train the decision-making process ofhuman. We propose a novel music learning approach based on the reinforcementlearning method, which makes use of compact and easy-to-read spectruminformation as visual clue to assist human beatboxing learning on smartphone.Experimental result shows that the visual information is easy to understand inreal-time, which improves the effectiveness of beatboxing self-learning.},
address = {Daejeon, Republic of Korea},
author = {Lui, Simon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Audio analysis,audio information retrieval.,music learning tool,reinforcement learning,smartphone app},
pages = {25--28},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Compact Spectrum-Assisted Human Beatboxing Reinforcement Learning Tool On Smartphone}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Nakanishi:2013,
abstract = {In this paper, the authors introduce an interactive device, ``POWDER BOX''for use by novices in musical sessions. ``POWDER BOX'' is equipped withsensor-based replaceable interfaces, which enable participants to discover andselect their favorite playing styles of musical instruments during a musicalsession. In addition, it has a wireless communication function thatsynchronizes musical scale and BPM between multiple devices. To date, various kinds of ``inventive'' electronic musical instruments havebeen created in the field of Computer Music field. The authors are interestedin formations of musical sessions, aiming for a balance between simpleinteraction and musical expression. This study focuses on the development ofperformance playing styles.Musicians occasionally change their playing styles (e.g., guitar pluckingstyle) during a musical session. Generally, it is difficult for nonmusicians toachieve this kind of smooth changing depends on levels of their skillacquisition. However, it is essentially important for enjoying musical sessionswhether people could acquire these skills. Here, the authors attempted to develop the device that supports nonmusicians toconquer this point using replaceable interfaces. The authors expected thatchanging interfaces would bring similar effect as changing playing style by theskillful player. This research aims to establish an environment in whichnonmusicians and musicians share their individual musical ideas easily. Here,the interaction design and configuration of the ``POWDER BOX'' is presented.},
address = {Daejeon, Republic of Korea},
author = {Nakanishi, Yoshihito and Matsumura, Seiichiro and Arakawa, Chuichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Musical instrument,replaceable interface,sensors,synthesizer},
pages = {373--376},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{POWDER BOX: An Interactive Device with Sensor Based Replaceable Interface For Musical Session}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Baalman2007,
address = {New York City, NY, United States},
author = {Baalman, Marije A and Moody-Grigsby, Daniel and Salter, Christopher L},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {adaptive audio scenography,audio dramaturgy,computing,dynamical systems,ing,interactive performance,nime07,situated,sound design,wireless sens-},
pages = {178--184},
title = {{Schwelle : Sensor Augmented, Adaptive Sound Design for Live Theatrical Performance}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}178.pdf},
year = {2007}
}
@inproceedings{Dimitrov2006,
address = {Paris, France},
author = {Dimitrov, Smilen and Serafin, Stefania},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
pages = {184--187},
title = {{A Simple Practical Approach to a Wireless Data Acquisition Board}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}184.pdf},
volume = {327},
year = {2006}
}
@inproceedings{Loscos2005,
address = {Vancouver, BC, Canada},
author = {Loscos, Alex and Aussenac, Thomas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {172--175},
title = {{The wahwactor: a voice controlled wah-wah pedal}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}172.pdf},
year = {2005}
}
@inproceedings{Murphy:2013,
abstract = {This paper provides a history of robotic guitars and bass guitars as well as adiscussion of the design, construction, and evaluation of two new roboticinstruments. Throughout the paper, a focus is made on different techniques toextend the expressivity of robotic guitars. Swivel and MechBass, two newrobots, are built and discussed. Construction techniques of likely interest toother musical roboticists are included. These robots use a variety oftechniques, both new and inspired by prior work, to afford composers andperformers with the ability to precisely control pitch and plucking parameters.Both new robots are evaluated to test their precision, repeatability, andspeed. The paper closes with a discussion of the compositional and performativeimplications of such levels of control, and how it might affect humans who wishto interface with the systems.},
address = {Daejeon, Republic of Korea},
author = {Murphy, Jim and McVay, James and Kapur, Ajay and Carnegie, Dale},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {kinetic sculpture,mechatronics,musical robotics},
pages = {557--562},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Designing and Building Expressive Robotic Guitars}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Mattek2010,
address = {Sydney, Australia},
author = {Mattek, Alison and Freeman, Mark and Humphrey, Eric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Computer-Assisted Composition,Multi-touch Interfaces},
pages = {479--480},
title = {{Revisiting Cagean Composition Methodology with a Modern Computational Implementation}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}479.pdf},
year = {2010}
}
@inproceedings{nime2014:asa,
abstract = {The text exposes a perceptual approach to instrument design and composition, and it introduces an instrument that outputs acoustic sound, digital sound, and digital image. We explore disparities between human perception and digital analysis as creative material. Because the instrument repurposes software intended to create video games, we establish a distinction between the notion of ``flow" in music and gaming, questioning how it may substantiate in interaction design. Furthermore, we extrapolate from cognition/attention research to describe how the projected image creates a reactive stage scene without deviating attention from the music.},
address = {London, United Kingdom},
author = {Sa, Adriana},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {331--334},
publisher = {Goldsmiths, University of London},
title = {{Repurposing Video Game Software for Musical Expression: A Perceptual Approach}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}343.pdf},
year = {2014}
}
@inproceedings{Gao2009,
address = {Pittsburgh, PA, United States},
author = {Gao, Mingfei and Hanson, Craig},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {live performance interface,lumi,nime09,pressure},
pages = {58--59},
title = {{LUMI : Live Performance Paradigms Utilizing Software Integrated Touch Screen and Pressure Sensitive Button Matrix}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}058.pdf},
year = {2009}
}
@inproceedings{Carter2005,
address = {Vancouver, BC, Canada},
author = {Carter, William and Liu, Leslie S},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
isbn = {1323422277},
pages = {176--179},
title = {{Location33: A Mobile Musical}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}176.pdf},
year = {2005}
}
@inproceedings{Kapur:2013,
abstract = {This paper describes the creation of new interfaces that extend traditionalKorean music and dance. Specifically, this research resulted in the design ofthe eHaegum (Korean bowed instrument), eJanggu (Korean drum), and ZiOm wearableinterfaces. The paper describes the process of making these new interfaces aswell as how they have been used to create new music and forms of digital artmaking that blend traditional practice with modern techniques.},
address = {Daejeon, Republic of Korea},
author = {Kapur, Ajay and Kim, Dae Hong and Kapur, Raakhi and Eom, Kisoon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Hyperinstrument,Korean interface design,bowed controllers,dance controllers,drum controllers,wearable sensors},
pages = {45--48},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{New Interfaces for Traditional Korean Music and Dance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Knapp2006,
address = {Paris, France},
author = {Knapp, Benjamin and Cook, Perry R},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {Community-Institutional Relations,Health Services Accessibility,Medically Uninsured,Organizational Case Studies,Primary Health Care,Public-Private Sector Partnerships,San Francisco},
pages = {124--128},
title = {{Creating a Network of Integral Music Controllers}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}124.pdf},
year = {2006}
}
@inproceedings{Choi2010,
address = {Sydney, Australia},
author = {Choi, Hongchan and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {algorithmic composition,audiovisual,automata,behavior simulation,music,music sequencer,musical interface,nime10,visualization},
number = {June},
pages = {15--18},
title = {{LUSH : An Organic Eco + Music System}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}112.pdf},
year = {2010}
}
@inproceedings{Nixdorf2006,
abstract = {In this paper we will report on the use of real-time soundspatialization in Challenging Bodies, a trans-disciplinaryperformance project at the University of Regina. Usingwell-understood spatialization techniques mapped to a custom interface, a computer system was built that allowedlive spatial control of ten sound signals from on-stage performers. This spatial control added a unique dynamic element to an already ultramodern performance. The systemis described in detail, including the main advantages overexisting spatialization systems: simplicity, usability, customization and scalability},
address = {Paris, France},
author = {Nixdorf, Joshua J and Gerhard, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {gem,live systems,pd,performance sys-,real-time systems,sound architecture,sound localization,sound spatialization,surround sound,tems},
pages = {318--321},
title = {{Real-time Sound Source Spatialization as Used in Challenging Bodies: Implementation and Performance}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}318.pdf},
year = {2006}
}
@inproceedings{cnash:2014,
abstract = {This paper explores the concept of end-user programming languages in music composition, and introduces the Manhattan system, which integrates formulas with a grid-based style of music sequencer. Following the paradigm of spreadsheets, an established model of end-user programming, Manhattan is designed to bridge the gap between traditional music editing methods (such as MIDI sequencing and typesetting) and generative and algorithmic music -seeking both to reduce the learning threshold of programming and support flexible integration of static and dynamic musical elements in a single work. Interaction draws on rudimentary knowledge of mathematics and spreadsheets to augment the sequencer notation with programming concepts such as expressions, built-in functions, variables, pointers and arrays, iteration (for loops), branching (goto), and conditional statements (if-then-else). In contrast to other programming tools, formulas emphasise the visibility of musical data (e.g. notes), rather than code, but also allow composers to interact with notated music from a more abstract perspective of musical processes. To illustrate the function and use cases of the system, several examples of traditional and generative music are provided, the latter drawing on minimalism (process-based music) as an accessible introduction to algorithmic composition. Throughout, the system and approach are evaluated using the cognitive dimensions of notations framework, together with early feedback for use by artists.},
address = {London, United Kingdom},
author = {Nash, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {221--226},
publisher = {Goldsmiths, University of London},
title = {{Manhattan: End-User Programming for Music}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}371.pdf},
year = {2014}
}
@inproceedings{Chew2005,
abstract = {In the Expression Synthesis Project (ESP), we propose adriving interface for expression synthesis. ESP aims toprovide a compelling metaphor for expressive performance soas to make high-level expressive decisions accessible to nonexperts. In ESP, the user drives a car on a virtual road thatrepresents the music with its twists and turns; and makesdecisions on how to traverse each part of the road. The driver'sdecisions affect in real-time the rendering of the piece. Thepedals and wheel provide a tactile interface for controlling thecar dynamics and musical expression, while the displayportrays a first person view of the road and dashboard from thedriver's seat. This game-like interface allows non-experts tocreate expressive renderings of existing music without havingto master an instrument, and allows expert musicians toexperiment with expressive choice without having to firstmaster the notes of the piece. The prototype system has beentested and refined in numerous demonstrations. This paperpresents the concepts underlying the ESP system and thearchitectural design and implementation of a prototype.},
address = {Vancouver, BC, Canada},
author = {Chew, Elaine and Francois, Alexander R and Liu, Jie and Yang, Aaron},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Music expression synthesis system,driving interface.},
pages = {224--227},
title = {{ESP: A Driving Interface for Expression Synthesis}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}224.pdf},
year = {2005}
}
@inproceedings{nime2014:tmurray-browne,
abstract = {We introduce Harmonic Motion, a free open source toolkit for artists, musicians and designers working with gestural data. Extracting musically useful features from captured gesture data can be challenging, with projects often requiring bespoke processing techniques developed through iterations of tweaking equations involving a number of constant values -- sometimes referred to as `magic numbers'. Harmonic Motion provides a robust interface for rapid prototyping of patches to process gestural data and a framework through which approaches may be encapsulated, reused and shared with others. In addition, we describe our design process in which both personal experience and a survey of potential users informed a set of specific goals for the software.},
address = {London, United Kingdom},
author = {Murray-Browne, Tim and Plumbley, Mark},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {213--216},
publisher = {Goldsmiths, University of London},
title = {{Harmonic Motion: A Toolkit for Processing Gestural Data for Interactive Sound}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}273.pdf},
year = {2014}
}
@inproceedings{Kim-Boyle2005,
address = {Vancouver, BC, Canada},
author = {Kim-Boyle, David},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {238--239},
title = {{Musical Score Generation in Valses and Etudes}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}238.pdf},
year = {2005}
}
@inproceedings{Gruenbaum2007,
address = {New York City, NY, United States},
author = {Gruenbaum, Leon},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::;::},
keywords = {2,computer keyboard,interval,keyboard,microtonal,midi controller,musical instrument,nime07,pitch,relative,samchillian,the samchillian system},
pages = {256--259},
title = {{The Samchillian Tip Tip Tip Cheeepeeeee : A Relativistic Keyboard Instrument}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}256.pdf},
year = {2007}
}
@inproceedings{Taylor2013,
abstract = {What is the place for Internet Art within the paradigm of remote musicperformance? In this paper, we discuss techniques for live audiovisualstorytelling through the Web browsers of remote viewers. We focus on theincorporation of socket technology to create a real-time link between performerand audience, enabling manipulation of Web media directly within the eachaudience member's browser. Finally, we describe Plum Street, an onlinemultimedia performance, and suggest that by involving remote performance,appropriating Web media such as Google Maps, social media, and Web Audio intothe work, we can tell stories in a way that more accurately addresses modernlife and holistically fulfills the Web browser's capabilities as a contemporaryperformance instrument.},
address = {Daejeon, Republic of Korea},
author = {Taylor, Ben and Allison, Jesse},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Internet Art,Network Music,Remote Performance,Storytelling},
pages = {477--478},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Plum St: Live Digital Storytelling with Remote Browsers}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Jones2009,
address = {Pittsburgh, PA, United States},
author = {Jones, Daniel and Hodgson, Tim and Grant, Jane and Matthias, John and Outram, Nicholas and Ryan, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {distributed,emergent,environmental,installation,neural network,nime09,sound,streaming audio},
pages = {297--302},
title = {{The Fragmented Orchestra}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}297.pdf},
year = {2009}
}
@inproceedings{Leeuw2009,
abstract = {The Electrumpet is an enhancement of a normal trumpet with a variety of electronic sensors and buttons. It is a new hybrid instrument that facilitates simultaneous acoustic and electronic playing. The normal playing skills of a trumpet player apply to the new instrument. The placing of the buttons and sensors is not a hindrance to acoustic use of the instrument and they are conveniently located. The device can be easily attached to and detached from a normal Bb-trumpet. The device has a wireless connection with the computer through Bluetooth-serial (Arduino). Audio and data processing in the computer is effected by three separate instances of MAX/MSP connected through OSC (controller data) and Soundflower (sound data). The current prototype consists of 7 analogue sensors (4 valve-like potentiometers, 2 pressure sensors, 1 "Ribbon" controller) and 9 digital switches. An LCD screen that is controlled by a separate Arduino (mini) is attached to the trumpet and displays the current controller settings that are sent through a serial connection.},
address = {Pittsburgh, PA, United States},
author = {Leeuw, Hans},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Bluetooth,LCD,MAX/MSP.,OSC,Trumpet,low latency,multiple Arduinos},
pages = {193--198},
title = {{The Electrumpet , a Hybrid Electro-Acoustic Instrument}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}193.pdf},
year = {2009}
}
@inproceedings{Meier2010,
address = {Sydney, Australia},
author = {Meier, Max and Schranner, Max},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {algorithmic composition,nime10,soft constraints,tangible interaction},
number = {Nime},
pages = {501--504},
title = {{The Planets}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}501.pdf},
year = {2010}
}
@inproceedings{Park2010,
address = {Sydney, Australia},
author = {Park, Sihwa and Kim, Seunghun and Lee, Samuel and Yeo, Woon Seung},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {composition,geo-,map interface,mashup,musical sonification,nime10,online map service,referenced data},
number = {Nime},
pages = {331--334},
title = {{Online Map Interface for Creative and Interactive}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}331.pdf},
year = {2010}
}
@inproceedings{Gillian2009a,
address = {Pittsburgh, PA, United States},
author = {Gillian, Nicholas and O'Modhrain, Sile and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {audio games,gesture,mobile devices,nime09},
pages = {308--311},
title = {{Scratch-Off : A Gesture Based Mobile Music Game with Tactile Feedback}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}308.pdf},
year = {2009}
}
@inproceedings{Burtner2003,
address = {Montreal},
author = {Burtner, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Burtner - 2003 - Composing for the (dis)Embodied Ensemble Notational Systems in (dis)Appearances.pdf:pdf},
pages = {63--69},
title = {{Composing for the (dis)Embodied Ensemble : Notational Systems in (dis)Appearances}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}063.pdf},
year = {2003}
}
@inproceedings{Crawford2009,
address = {Pittsburgh, PA, United States},
author = {Crawford, Langdon and Fastenow, William D},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {149--150},
title = {{The Midi-AirGuitar , A serious Musical Controller with a Funny Name Music Technology Program}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}149.pdf},
year = {2009}
}
@inproceedings{Serafin2006,
address = {Paris, France},
author = {Serafin, Stefania and de G{\"{o}}tzen, Amalia and B{\"{o}}ttcher, Niels and Gelineck, Steven},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {everyday sounds,noise machines,physical models},
pages = {240--245},
title = {{Synthesis and Control of Everyday Sounds Reconstructing Russolo's Intonarumori}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}240.pdf},
year = {2006}
}
@inproceedings{Michon:2013,
abstract = {Black Box is a site based installation that allows users to create uniquesounds through physical interaction. The installation consists of a geodesicdome, surround sound speakers, and a custom instrument suspended from the apexof thedome. Audience members entering the space are able to create sound by strikingor rubbing the cube, and are able to control a delay system by moving the cubewithin the space.},
address = {Daejeon, Republic of Korea},
author = {Michon, Romain and Borins, Myles and Meisenholder, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Beagleboard,Embedded-Linux,Faust,Open Sound Control,PureData,Satellite CCRMA},
pages = {464--465},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{The Black Box}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Canazza2008,
address = {Genoa, Italy},
author = {Canazza, Sergio and Dattolo, Antonina},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {multimedia information systems,musical open work,nime08,software agents,zz-structures},
pages = {140--143},
title = {{New Data Structure for Old Musical Open Works}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}140.pdf},
year = {2008}
}
@inproceedings{Beilharz2010,
address = {Sydney, Australia},
author = {Beilharz, Kirsty and {Vande Moere}, Andrew and Stiel, Barbara and Calo, Claudia and Tomitsch, Martin and Lombard, Adrian},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {bimodal display,design aesthetics,multimodal expression,nime10,physical computing,sonification,visualisation,wearable display},
number = {Nime},
pages = {323--326},
title = {{Expressive Wearable Sonification and Visualisation : Design and Evaluation of a Flexible Display}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}323.pdf},
year = {2010}
}
@inproceedings{KITA:2013,
abstract = {When people learn using Web-based educational resources such as an LMS(Learning Management System) or other e-learning related systems, they aresitting in front of their own computer at home and are often physicallyisolated from other online learners. In some courses they are typically gettingin touch online with each others for doing some particular group workassignments, but most of the time they must do their own learning tasks alone.In other courses simply the individual assignments and quizzes are provided, sothe learners are alone all the time from the beginning until the end of thecourse.In order to keep the learners' motivation, it helps to feel other learnersdoing the same learning activities and belonging to the same course.Communicating formally or informally with other learners via Social NetworkingServices or something is one way for learners to get such a feeling, though ina way it might sometimes disturb their learning. Sonification of the access logof the e-learning system could be another indirect way to provide such afeeling.},
address = {Daejeon, Republic of Korea},
author = {KITA, Toshihiro and Osaka, Naotoshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Csound,Moodle,OSC (Open Sound Control),e-learning,online learners,realtime sonification},
pages = {198--199},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Providing a feeling of other remote learners' presence in an online learning environment via realtime sonification of Moodle access log}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Ferguson2010,
abstract = {In this paper, we describe a comparison between parameters drawn from 3-dimensional measurement of a dance performance, and continuous emotional response data recorded from an audience present during this performance. A continuous time series representing the mean movement as the dance unfolds is extracted from the 3-dimensional data. The audiences' continuous emotional response data are also represented as a time series, and the series are compared. We concluded that movement in the dance performance directly influences the emotional arousal response of the audience. },
address = {Sydney, Australia},
author = {Ferguson, Sam and Schubert, Emery and Stevens, Catherine},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Continuous Response.,Dance,Emotion,Motion Capture},
pages = {481--484},
title = {{Movement in a Contemporary Dance Work and its Relation to Continuous Emotional Response}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}481.pdf},
year = {2010}
}
@inproceedings{Cannon2003,
address = {Montreal, QC, Canada},
author = {Cannon, Cormac and Hughes, Stephen and O'Modhrain, Sile},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {3--8},
title = {{EpipE: Exploration of the Uilleann Pipes as a Potential Controller for Computer-based Music}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}003.pdf},
year = {2003}
}
@inproceedings{Oh:2013,
abstract = {"Significant progress in the domains of speech- and singing-synthesis hasenhanced communicative potential of machines. To make computers more vocallyexpressive, however, we need a deeper understanding of how nonlinguistic socialsignals are patterned and perceived. In this paper, we focus on laughterexpressions: how a phrase of vocalized notes that we call ""laughter"" may bemodeled and performed to implicate nuanced meaning imbued in the acousticsignal. In designing our model, we emphasize (1) using high-level descriptorsas control parameters, (2) enabling real-time performable laughter, and (3)prioritizing expressiveness over realism. We present an interactive systemimplemented in ChucK that allows users to systematically play with the musicalingredients of laughter. A crowdsourced study on the perception of synthesizedlaughter showed that our model is capable of generating a range of laughtertypes, suggesting an exciting potential for expressive laughter synthesis."},
address = {Daejeon, Republic of Korea},
author = {Oh, Jieun and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {interface for musical expression,laughter,real-time controller,synthesis model,vocalization},
pages = {190--195},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{LOLOL: Laugh Out Loud On Laptop}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Park:2013,
abstract = {"Since Euler's development of the Tonnetz in 1739, musicians, composers andinstrument designers have been fascinated with the concept of musicalisomorphism, the idea that by arranging tones by their harmonic relationshipsrather than by their physical properties, the common shapes of musicalconstructs will appear, facilitating learning and new ways of exploringharmonic spaces. The construction of isomorphic instruments, beyond limitedsquare isomorphisms present in many stringed instruments, has been a challengein the past for two reasons: The first problem, that of re-arranging noteactuators from their sounding elements, has been solved by digital instrumentdesign. The second, more conceptual problem, is that only a single isomorphismcan be designed for any one instrument, requiring the instrument designer (aswell as composer and performer) to ""lock in"" to a single isomorphism, or tohave a different instrument for each isomorphism in order to experiment. Musix(an iOS application) and Rainboard (a physical device) are two new musicalinstruments built to overcome this and other limitations of existing isomorphicinstruments. Musix was developed to allow experimentation with a wide varietyof different isomorphic layouts, to assess the advantages and disadvantages ofeach. The Rainboard consists of a hexagonal array of arcade buttons embeddedwith RGB-LEDs, which are used to indicate characteristics of the isomorphismcurrently in use on the Rainboard. The creation of these two instruments /experimentation platforms allows for isomorphic layouts to be explored in waysthat are not possible with existing instruments."},
address = {Daejeon, Republic of Korea},
author = {Park, Brett and Gerhard, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {hexagon,isomorphic,keyboard,mobile application},
pages = {319--324},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Rainboard and Musix: Building dynamic isomorphic interfaces}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Kondapalli2011,
address = {Oslo, Norway},
author = {Kondapalli, Ravi and Sung, Ben-Zhen},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {bluetooth,controller,daft datum,dance,dance pad,feet,musical interface,sensor sheet,wii},
number = {June},
pages = {140--141},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Daft Datum  An Interface for Producing Music Through Foot-based Interaction}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}140.pdf},
year = {2011}
}
@inproceedings{Liu:2013,
abstract = {Cloud Bridge is an immersive interactive audiovisual software interface forboth data exploration and artistic creation. It explores how information can besonified and visualized to facilitate findings, and eventually becomeinteractive musical compositions. Cloud Bridge functions as a multi-user,multimodal instrument. The data represents the history of items checked out bypatrons of the Seattle Public Library. A single user or agroup of users functioning as a performance ensemble participate in the pieceby interactively querying the database using iOS devices. Each device isassociated with aunique timbre and color for contributing to the piece, whichappears on large shared screens and a surround-sound system for allparticipants and observers. Cloud Bridge leads to a new media interactiveinterface utilizing audio synthesis, visualization and real-time interaction.},
address = {Daejeon, Republic of Korea},
author = {Liu, Qian and Han, Yoon Chung and Kuchera-Morin, JoAnn and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Data Sonification,Data Visualization,Open Sound Control,Sonic Interaction Design,Sonification,User Interface},
pages = {431--436},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Cloud Bridge: a Data-driven Immersive Audio-Visual Software Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Magnusson2007,
abstract = {This paper reports on a survey conducted in the autumn of 2006 with the objective to understand people's relationship to their musical tools. The survey focused on the question of embodiment and its different modalities in the fields of acoustic and digital instruments. The questions of control, instrumental entropy, limitations and creativity were addressed in relation to people's activities of playing, creating or modifying their instruments. The approach used in the survey was phenomenological, i.e. we were concerned with the experience of playing, composing for and designing digital or acoustic instruments. At the time of analysis, we had 209 replies from musicians, composers, engineers, designers, artists and others interested in this topic. The survey was mainly aimed at instrumentalists and people who create their own instruments or compositions in flexible audio programming environments such as SuperCollider, Pure Data, ChucK, Max/MSP, CSound, etc. },
address = {New York City, NY, United States},
author = {Magnusson, Thor and Mendieta, Enrike H},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Survey,affordances,audio programming.,constraints,embodiment,ergonomics,instrumental entropy,mapping,musical instruments,usability},
pages = {94--99},
title = {{The Acoustic, the Digital and the Body : A Survey on Musical Instruments}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}094.pdf},
year = {2007}
}
@inproceedings{Leroy2006,
address = {Paris, France},
author = {Leroy, Nicolas and Fl{\'{e}}ty, Emmanuel and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
pages = {204--207},
title = {{Reflective Optical Pickup For Violin}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}204.pdf},
year = {2006}
}
@inproceedings{Burtner2006,
abstract = {This paper explores the use of perturbation in designing multiperformer or multi-agent interactive musical interfaces. A problem with the multi-performer approach is how to cohesively organize the independent data inputs into useable control information for synthesis engines. Perturbation has proven useful for navigating multi-agent NIMEs. The ,
,
author's Windtree is discussed as an example multi-performer instrument in which perturbation is used for multichannel ecological modeling. The Windtree uses a physical system turbulence model controlled in real time by four performers. },
address = {Paris, France},
author = {Burtner, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {interface,mapping,movement,multi-agent,multi-performer,music composition,perturbation},
pages = {129--133},
title = {{Perturbation Techniques for Multi-Performer or Multi- Agent Interactive Musical Interfaces}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}129.pdf},
year = {2006}
}
@inproceedings{Giordano:2012,
abstract = {Force-feedback devices can provide haptic feedback duringinteraction with physical models for sound synthesis. However, low-end devices may not always provide high-fidelitydisplay of the acoustic characteristics of the model. This article describes an enhanced handle for the Phantom Omnicontaining a vibration actuator intended to display the highfrequency portion of the synthesized forces. Measurementsare provided to show that this approach achieves a morefaithful representation of the acoustic signal, overcominglimitations in the device control and dynamics.},
address = {Ann Arbor, Michigan},
author = {Giordano, Marcello and Sinclair, Stephen and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Haptics,audio,bowing,force feedback,interaction},
month = {may},
publisher = {University of Michigan},
title = {{Bowing a vibration-enhanced force feedback device}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}37.pdf},
year = {2012}
}
@inproceedings{Momeni2003,
address = {Montreal, QC, Canada},
author = {Momeni, Ali and Wessel, David},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {54--62},
title = {{Characterizing and Controlling Musical Material Intuitively with Geometric Models}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}054.pdf},
year = {2003}
}
@inproceedings{Wingstedt2005,
address = {Vancouver, BC, Canada},
author = {Wingstedt, Johnny and Liljedahl, Mats and Lindberg, Stefan and Berg, Jan},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::;::},
pages = {232--235},
title = {{REMUPP  An Interactive Tool for Investigating Musical Properties and Relations}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}232.pdf},
year = {2005}
}
@inproceedings{Lee:2012,
abstract = {Empatheater is a video playing system that is controlled by multimodal interaction. As the video is played, the user must interact and emulate predefined ``events'' for the video to continue on. The user is given the illusion of playing an active role in the unraveling video content and can empathize with the performer. In this paper, we report about user experiences with Empatheater when applied to musical video contents.},
address = {Ann Arbor, Michigan},
author = {Lee, Myunghee and Kim, Youngsun and Kim, Gerard},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Empathy,Interactive video,Multimodal interaction.,Music video,Musical event},
publisher = {University of Michigan},
title = {{Empathetic Interactive Music Video Experience}},
year = {2012}
}
@inproceedings{nime2014:gwakefield,
abstract = {We discuss live coding audio-visual worlds for large-scale virtual reality environments. We describe Alive, an instrument allowing multiple users to develop sonic and visual behaviors of agents in a virtual world, through a browser- based collaborative code interface, accessible while being immersed through spatialized audio and stereoscopic display. The interface adds terse syntax for query-based precise or stochastic selections and declarative agent manipulations, lazily-evaluated expressions for synthesis and behavior, event handling, and flexible scheduling.},
address = {London, United Kingdom},
author = {Wakefield, Graham and Roberts, Charlie and Wright, Matthew and Wood, Timothy and Yerkes, Karl},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {505--508},
publisher = {Goldsmiths, University of London},
title = {{Collaborative Live-Coding with an Immersive Instrument}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}328.pdf},
year = {2014}
}
@inproceedings{Stark2007,
address = {New York City, NY, United States},
author = {Stark, Adam M and Plumbley, Mark D and Davies, Matthew E},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {a beat-synchronous tremolo effect,audio effects,beat tracking,figure 1,im-,nime07,plemented as a vst,plug-in,real-time,the rate is controlled,vst plug-in},
pages = {344--345},
title = {{Real-Time Beat-Synchronous Audio Effects}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}344.pdf},
year = {2007}
}
@inproceedings{Miyama2010,
address = {Sydney, Australia},
author = {Miyama, Chikashi},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {computer music,hardware and software design,musical interface,nime10,sensor technologies},
number = {Nime},
pages = {380--382},
title = {{Peacock : A Non-Haptic 3D Performance Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}380.pdf},
year = {2010}
}
@inproceedings{Goto2004,
address = {Hamamatsu, Japan},
author = {Goto, Suguru and Suzuki, Takahiko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
pages = {207--208},
title = {{The Case Study of Application of Advanced Gesture Interface and Mapping Interface, Virtual Musical Instrument "Le SuperPolm" and Gesture Controller "BodySuit"}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}207.pdf},
year = {2004}
}
@inproceedings{Leroy2006,
address = {Paris, France},
author = {Leroy, Nicolas and Fl{\'{e}}ty, Emmanuel and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
pages = {204--207},
title = {{Reflective Optical Pickup For Violin}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}204.pdf},
year = {2006}
}
@inproceedings{Bernard:2002,
abstract = {This paper describes the design and development of several musical instruments and MIDI controllers built byDavid Bernard (as part of The Sound Surgery project:www.thesoundsurgery.co.uk) and used in club performances around Glasgow during 1995-2002. It argues that changing technologies and copyright are shifting ourunderstanding of music from "live art" to "recorded medium" whilst blurring the boundaries between sound and visual production.},
address = {Dublin},
author = {Bernard, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Live electronic music,MIDI controllers,SKINS digital hand drum.,audio-visual synchronisation,copyright,experimental instruments},
pages = {10--11},
title = {{Experimental Controllers for Live Electronic Music Performance (vs. Copyright).}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}010.pdf},
year = {2002}
}
@inproceedings{Kessous2006,
address = {Paris, France},
author = {Kessous, Lo{\"{i}}c and Castet, Julien and Arfib, Daniel},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {alternate controller,guitar,multidimensional,sensors,synthesizer},
pages = {192--195},
title = {{'GXtar', an Interface Using Guitar Techniques}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}192.pdf},
year = {2006}
}
@inproceedings{Roberts2010,
address = {Sydney, Australia},
author = {Roberts, Charles and Wright, Matthew and Kuchera-Morin, JoAnn and Putnam, Lance},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {AlloSphere,HCI,OSC,Vir- tual Reality,interactivity,mapping,multi-user,network,performance},
pages = {57--62},
title = {{Dynamic Interactivity Inside the AlloSphere}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}057.pdf},
year = {2010}
}
@inproceedings{Bottoni2007,
abstract = {This paper reports our experiments on using a dual-coreDSP processor in the construction of a user-programmablemusical instrument and controller called the TouchBox.},
address = {New York City, NY, United States},
author = {Bottoni, Paolo and Caporali, Riccardo and Capuano, Daniele and Faralli, Stefano and Labella, Anna and Pierro, Mario},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {DSP,controller,dual-core,synthesizer,touch-screen},
pages = {394--395},
title = {{Use of a Dual-Core {\{}DSP{\}} in a Low-Cost, Touch-Screen Based Musical Instrument}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}394.pdf},
year = {2007}
}
@inproceedings{Cassinelli2010,
abstract = {scoreLight is a playful musical instrument capable of generating sound from the lines of drawings as well as from theedges of three-dimensional objects nearby (including everyday objects, sculptures and architectural details, but alsothe performer's hands or even the moving silhouettes ofdancers). There is no camera nor projector: a laser spotexplores shapes as a pick-up head would search for soundover the surface of a vinyl record - with the significant difference that the groove is generated by the contours of thedrawing itself.},
address = {Sydney, Australia},
author = {Cassinelli, Alavaro and Kuribara, Yusaku and Zerroug, Alexis and Ishikawa, Masatoshi and Manabe, Daito},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {H5.2 [User Interfaces] interaction styles / H.5.5},
pages = {144--149},
title = {{scoreLight : Playing with a Human-Sized Laser Pick-Up}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}144.pdf},
year = {2010}
}
@inproceedings{Hankins2002,
address = {Dublin, Ireland},
author = {Hankins, Tim and Merrill, David and Robert, Jocelyn},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {collaborative,input devices,music controllers,real-time},
pages = {80--81},
title = {{Circular Optical Object Locator}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}080.pdf},
year = {2002}
}
@inproceedings{Flanigan2008,
address = {Genoa, Italy},
author = {Flanigan, Lesley and Doro, Andrew},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {a single thin metal,automation,diy,infra-instrument,interaction design,nime08,performing technology,plucked by motors with,repurposing of consumer technology,robotics,the guitar strings are},
pages = {349--351},
title = {{Plink Jet}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}349.pdf},
year = {2008}
}
@inproceedings{Chordia2008,
address = {Genoa, Italy},
author = {Chordia, Parag and Rae, Alex},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {automatic recognition,indian music,nime08,raag,raga},
pages = {331--334},
title = {{Real-Time Raag Recognition for Interactive Music}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}331.pdf},
year = {2008}
}
@inproceedings{Ariza:2012,
abstract = {This paper demonstrates the practical benefits and performance opportunities of using the dual-analog gamepad as a controller for real-time live electronics. Numerous diverse instruments and interfaces, as well as detailed control mappings, are described. Approaches to instrument and preset switching are also presented. While all of the instrument implementations presented are made available through the Martingale Pd library, resources for other synthesis languages are also described. },
address = {Ann Arbor, Michigan},
author = {Ariza, Christopher},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Controllers,computer music,dual-analog,gamepad,instrument,interface,joystick,live electronics},
publisher = {University of Michigan},
title = {{The Dual-Analog Gamepad as a Practical Platform for Live Electronics Instrument and Interface Design}},
year = {2012}
}
@inproceedings{Kapur2010,
address = {Sydney, Australia},
author = {Kapur, Ajay and Darling, Michael},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {dartron,digital classroom,laptop orchestra,machine orchestra,musical robotics,nime pedagogy,nime10,solenoid},
number = {Nime},
pages = {162--165},
title = {{A Pedagogical Paradigm for Musical Robotics}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}162.pdf},
year = {2010}
}
@inproceedings{Sharon2004,
address = {Hamamatsu, Japan},
author = {Sharon, Michael E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {accelerometers,gestural control,mapping,microcontrollers,midi,musical instruments,pd,pure data,synthesis},
pages = {213--214},
title = {{The Stranglophone: Enhancing Expressiveness In Live Electronic Music}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}213.pdf},
year = {2004}
}
@inproceedings{Stowell2008,
address = {Genoa, Italy},
author = {Stowell, Dan and Plumbley, Mark D and Bryan-Kinns, Nick},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {discourse analysis,evaluation,nime08,qualitative methods,voice},
pages = {81--86},
title = {{Discourse Analysis Evaluation Method for Expressive Musical Interfaces}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}081.pdf},
year = {2008}
}
@inproceedings{Kapur:2013,
abstract = {This paper describes the creation of new interfaces that extend traditionalKorean music and dance. Specifically, this research resulted in the design ofthe eHaegum (Korean bowed instrument), eJanggu (Korean drum), and ZiOm wearableinterfaces. The paper describes the process of making these new interfaces aswell as how they have been used to create new music and forms of digital artmaking that blend traditional practice with modern techniques.},
address = {Daejeon, Republic of Korea},
author = {Kapur, Ajay and Kim, Dae Hong and Kapur, Raakhi and Eom, Kisoon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Hyperinstrument,Korean interface design,bowed controllers,dance controllers,drum controllers,wearable sensors},
pages = {45--48},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{New Interfaces for Traditional Korean Music and Dance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Mullen2011,
abstract = {The use of non-invasive electroencephalography (EEG) in the experimental arts is not a novel concept. Since 1965, EEG has been used in a large number of, sometimes highly sophisticated, systems for musical and artistic expression. However, since the advent of the synthesizer, most such systems have utilized digital and/or synthesized media in sonifying the EEG signals. There have been relatively few attempts to create interfaces for musical expression that allow one to mechanically manipulate acoustic instruments by modulating one's mental state. Secondly, few such systems afford a distributed performance medium, with data transfer and audience participation occurring over the Internet. The use of acoustic instruments and Internet-enabled communication expands the realm of possibilities for musical expression in Brain-Computer Music Interfaces (BCMI), while also introducing additional challenges. In this paper we report and examine a first demonstration (Music for Online Performer) of a novel system for Internet-enabled manipulation of robotic acoustic instruments, with feedback, using a non-invasive EEG-based BCI and low-cost, commercially available robotics hardware. },
address = {Oslo, Norway},
author = {Mullen, Tim and Warp, Richard and Jansch, Adam},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Arduino.,Brain-Computer Music Interface,EEG,Internet},
pages = {469--472},
title = {{Minding the (Transatlantic) Gap: An Internet-Enabled Acoustic Brain-Computer Music Interface}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}469.pdf},
year = {2011}
}
@inproceedings{Tanaka2004,
address = {Hamamatsu, Japan},
author = {Tanaka, Atau},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {mobile music,peer-to-peer,wireless ad-hoc networks},
pages = {154--156},
title = {{Mobile Music Making}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}154.pdf},
year = {2004}
}
@inproceedings{Frounberg2010,
abstract = {The paper reports on the development of prototypes of glassinstruments. The focus has been on developing acousticinstruments specifically designed for electronic treatment,and where timbral qualities have had priority over pitch.The paper starts with a brief historical overview of glassinstruments and their artistic use. Then follows an overviewof the glass blowing process. Finally the musical use of theinstruments is discussed.},
address = {Sydney, Australia},
author = {Frounberg, Ivar and Innervik, Kjell Tore and Jensenius, Alexander R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {glass instruments,nime,nime10,performance practice},
pages = {287--290},
title = {{Glass Instruments -- From Pitch to Timbre}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}287.pdf},
year = {2010}
}
@inproceedings{Nagashima2009,
abstract = {This is a technical and experimental report of parallel processing, using the "Propeller" chip. Its eight 32 bits processors (cogs) can operate simultaneously, either independently or cooperatively, sharing common resources through a central hub. I introduce this unique processor and discuss about the possibility to develop interactive systems and smart interfaces in media arts, because we need many kinds of tasks at a same time with NIMErelated systems and installations. I will report about (1) Propeller chip and its powerful IDE, (2) external interfaces for analog/digital inputs/outputs, (3) VGA/NTSC/PAL video generation, (4) audio signal processing, and (5) originally-developed MIDI input/output method. I also introduce three experimental prototype systems.},
address = {Pittsburgh, PA, United States},
author = {Nagashima, Yoichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {MIDI,Propeller,interfaces.,parallel processing,sensor},
pages = {169--170},
title = {{Parallel Processing System Design with "Propeller" Processor}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}169.pdf},
year = {2009}
}
@inproceedings{Flety2003,
address = {Montreal, QC, Canada},
author = {Fl{\'{e}}ty, Emmanuel and Sirguy, Marc},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
number = {Table 1},
pages = {225--226},
title = {{EoBody : a Follow-up to AtoMIC Pro's Technology}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}225.pdf},
year = {2003}
}
@inproceedings{Bullock2011,
address = {Oslo, Norway},
author = {Bullock, Jamie and Beattie, Daniel and Turner, Jerome},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {live electronics,software,usability,user experience},
pages = {387--392},
title = {{Integra Live : a New Graphical User Interface for Live Electronic Music}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}387.pdf},
year = {2011}
}
@inproceedings{Maniatakos2008,
address = {Genoa, Italy},
author = {Maniatakos, Vassilios-Fivos A and Jacquemin, Christian},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {affective computing,affective interface,anthropomorphic,gesture,hand-,head,hmm,intelligent mapping,interactive performance,is an affective,nime08,pogany,recognition},
pages = {122--127},
title = {{Towards an Affective Gesture Interface for Expressive Music Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}122.pdf},
year = {2008}
}
@inproceedings{Barri2009a,
address = {Pittsburgh, PA, United States},
author = {Barri, Tarik},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {325},
title = {{Versum  Fluor}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}325.pdf},
year = {2009}
}
@inproceedings{Fox2005,
address = {Vancouver, BC, Canada},
author = {Fox, Jesse and Carlile, Jennifer},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {242--243},
title = {{SoniMime: Movement Sonification for Real-Time Timbre Shaping}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}242.pdf},
year = {2005}
}
@inproceedings{Kimura2008,
abstract = {This paper describes the compositional process for creatingthe interactive work for violin entitled VITESSIMO using theAugmented Violin [1].},
address = {Genoa, Italy},
author = {Kimura, Mari},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Augmented Violin,gesture tracking,interactive performance},
pages = {219--220},
title = {{Making of VITESSIMO for Augmented Violin : Compositional Process and Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}219.pdf},
year = {2008}
}
@inproceedings{Barbosa2005,
abstract = {In recent years Computer Network-Music has increasingly captured the attention of the Computer Music Community. With the advent of Internet communication, geographical displacement amongst the participants of a computer mediated music performance achieved world wide extension. However, when established over long distance networks, this form of musical communication has a fundamental problem: network latency (or net-delay) is an impediment for real-time collaboration. From a recent study, carried out by the , , authors, a relation between network latency tolerance and Music Tempo was established. This result emerged from an experiment, in which simulated network latency conditions were applied to the performance of different musicians playing jazz standard tunes. The Public Sound Objects (PSOs) project is web-based shared musical space, which has been an experimental framework to implement and test different approaches for on-line music communication. This paper describe features implemented in the latest version of the PSOs system, including the notion of a network-music instrument incorporating latency as a software function, by dynamically adapting its tempo to the communication delay measured in real-time.},
address = {Vancouver, BC, Canada},
author = {Barbosa, Alvaro and Cardoso, Jorge and Geiger, G{\"{u}}nter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Barbosa, Cardoso, Geiger - 2005 - Network Latency Adaptive Tempo in the Public Sound Objects System(2).pdf:pdf},
keywords = {Latency in Real-Time Pe,Network Music Instruments},
pages = {184--187},
title = {{Network Latency Adaptive Tempo in the Public Sound Objects System}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}184.pdf},
year = {2005}
}
@inproceedings{Smyth2006,
address = {Paris, France},
author = {Smyth, Tamara},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {khaen,mapping,musical acoustics,sound synthesis control},
pages = {314--317},
title = {{Handheld Acoustic Filter Bank for Musical Control}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}314.pdf},
year = {2006}
}
@inproceedings{Allison2003,
address = {Montreal, QC, Canada},
author = {Allison, Jesse T and Place, Timothy},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {208--210},
title = {{SensorBox: Practical Audio Interface for Gestural Performance}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}208.pdf},
year = {2003}
}
@inproceedings{Suiter2010b,
author = {Suiter, Wendy},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
file = {::},
keywords = {fuzzy logic,music composition,musical expression,nime10},
number = {Nime},
pages = {319--322},
title = {{Toward Algorithmic Composition of Expression in Music Using Fuzzy Logic}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}319.pdf},
year = {2010}
}
@inproceedings{Dubrau2010,
address = {Sydney, Australia},
author = {Dubrau, Josh M and Havryliv, Mark},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {nime10},
number = {Nime},
pages = {467--468},
title = {{P[a]ra[pra]xis : Towards Genuine Realtime 'Audiopoetry'}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}467.pdf},
year = {2010}
}
@inproceedings{Deleflie2010,
address = {Sydney, Australia},
author = {Deleflie, Etienne and Schiemer, Greg},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {ambisonics,decorrelation,diffusion,granular synthesis,nime10,spatial audio,surround sound},
number = {Nime},
pages = {130--135},
title = {{Images as Spatial Sound Maps}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}130.pdf},
year = {2010}
}
@inproceedings{Bryan2010,
address = {Sydney, Australia},
author = {Bryan, Nicholas J and Herrera, Jorge and Oh, Jieun and Wang, Ge},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {instrument design,iphone,mobile music,nime10,software develop-},
number = {Nime},
pages = {174--177},
title = {{MoMu : A Mobile Music Toolkit}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}174.pdf},
year = {2010}
}
@inproceedings{Merrill2003,
address = {Montreal, QC, Canada},
author = {Merrill, David},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {218--219},
title = {{Head-Tracking for Gestural and Continuous Control of Parameterized Audio Effects}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}218.pdf},
year = {2003}
}
@inproceedings{Gurevich2009,
address = {Pittsburgh, PA, United States},
author = {Gurevich, Michael and Stapleton, Paul and Bennett, Peter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {a dynamically-,be variations in the,expression,for example,ing one,mapping from qualita-,nime09,observations,s weight load onto,shift-,skill,structure,style,the right leg is,tive states to quantitative,virtuosity,we take style to},
pages = {213--217},
title = {{Designing for Style in New Musical Interactions}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}213.pdf},
year = {2009}
}
@inproceedings{Machover2002,
address = {Dublin, Ireland},
author = {Machover, Tod},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
pages = {115},
title = {{Instruments, Interactivity, and Inevitability}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}115.pdf},
year = {2002}
}
@inproceedings{nime2014:tmays,
abstract = {In this paper we expose the need to go beyond the composer/performer model of electronic instrument design and programming to encourage the transmission of compositions and the creation of a repertory via notation of repeatable performance practice. Drawing on 4 years of practice using the Karlax controller (Da Fact) as a base for new digital musical instruments, we present our notation system in detail and cite some mapping strategies and examples from to pieces in a growing repertory of chamber music compositions for electronic and acoustic instruments},
address = {London, United Kingdom},
author = {Mays, Tom and Faber, Francis},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {553--556},
publisher = {Goldsmiths, University of London},
title = {{A Notation System for the Karlax Controller}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}509.pdf},
year = {2014}
}
@inproceedings{Derbinsky2011,
abstract = {This paper explores how a general cognitive architecture canpragmatically facilitate the development and exploration ofinteractive music interfaces on a mobile platform. To thisend we integrated the Soar cognitive architecture into themobile music meta-environment urMus. We develop anddemonstrate four artificial agents which use diverse learningmechanisms within two mobile music interfaces. We alsoinclude details of the computational performance of theseagents, evincing that the architecture can support real-timeinteractivity on modern commodity hardware.},
address = {Oslo, Norway},
author = {Derbinsky, Nate and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {cognitive architecture,machine learning,mobile music},
pages = {104--107},
title = {{Cognitive Architecture in Mobile Music Interactions}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}104.pdf},
year = {2011}
}
@inproceedings{Schiemer2005,
address = {Vancouver, BC, Canada},
author = {Schiemer, Greg and Havryliv, Mark},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {156--159},
title = {{Pocket Gamelan: a Pure Data interface for mobile phones}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}156.pdf},
year = {2005}
}
@inproceedings{nime2014:rcanning,
abstract = {This paper describes the Parallaxis Score System, part of the authors ongoing research into to the development of technological tools that foster creative interactions between improvising musicians and predefined instructional texts. The Parallaxis platform places these texts within a networked, interactive environment with a generalised set of controls in order to explore and devise ontologies of network performance. As an interactive tool involved in music production the score system itself undergoes a functional transformation and becomes a distributed meta-instrument in its own right, independent from, yet intrinsically connected to those instruments held by the performers.},
address = {London, United Kingdom},
author = {Canning, Rob},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {144--146},
publisher = {Goldsmiths, University of London},
title = {{Interactive Parallax Scrolling Score Interface for Composed Networked Improvisation}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}421.pdf},
year = {2014}
}
@inproceedings{Paine2009,
address = {Pittsburgh, PA, United States},
author = {Paine, Garth and Atherton, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {324},
title = {{Fue Sho  Electrofusion}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}324.pdf},
year = {2009}
}
@inproceedings{Caramiaux2011,
address = {Oslo, Norway},
author = {Caramiaux, Baptiste and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Schnell, Norbert},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {6,example is the,query by gesture,query by tapping,rhythm,song by tapping the,sonic interaction,system,that allows the,this system is,time series analysis,user to find a},
number = {June},
pages = {329--330},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Sound Selection by Gestures}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}329.pdf},
year = {2011}
}
@inproceedings{Robertson2007,
abstract = {This paper describes the development of B-Keeper, a reatime beat tracking system implemented in Java and Max/MSP,which is capable of maintaining synchronisation between anelectronic sequencer and a drummer. This enables musicians to interact with electronic parts which are triggeredautomatically by the computer from performance information. We describe an implementation which functions withthe sequencer Ableton Live.},
address = {New York City, NY, United States},
author = {Robertson, Andrew and Plumbley, Mark D},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Automatic Accompaniment,Human-Computer Interaction,Performance},
pages = {234--237},
title = {{B-Keeper : A Beat-Tracker for Live Performance}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}234.pdf},
year = {2007}
}
@inproceedings{FASCIANI:2012,
abstract = {Sound generators and synthesis engines expose a large set of parameters, allowing run-time timbre morphing and exploration of sonic space. However, control over these high-dimensional interfaces is constrained by the physical limitations of performers. In this paper we propose the exploitation of vocal gesture as an extension or alternative to traditional physical controllers. The approach uses dynamic aspects of vocal sound to control variations in the timbre of the synthesized sound. The mapping from vocal to synthesis parameters is automatically adapted to information extracted from vocal examples as well as to the relationship between parameters and timbre within the synthesizer. The mapping strategy aims to maximize the breadth of the explorable perceptual sonic space over a set of the synthesizer's real-valued parameters, indirectly driven by the voice-controlled interface.},
address = {Ann Arbor, Michigan},
author = {FASCIANI, STEFANO and WYSE, LONCE},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
month = {may},
publisher = {University of Michigan},
title = {{A Voice Interface for Sound Generators: adaptive and automatic mapping of gestures to sound}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}57.pdf},
year = {2012}
}
@inproceedings{Cannon2010,
address = {Sydney, Australia},
author = {Cannon, Joanne and Favilla, Stuart},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {augmented instruments,expressive spatial,nime10,playable instruments},
number = {Nime},
pages = {120--124},
title = {{Expression and Spatial Motion : Playable Ambisonics}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}120.pdf},
year = {2010}
}
@inproceedings{nime2014:tmurray-browne1,
abstract = {The Cave of Sounds is an interactive sound installation made up of new musical instruments. Exploring what it means to create instruments together within the context of NIME and the maker scene, each instrument was created by an individual but with the aim of forming a part of this new ensemble over ten months, with the final installation debuting at the Barbican in London in August 2013. In this paper, we describe how ideas of prehistoric collective music making inspired and guided this participatory musical work, both in terms of how it was created and the audience experience of musical collaboration we aimed to create in the final installation. Following a detailed description of the installation itself, we reflect on the successes, lessons and future challenges of encouraging creative musical collaboration among members of an audience.},
address = {London, United Kingdom},
author = {Murray-Browne, Tim and Aversano, Dom and Garcia, Susanna and Hobbes, Wallace and Lopez, Daniel and Sendon, Tadeo and Tigas, Panagiotis and Ziemianin, Kacper and Chapman, Duncan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {307--310},
publisher = {Goldsmiths, University of London},
title = {{The Cave of Sounds: An Interactive Installation Exploring How We Create Music Together}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}288.pdf},
year = {2014}
}
@inproceedings{Sirguy2007,
address = {New York City, NY, United States},
author = {Sirguy, Marc and Gallin, Emmanuelle},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {401--402},
title = {{Eobody2 : A Follow-up to Eobody's Technology}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}401.pdf},
year = {2007}
}
@inproceedings{Kessous2002,
address = {Dublin, Ireland},
author = {Kessous, Lo{\"{i}}c},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {bi-manual,fundamental fre-,mapping,off-the-shelf input devices,quency control,sound color navigation},
pages = {113--114},
title = {{Bi-manual Mapping Experimentation, with Angular Fundamental Frequency Control and Sound Color Navigation}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}113.pdf},
year = {2002}
}
@inproceedings{nime2014:tmitchell,
abstract = {Wireless technology is growing increasingly prevalent in the development of new interfaces for live music performance. However, with a number of different wireless technologies operating in the 2.4 GHz band, there is a high risk of interference and congestion, which has the potential to severely disrupt live performances. With its high transmission power, channel bandwidth and throughput, Wi-Fi (IEEE 802.11) presents an opportunity for highly robust wireless communications. This paper presents our preliminary work optimising the components of a Wi-Fi system for live performance scenarios. We summarise the manufacture and testing of a prototype directional antenna that is designed to maximise sensitivity to a performer's signal while suppressing interference from elsewhere. We also propose a set of recommended Wi-Fi configurations to reduce latency and increase throughput. Practical investigations utilising these arrangements demonstrate a single x-OSC device achieving a latency of {\textless}3 ms and a distributed network of 15 devices achieving a net throughput of {\~{}}4800 packets per second ({\~{}}320 per device); where each packet is a 104-byte OSC message containing 16 analogue input channels acquired by the device.},
address = {London, United Kingdom},
author = {Mitchell, Thomas and Madgwick, Sebastian and Rankine, Simon and Hilton, Geoffrey and Freed, Adrian and Nix, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Mitchell et al. - 2014 - Making the Most of Wi-Fi Optimisations for Robust Wireless Live Music Performance.pdf:pdf},
pages = {251--256},
publisher = {Goldsmiths, University of London},
title = {{Making the Most of Wi-Fi: Optimisations for Robust Wireless Live Music Performance}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}386.pdf},
year = {2014}
}
@inproceedings{Kapur:2012,
abstract = {In this paper, we introduce Kritaanjli, a robotic harmo- nium. Details concerning the design, construction, and use of Kritaanjli are discussed. After an examination of related work, quantitative research concerning the hardware chosen in the construction of the instrument is shown, as is a thor- ough exposition of the design process and use of CAD/CAM techniques in the design lifecycle of the instrument. Addi- tionally, avenues for future work and compositional prac- tices are focused upon, with particular emphasis placed on human/robot interaction, pedagogical techniques afforded by the robotic instrument, and compositional avenues made accessible through the use of Kritaanjli.},
address = {Ann Arbor, Michigan},
author = {Kapur, Ajay and Murphy, Jim and Carnegie, Dale},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Musical Robotics,North Indian Classical Music,augmented instruments,pedagogy},
publisher = {University of Michigan},
title = {{Kritaanjali: A Robotic Harmonium for Performance, Pedogogy and Research}},
year = {2012}
}
@inproceedings{Berdahl:2013,
abstract = {Satellite CCRMA is a platform for making embedded musical instruments andembedded installations. The project aims to help prototypes live longer byproviding a complete prototyping platform in a single, small, and stand-aloneembedded form factor. A set of scripts makes it easier for artists andbeginning technical students to access powerful features, while advanced usersenjoy the flexibility of the open-source software and open-source hardwareplatform.This paper focuses primarily on networking capabilities of Satellite CCRMA andnew software for enabling interactive control of the hardware-acceleratedgraphical output. In addition, some results are presented from robustness testsalongside specific advice and software support for increasing the lifespan ofthe flash memory.},
address = {Daejeon, Republic of Korea},
author = {Berdahl, Edgar and Salazar, Spencer and Borins, Myles},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Interface.js,Node.js,OpenGLES,Satellite CCRMA,SimpleGraphicsOSC,embedded installations,embedded musical instruments,hardware-accelerated graphics,union file system,write endurance},
pages = {325--330},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Embedded Networking and Hardware-Accelerated Graphics with Satellite CCRMA}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Mudd:2013,
abstract = {This paper presents a system for exploring different dimensions of a soundthrough the use of haptic feedback. The Novint Falcon force feedback interfaceis used to scan through soundfiles as a subject moves their hand horizontallyfrom left to right, and to relay information about volume, frequency content,noisiness, or potentially any analysable parameter back to the subject throughforces acting on their hand. General practicalities of mapping sonic elements to physical forces areconsidered, such as the problem of representing detailed data through vaguephysical sensation, approaches to applying forces to the hand that do notinterfering with the smooth operation of the device, and the relative merits ofdiscreet and continuous mappings. Three approaches to generating the forcevector are discussed: 1) the use of simulated detents to identify areas of anaudio parameter over a certain threshold, 2) applying friction proportional tothe level of the audio parameter along the axis of movement, and 3) creatingforces perpendicular to the subject's hand movements.Presentation of audio information in this manner could be beneficial for`pre-feeling' as a method for selecting material to play during a liveperformance, assisting visually impaired audio engineers, and as a generalaugmentation of standard audio editing environments.},
address = {Daejeon, Republic of Korea},
author = {Mudd, Tom},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Haptics,force feedback,human-computer interaction,mapping},
pages = {369--372},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Feeling for Sound: Mapping Sonic Data to Haptic Perceptions}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Singer2005,
abstract = {This paper describes an installation created by LEMUR(League of Electronic Musical Urban Robots) in January, 2005.The installation included over 30 robotic musical instrumentsand a multi-projector real-time video projection and wascontrollable and programmable over a MIDI network. Theinstallation was also controllable remotely via the Internet andcould be heard and viewed via room mics and a robotic webcam connected to a streaming server.},
address = {Vancouver, BC, Canada},
author = {Singer, Eric and Feddersen, Jeff and Bowen, Bil},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {50--55},
title = {{A Large-Scale Networked Robotic Musical Instrument Installation}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}050.pdf},
year = {2005}
}
@inproceedings{Kikukawa:2013,
abstract = {So far, there are few studies of string instruments with bows because there aremany parameters to acquire skills and it is difficult to measure theseparameters. Therefore, the aim of this paper is to propose a design of alearning environment for a novice learner to acquire an accurate fingerposition skill. For achieving the aim, we developed a learning environmentwhich can diagnose learner's finger position and give the learner advice byusing magnetic position sensors. The system shows three windows; a fingerposition window for visualization of finger position, a score window fordiagnosing finger position along the score and command prompt window forshowing states of system and advices. Finally, we evaluated the system by anexperiment. The experimental group improved accuracy values about fingerpositions and also improved accuracy of pitches of sounds compared withcontrol group. These results shows significant differences.},
address = {Daejeon, Republic of Korea},
author = {Kikukawa, Fumitaka and Ishihara, Sojiro and Soga, Masato and Taki, Hirokazu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Finger Position,Learning Environment,Magnetic Position Sensors,Skill,String Instruments},
pages = {271--276},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Development of A Learning Environment for Playing Erhu by Diagnosis and Advice regarding Finger Position on Strings}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Hamilton:2013,
abstract = {With a nod towards digital puppetry and game-based film genres such asmachinima, recent additions to UDKOSC of- fer an Open Sound Control (OSC)control layer for external control over both third-person ''pawn'' entitiesand camera controllers in fully rendered game-space. Real-time OSC input,driven by algorithmic process or parsed from a human-readable timed scriptingsyntax allows users to shape choreographies of gesture, in this case actormotion and action, as well as an audiences view into the game-spaceenvironment. As UDKOSC outputs real-time coordinate and action data generatedby UDK pawns and players with OSC, individual as well as aggregate virtualactor gesture and motion can be leveraged as a driver for both creative andprocedural/adaptive gaming music and audio concerns.},
address = {Daejeon, Republic of Korea},
author = {Hamilton, Rob},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Open Sound Control,game music,interactive sonification,procedural audio,procedural music},
pages = {446--449},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Sonifying Game-Space Choreographies With UDKOSC}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Vamvakousis2012,
abstract = {In this paper we describe the EyeHarp, a new gaze-controlled musical instrument, and the new features we recently added to its design. In particular, we report on the EyeHarp new controls, the arpeggiator, the new remote eye-tracking device, and the EyeHarp capacity to act as a MIDI controller for any VST plugin virtual instrument. We conducted an evaluation of the EyeHarp Temporal accuracy by monitor- ing 10 users while performing a melody task, and comparing their gaze control accuracy with their accuracy using a com- puter keyboard. We report on the results of the evaluation.},
address = {Ann Arbor, Michigan},
author = {Vamvakousis, Zacharias and Ramirez, Rafael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Eye-tracking systems,gaze interaction,music interfaces},
publisher = {University of Michigan},
title = {{Temporal Control In the EyeHarp Gaze-Controlled Musical Interface}},
year = {2012}
}
@inproceedings{Stewart2006,
address = {Paris, France},
author = {Stewart, D Andrew},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {composition,controller,cross-,gesture,materials,process},
pages = {103--105},
title = {{SonicJumper Composer}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}103.pdf},
year = {2006}
}
@inproceedings{Park:2013b,
abstract = {SSN (Sound Surfing Network) is a performance system that provides a new musicalexperience by incorporating mobile phone-based spatial sound control tocollaborative music performance. SSN enables both the performer and theaudience to manipulate the spatial distribution of sound using the smartphonesof the audience as distributed speaker system. Proposing a new perspective tothe social aspect music appreciation, SSN will provide a new possibility tomobile music performances in the context of interactive audience collaborationas well as sound spatialization.},
address = {Daejeon, Republic of Korea},
author = {Park, Saebyul and Ban, Seonghoon and Hong, Dae Ryong and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Park et al. - 2013 - Sound Surfing Network (SSN) Mobile Phone-based Sound Spatialization with Audience Collaboration.pdf:pdf},
keywords = {Mobile music,audience participation,digital performance,smartphone,spatial sound control},
pages = {111--114},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Sound Surfing Network (SSN): Mobile Phone-based Sound Spatialization with Audience Collaboration}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Neill2009,
address = {Pittsburgh, PA, United States},
author = {Neill, Ben and Singer, Eric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {331},
title = {{Ben Neill and LEMUR}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}331.pdf},
year = {2009}
}
@inproceedings{Fyfe2010,
abstract = {In this paper we discuss SurfaceMusic, a tabletop music system in which touch gestures are mapped to physical modelsof instruments. With physical models, parametric controlover the sound allows for a more natural interaction between gesture and sound. We discuss the design and implementation of a simple gestural interface for interactingwith virtual instruments and a messaging system that conveys gesture data to the audio system.},
address = {Sydney, Australia},
author = {Fyfe, Lawrence and Lynch, Sean and Hull, Carmen and Carpendale, Sheelagh},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Open Sound Control.,Tabletop,gesture,multi-touch,physical model},
pages = {360--363},
title = {{SurfaceMusic : Mapping Virtual Touch-based Instruments to Physical Models}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}360.pdf},
year = {2010}
}
@inproceedings{Gatzsche2008,
abstract = {In this paper, we present a pitch space based musical interface approach. A pitch space arranges tones in a way that meaningful tone combinations can be easily generated. Using a touch sensitive surface or a 3D-Joystick a player can move through the pitch space and create the desired sound by selecting tones. The more optimal the tones are geometrically arranged, the less control parameters are required to move through the space and to select the desired pitches. For this the quality of pitch space based musical interfaces depends on two factors: 1. the way how the tones are organized within the pitch space and 2. the way how the parameters of a given controller are used to move through the space and to select pitches. This paper presents a musical interface based on a tonal pitch space derived from a four dimensional model found by the music psychologists [11], [2]. The proposed pitch space particularly eases the creation of tonal harmonic music. Simultaneously it outlines music psychological and theoretical principles of music. },
address = {Genoa, Italy},
author = {Gatzsche, Gabriel and Mehnert, Markus and St{\"{o}}cklmeier, Christian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {3D,3D tonality model,Carol L. Krumhansl,Hardware controller,Pitch space,Symmetry model,music psychology,music theory,musical interface,spiral of thirds,western tonal music},
pages = {325--330},
title = {{Interaction with Tonal Pitch Spaces}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}325.pdf},
year = {2008}
}
@inproceedings{Weinberg2009a,
address = {Pittsburgh, PA, United States},
author = {Weinberg, Gil and Blosser, Brian and Mallikarjuna, Trishul and Raman, Aparna},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {haile,nime09,robotic musicianship,shimon},
pages = {70--73},
title = {{The Creation of a Multi-Human, Multi-Robot Interactive Jam Session}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}070.pdf},
year = {2009}
}
@inproceedings{Buschert:2012,
abstract = {Musician Maker is a system to allow novice players the opportunity to create expressive improvisational music. While the system plays an accompaniment background chord progression, each participant plays some kind of controller to make music through the system. The program takes the signals from the controllers and adjusts the pitches somewhat so that the players are limited to notes which fit the chord progression. The various controllers are designed to be very easy and intuitive so anyone can pick one up and quickly be able to play it. Since the computer is making sure that wrong notes are avoided, even inexperienced players can immediately make music and enjoy focusing on some of the more expressive elements and thus become musicians.},
address = {Ann Arbor, Michigan},
author = {Buschert, John},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Computer Music,Controller,Electronic,Musical Instrument,Novice},
publisher = {University of Michigan},
title = {{Musician Maker: Play expressive music without practice}},
year = {2012}
}
@inproceedings{Hsu2011,
address = {Oslo, Norway},
author = {Hsu, William},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {animation,audio-visual,generative,improvisation,interactive},
number = {June},
pages = {417--420},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{On Movement , Structure and Abstraction in Generative Audiovisual Improvisation}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}417.pdf},
year = {2011}
}
@inproceedings{Dahlstedt:2012,
abstract = {I present a novel low-tech multidimensional gestural con- troller, based on the resistive properties of a 2D field of pencil markings on paper. A set of movable electrodes (+, -, ground) made from soldered stacks of coins create a dynamic voltage potential field in the carbon layer, and an- other set of movable electrodes tap voltages from this field. These voltages are used to control complex sound engines in an analogue modular synthesizer. Both the voltage field and the tap electrodes can be moved freely. The design was inspired by previous research in complex mappings for advanced digital instruments, and provides a similarly dynamic playing environment for analogue synthesis. The interface is cheap to build, and provides flexible control over a large set of parameters. It is musically satisfying to play, and allows for a wide range of playing techniques, from wild exploration to subtle expressions. I also present an inven- tory of the available playing techniques, motivated by the interface design, musically, conceptually and theatrically. The performance aspects of the interface are also discussed. The interface has been used in a number of performances in Sweden and Japan in 2011, and is also used by other musicians.},
address = {Ann Arbor, Michigan},
author = {Dahlstedt, Palle},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {2d,analog synthesis,gestural interface,improvisation,performance},
publisher = {University of Michigan},
title = {{Pencil Fields: An Expressive Low-Tech Performance Interface for Analog Synthesis}},
year = {2012}
}
@inproceedings{Beck2011,
address = {Oslo, Norway},
author = {Beck, Stephen D and Branton, Chris and Maddineni, Sharath},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {grid computing,laptop orchestra,tangible interaction},
number = {June},
pages = {207--210},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Tangible Performance Management of Grid-based Laptop Orchestras}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}207.pdf},
year = {2011}
}
@inproceedings{Hazlewood2008,
abstract = {In this work we describe our initial explorations in building a musical instrument specifically for providing listenerswith simple, but useful, ambient information. The termAmbient Musical Information Systems (AMIS) is proposedto describe this kind of research. Instruments like these differ from standard musical instruments in that they are tobe perceived indirectly from outside one's primary focus ofattention. We describe our rationale for creating such a device, a discussion on the appropriate qualities of sound fordelivering ambient information, and a description of an instrument created for use in a series of experiments that wewill use to test out ideas. We conclude with a discussion ofour initial findings, and some further directions we wish toexplore.},
address = {Genoa, Italy},
author = {Hazlewood, William R and Knopke, Ian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Ambient Musical Information Systems,Markov chain,al- gorithmic composition,human computer interaction,musical instruments,probability},
pages = {281--284},
title = {{Designing Ambient Musical Information Systems}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}281.pdf},
year = {2008}
}
@inproceedings{PalacioQuintin2003,
address = {Montreal, QC, Canada},
author = {Palacio-Quintin, Cl{\'{e}}o},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Palacio-Quintin - 2003 - The Hyper-Flute.pdf:pdf},
pages = {206--207},
title = {{The Hyper-Flute}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}206.pdf},
year = {2003}
}
@inproceedings{Villar2005,
abstract = {We present the Pin{\&}Play{\&}Perform system: an interface inthe form of a tablet on which a number of physical controlscan be added, removed and arranged on the fly. These controls can easily be mapped to existing music sofware usingthe MIDI protocol. The interface provides a mechanism fordirect manipulation of application parameters and eventsthrough a set of familiar controls, while also encouraging ahigh degree of customisation through the ability to arrange,rearrange and annotate the spatial layout of the interfacecomponents on the surface of the tablet.The paper describes how we have realized this concept using the Pin{\&}Play technology. As an application example, wedescribe our experiences in using our interface in conjunction with Propellerheads' Reason, a popular piece of musicsynthesis software.},
address = {Vancouver, BC, Canada},
author = {Villar, Nicolas and Lindsay, Adam T and Gellersen, Hans},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {midi controllers,rearrangeable interface,tangible interface},
pages = {188--191},
title = {{Pin {\&} Play {\&} Perform: A rearrangeable interface for musical composition and performance}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}188.pdf},
year = {2005}
}
@inproceedings{nime2014:jratcliffe,
abstract = {This paper presents a control surface interface for music mixing using real time computer vision. Two input sensors are considered: the Leap Motion and the Microsoft Kinect. The author presents significant design considerations, including improving of the user's sense of depth and panorama, maintaining broad accessibility by integrating the system with Digital Audio Workstation (DAW) software, and implementing a system that is portable and affordable. To provide the user with a heightened sense of sound spatialization over the traditional channel strip, the concept of depth is addressed directly using the stage metaphor. Sound sources are represented as colored spheres in a graphical user interface to provide the user with visual feedback. Moving sources back and forward controls volume, while left to right controls panning. To provide broader accessibility, the interface is configured to control mixing within the Ableton Live DAW. The author also discusses future plans to expand functionality and evaluate the system.},
address = {London, United Kingdom},
author = {Ratcliffe, Jarrod},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {136--139},
publisher = {Goldsmiths, University of London},
title = {{Hand and Finger Motion-Controlled Audio Mixing Interface}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}518.pdf},
year = {2014}
}
@inproceedings{Schnell2011,
address = {Oslo, Norway},
author = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Rasamimanana, Nicolas and Blois, Julien and Gu{\'{e}}dy, Fabrice and Fl{\'{e}}ty, Emmanuel},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Schnell et al. - 2011 - Playing the MO  Gestural Control and Re-Embodiment of Recorded Sound and Music.pdf:pdf},
keywords = {audio processing,control and embodiment of,design,gesture,gesture recog-,interaction,interface,music,nition,sound and,the study of the,wireless sensors},
number = {June},
pages = {535--536},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Playing the "MO"  Gestural Control and Re-Embodiment of Recorded Sound and Music}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}535.pdf},
year = {2011}
}
@inproceedings{Henriques2008,
address = {Genoa, Italy},
author = {Henriques, Tom{\'{a}}s},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {computer music,musical instrument,nime08,sensor technologies},
pages = {307--310},
title = {{META-{\{}EV{\}}I Innovative Performance Paths with a Wind Controller}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}307.pdf},
year = {2008}
}
@inproceedings{Schiesser:2012,
abstract = {An augmented bass clarinet is developed in order to extend the performance and composition potential of the instru- ment. Four groups of sensors are added: key positions, inertial movement, mouth pressure and trigger switches. The instrument communicates wirelessly with a receiver setup which produces an OSC data stream, usable by any appli- cation on a host computer.
The SABRe projects intention is to be neither tied to its inventors nor to one single player but to offer a reference design for a larger community of bass clarinet players and composers. For this purpose, several instruments are made available and a number of composer residencies, workshops, presentations and concerts are organized. These serve for evaluation and improvement purposes in order to build a robust and user friendly extended musical instrument, that opens new playing modalities.},
address = {Ann Arbor, Michigan},
author = {Schiesser, S{\'{e}}bastien and Schacher, Jan C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {OSC,air pressure,augmented instrument,bass clarinet,gesture,sensors},
publisher = {University of Michigan},
title = {{SABRe: The Augmented Bass Clarinet}},
year = {2012}
}
@inproceedings{McElligott:2002,
address = {Dublin},
author = {McElligott, Lisa and Dixon, Edward and Dillon, Michelle},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {3D sensing pressure,Gesture,control device,effort,expression,force,input.,intent,movement,music,resolution,sensor,sound,weight distribution},
pages = {126--130},
title = {{`PegLegs in Music' Processing the Effort Generated by Levels of Expressive Gesturing in Music}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}126.pdf},
year = {2002}
}
@inproceedings{Han:2013,
abstract = {This paper proposes a musical performance feedback system based on real-time audio-score alignment for musical instrument education of beginner musicians. In the proposed system, we do not make use of symbolic data such as MIDI, but acquire a real-time audio input from on-board microphone of smartphone. Then, the system finds onset and pitch of the note from the signal, to align this information with the ground truth musical score. Real-time alignment allows the system to evaluate whether the user played the correct note or not, regardless of its timing, which enables user to play at their own speed, as playing same tempo with original musical score is problematic for beginners. As an output of evaluation, the system notifies the user about which part they are currently performing, and which note were played incorrectly.},
address = {Daejeon, Republic of Korea},
author = {Han, Yoonchang and Kwon, Sejun and Lee, Kibeom and Lee, Kyogu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Music education,Music performance analysis,Real-time score following},
pages = {120--121},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Musical Performance Evaluation System for Beginner Musician based on Real-time Score Following}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Hornof2004,
address = {Hamamatsu, Japan},
author = {Hornof, Anthony J and Sato, Linda},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {a series of quick,and the gazepoint,around a scene with,between,each of which lasts,jumps called,roughly 30 ms,saccades,stay at the same,the eyes},
pages = {185--188},
title = {{EyeMusic: Making Music with the Eyes}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}185.pdf},
year = {2004}
}
@inproceedings{Groux2007,
address = {New York City, NY, United States},
author = {{Le Groux}, Sylvain and Manzolli, Jonatas and Verschure, Paul F},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {371--374},
title = {{VR-RoBoser : Real-Time Adaptive Sonification of Virtual Environments Based on Avatar Behavior}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}371.pdf},
year = {2007}
}
@inproceedings{Walther2013,
abstract = {Although multi-touch user interfaces have become a widespread form of humancomputer interaction in many technical areas, they haven't found their way intolive performances of musicians and keyboarders yet. In this paper, we present anovel multi-touch interface method aimed at professional keyboard players. Themethod, which is inspired by computer trackpads, allows controlling up to tencontinuous parameters of a keyboard with one hand, without requiring the userto look at the touch area - a significant improvement over traditional keyboardinput controls. We discuss optimizations needed to make our interface reliable,and show in an evaluation with four keyboarders of different skill level thatthis method is both intuitive and powerful, and allows users to more quicklyalter the sound of their keyboard than they could with current input solutions.},
address = {Daejeon, Republic of Korea},
author = {Walther, Thomas and Ismailovi{\'{c}}, Damir and Br{\"{u}}gge, Bernd},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {interface,keyboard,mobile,multi-touch},
pages = {98--101},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Rocking the Keys with a Multi-Touch Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Migneco:2012,
abstract = {Platforms for mobile computing and gesture recognitionprovide enticing interfaces for creative expression on virtualmusical instruments. However, sound synthesis on thesesystems is often limited to sample-based synthesizers, whichlimits their expressive capabilities. Source-filter models areadept for such interfaces since they provide flexible, algorithmic sound synthesis, especially in the case of the guitar.In this paper, we present a data-driven approach for modeling guitar excitation signals using principal componentsderived from a corpus of excitation signals. Using thesecomponents as features, we apply nonlinear principal components analysis to derive a feature space that describesthe expressive attributes characteristic to our corpus. Finally, we propose using the reduced dimensionality space asa control interface for an expressive guitar synthesizer.},
address = {Ann Arbor, Michigan},
author = {Migneco, Raymond and Kim, Youngmoo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {PCA,Source-filter models,musical instrument synthesis,touch musical interfaces},
month = {may},
publisher = {University of Michigan},
title = {{A Component-Based Approach for Modeling Plucked-Guitar Excitation Signals}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}63.pdf},
year = {2012}
}
@inproceedings{Kapur2011,
address = {Oslo, Norway},
author = {Kapur, Ajay and Darling, Michael and Murphy, Jim and Hochenbaum, Jordan and Diakopoulos, Dimitri and Trimpin, Trimpin},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {music technology,musical robotics,robotic performance},
number = {June},
pages = {228--231},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{The KarmetiK NotomotoN : A New Breed of Musical Robot for Teaching and Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}228.pdf},
year = {2011}
}
@inproceedings{Tanaka2006,
abstract = {This paper presents the concepts and techniques used in afamily of location based multimedia works. The paper hasthree main sections: 1.) to describe the architecture of anaudio-visual hardware/software framework we havedeveloped for the realization of a series of locative mediaartworks, 2.) to discuss the theoretical and conceptualunderpinnings motivating the design of the technicalframework, and 3.) to elicit from this, fundamental issuesand questions that can be generalized and applicable to thegrowing practice of locative media.},
address = {Paris, France},
author = {Tanaka, Atau and Gemeinboeck, Petra},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Mobile music,locative media.,urban fiction},
pages = {26--30},
title = {{A Framework for Spatial Interaction in Locative Media}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}026.pdf},
year = {2006}
}
@inproceedings{Berthaut2011,
address = {Oslo, Norway},
author = {Berthaut, Florent and Katayose, Haruhiro and Wakama, Hironori and Totani, Naoyuki and Sato, Yuichi},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {3d interaction,collaborative,couacs in faders for,figure 1,first person shooters,fps,multiprocess instrument,the couacs,three musicians playing the},
number = {June},
pages = {44--47},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{First Person Shooters as Collaborative Multiprocess Instruments}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}044.pdf},
year = {2011}
}
@inproceedings{Howard2003,
address = {Montreal, QC, Canada},
author = {Howard, David M and Rimell, Stuart and Hunt, Andy D},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {95--98},
title = {{Force Feedback Gesture Controlled Physical Modelling Synthesis}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}095.pdf},
year = {2003}
}
@inproceedings{Schacher2007,
address = {New York City, NY, United States},
author = {Schacher, Jan C},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {emergent structures,gesture,mapping,matrix,nime07,surround sound,trajectory,transform,tree hierarchy},
pages = {358--362},
title = {{Gesture Control of Sounds in 3D Space}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}358.pdf},
year = {2007}
}
@inproceedings{Gillian2009,
abstract = {This paper presents the SARC EyesWeb Catalog (SEC), agroup of blocks designed for real-time gesture recognitionthat have been developed for the open source program EyesWeb. We describe how the recognition of real-time bodymovements can be used for musician-computer-interaction.},
address = {Pittsburgh, PA, United States},
author = {Gillian, Nicholas and Knapp, Benjamin and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {SARC EyesWeb Catalog,gesture recognition},
pages = {60--61},
title = {{The {\{}SAR{\}}C EyesWeb Catalog : A Pattern Recognition Toolbox for Musician-Computer Interaction}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}060.pdf},
year = {2009}
}
@inproceedings{Dannenberg2004,
address = {Hamamatsu, Japan},
author = {Dannenberg, Roger B},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
pages = {132--137},
title = {{Aura II: Making Real-Time Systems Safe for Music}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}132.pdf},
year = {2004}
}
@inproceedings{nime2014:mcartwright,
abstract = {Programming an audio synthesizer can be a difficult task for many. However, if a user has a general idea of the sound they are trying to program, they may be able to imitate it with their voice. This paper presents SynthAssist, a system for interactively searching the synthesis space of an audio synthesizer. In this work, we present how to use the system for querying a database of audio synthesizer patches (i.e. settings/parameters) by vocal imitation and user feed- back. To account for the limitations of the human voice, it uses both absolute and relative time series representations of features and relevance feedback on both the feature weights and time series to refine the query. The method presented in this paper can be used to search through large databases of previously existing ``factory presets" or program a synthesizer using the data-driven approach to automatic synthesizer programming.},
address = {London, United Kingdom},
author = {Cartwright, Mark and Pardo, Bryan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {363--366},
publisher = {Goldsmiths, University of London},
title = {{SynthAssist: Querying an Audio Synthesizer by Vocal Imitation}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}446.pdf},
year = {2014}
}
@inproceedings{Koehly2006,
address = {Paris, France},
author = {Koehly, Rodolphe and Curtil, Denis and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {a,conductive pigments,ink,interface design,is fundamental,it must keep the,piezoresistive technology,sensitive materials,the choice of medium,touch sensors},
pages = {230--233},
title = {{Paper FSRs and Latex/Fabric Traction Sensors: Methods for the Development of Home-Made Touch Sensors}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}230.pdf},
year = {2006}
}
@inproceedings{Luhtala2011,
address = {Oslo, Norway},
author = {Luhtala, Matti and Kym{\"{a}}l{\"{a}}inen, Tiina and Plomp, Johan},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {design,design for all,dfa,hti,human-technology interaction,modifiable interfaces,music interfaces,music therapy,performance,prototyping,tools,ucd,user-centred},
number = {June},
pages = {429--432},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Designing a Music Performance Space for Persons with Intellectual Learning Disabilities}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}429.pdf},
year = {2011}
}
@inproceedings{Grosshauser:2013,
abstract = {Several new technologies to capture motion, gesture and forces for musical instrument players' analyses have been developed in the last years. In research and for augmented instruments one parameter is underrepresented so far. It is finger position and pressure measurement, applied by the musician while playing the musical instrument. In this paper we show a flexible linear-potentiometer and forcesensitive-resistor (FSR) based solution for position, pressure and force sensing between the contact point of the fingers and the musical instrument. A flexible matrix printed circuit board (PCB) is fixed on a piano key. We further introduce linear potentiometer based left hand finger position sensing for string instruments, integrated into a violin and a guitar finger board. Several calibration and measurement scenarios are shown. The violin sensor was evaluated with 13 music students regarding playability and robustness of the system. Main focus was a the integration of the sensors into these two traditional musical instruments as unobtrusively as possible to keep natural haptic playing sensation. The musicians playing the violin in different performance situations stated good playability and no differences in the haptic sensation while playing. The piano sensor is rated, due to interviews after testing it in a conventional keyboard quite unobtrusive, too, but still evokes a different haptic sensation.},
address = {Daejeon, Republic of Korea},
author = {Grosshauser, Tobias and Tr{\"{o}}ster, Gerhard},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Guitar,Keyboard,Piano,Position,Pressure,Sensor,Violin},
month = {may},
pages = {479--484},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Finger Position and Pressure Sensing Techniques for String and Keyboard Instruments}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{nime2014:jbowers,
abstract = {This paper outlines a concept of hybrid resonant assemblages, combinations of varied materials excited by sound transducers, feeding back to themselves via digital signal processing. We ground our concept as an extension of work by David Tudor, Nicolas Collins and Bowers and Archer [NIME 2005] and draw on a variety of critical perspectives in the social sciences and philosophy to explore such assemblages as an alternative to more familiar ideas of instruments and interfaces. We lay out a conceptual framework for the exploration of hybrid resonant assemblages and describe how we have approached implementing them. Our performance experience is presented and implications for work are discussed. In the light of our work, we urge a reconsideration of the implicit norms of performance which underlie much research in NIME. In particular, drawing on the philosophical work of Jean-Luc Nancy, we commend a wider notion of touch that also recognises the performative value of withholding contact.},
address = {London, United Kingdom},
author = {Bowers, John and Haas, Annika},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {7--12},
publisher = {Goldsmiths, University of London},
title = {{Hybrid Resonant Assemblages: Rethinking Instruments, Touch and Performance in New Interfaces for Musical Expression}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}438.pdf},
year = {2014}
}
@inproceedings{Jorda2006,
abstract = {This paper introduces the scoreTable*, a tangible interactive music score editor which started as a simple application for demoing "traditional" approaches to music creation, using the reacTable* technology, and which has evolved into an independent research project on its own. After a brief discussion on the role of pitch in music, we present a brief overview of related tangible music editors, and discuss several paradigms in computer music creation, contrasting synchronous with asynchronous approaches. The final part of the paper describes the current state of the scoreTable* as well as its future lines of research.},
address = {Paris, France},
author = {Jord{\`{a}}, Sergi and Alonso, Marcos},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Collaborative Music,Computer Supported Collaborative Work,Music Theory.,Musical instrument,Tangible User Interface},
pages = {208--211},
title = {{Mary Had a Little scoreTable* or the reacTable* Goes Melodic}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}208.pdf},
year = {2006}
}
@inproceedings{Burtner2006,
address = {Paris, France},
author = {Burtner, Matthew},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {interface,mapping,movement,multi-agent,multi-performer,music composition,perturbation},
pages = {129--133},
title = {{Perturbation Techniques for Multi-Performer or Multi- Agent Interactive Musical Interfaces}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}129.pdf},
year = {2006}
}
@inproceedings{Gomez2007,
address = {New York City, NY, United States},
author = {G{\'{o}}mez, Daniel and Donner, Tjebbe and Posada, Andr{\'{e}}s},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {327--329},
title = {{A Look at the Design and Creation of a Graphically Controlled Digital Musical Instrument}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}327.pdf},
year = {2007}
}
@inproceedings{Magnusson2005,
address = {Vancouver, BC, Canada},
author = {Magnusson, Thor},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {212--215},
title = {{ixi software: The Interface as Instrument}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}212.pdf},
year = {2005}
}
@inproceedings{nime2014:bjohnson,
abstract = {This paper introduces recent developments in the Chronus series, a family of custom controllers that afford a performer gestural interaction with surround sound systems that can be easily integrated into their personal performance systems. The controllers are built with the goal of encouraging more electronic musicians to include the creation of dynamic pantophonic fields in performance. The paper focuses on technical advances of the Chronus 2.0 prototype that extend the interface to control both radial and angular positional data, and the controllers' ease of integration into electronic performance configurations, both for diffusion and for performance from the wider electronic music community.},
address = {London, United Kingdom},
author = {Johnson, Bridget and Norris, Michael and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {335--338},
publisher = {Goldsmiths, University of London},
title = {{The Development Of Physical Spatial Controllers}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}349.pdf},
year = {2014}
}
@inproceedings{Cook2009,
address = {Pittsburgh, PA, United States},
author = {Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {1,batteries,composed instruments,hci,i will restate the,laptop orchestras,nime09,original 13 principles for,sensas,the original principles,to begin,voice synthesis,wireless},
pages = {218--221},
title = {{Re-Designing Principles for Computer Music Controllers : a Case Study of SqueezeVox Maggie}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}218.pdf},
year = {2009}
}
@inproceedings{PalacioQuintin2008,
address = {Genoa, Italy},
author = {Palacio-Quintin, Cl{\'{e}}o},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {composition,gestural control,hyper-flute,hyper-instruments,improvisation,interactive music,mapping,nime08,sensors},
pages = {293--298},
title = {{Eight Years of Practice on the Hyper-Flute : Technological and Musical Perspectives}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}293.pdf},
year = {2008}
}
@inproceedings{dAlessandro2006,
address = {Paris, France},
author = {D'Alessandro, Nicolas and D'Alessandro, Christophe and {Le Beux}, Sylvain and Doval, Boris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Singing synthesis,formant synthesis,gestural control.,instrument,spectral model,voice quality,voice source},
pages = {266--271},
title = {{Real-time CALM Synthesizer: New Approaches in Hands-Controlled Voice Synthesis}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}266.pdf},
year = {2006}
}
@inproceedings{Ishida:2004,
address = {Hamamatsu, Japan},
author = {Ishida, Katsuhisa and Kitahara, Tetsuro and Takeda, Masayuki},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {Improvisation support,N-gram model,jam session,melody correction,melody modeling,musical instrument},
pages = {177--180},
title = {{ism: Improvisation Supporting System based on Melody Correction}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}177.pdf},
year = {2004}
}
@inproceedings{Wierenga:2012,
abstract = {In an attempt to utilize the expert pianist's technique and spare bandwidth, a new keyboard-based instrument augmented by sensors suggested by the examination of existing acoustic instruments is introduced. The complete instrument includes a keyboard, various pedals and knee levers, several bowing controllers, and breath and embouchure sensors connected to an Arduino microcontroller that sends sensor data to a laptop running Max/MSP, where custom software maps the data to synthesis algorithms. The audio is output to a digital amplifier powering a transducer mounted on a resonator box to which several of the sensors are attached. Careful sensor selection and mapping help to facilitate performance mode.},
address = {Ann Arbor, Michigan},
author = {Wierenga, Red},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Digital Musical Instrument,Gesture,controllers,keyboard},
month = {may},
publisher = {University of Michigan},
title = {{A New Keyboard-Based, Sensor-Augmented Instrument For Live Performance}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}211.pdf},
year = {2012}
}
@inproceedings{Schmeder2008,
address = {Genoa, Italy},
author = {Schmeder, Andrew and Freed, Adrian},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Schmeder, Freed - 2008 - uOSC The Open Sound Control Reference Platform for Embedded Devices.pdf:pdf},
keywords = {jitter,latency,nime08,open sound control,pic microcontroller,usb},
pages = {175--180},
title = {{uOSC : The Open Sound Control Reference Platform for Embedded Devices}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}175.pdf},
year = {2008}
}
@inproceedings{Pak2006,
address = {Paris, France},
author = {Pak, Jonathan},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
pages = {342--345},
title = {{The Light Matrix: An Interface for Musical Expression and Performance}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}342.pdf},
year = {2006}
}
@inproceedings{BryanKinns2006,
abstract = {This paper reports on ongoing studies of the design and use ofsupport for remote group music making. In this paper weoutline the initial findings of a recent study focusing on thefunction of decay of contributions in collaborative musicmaking. Findings indicate that persistent contributions lendthemselves to individual musical composition and learningnovel interfaces, whilst contributions that quickly decayengender a more focused musical interaction in experiencedparticipants.},
address = {Paris, France},
author = {Bryan-Kinns, Nick and Healey, Patrick G},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {creativity,design,group interaction,music improvisation},
pages = {114--117},
title = {{Decay in Collaborative Music Making}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}114.pdf},
year = {2006}
}
@inproceedings{Caramiaux:2013,
abstract = {We present an overview of machine learning (ML) techniques and theirapplication in interactive music and new digital instruments design. We firstgive to the non-specialist reader an introduction to two ML tasks,classification and regression, that are particularly relevant for gesturalinteraction. We then present a review of the literature in current NIMEresearch that uses ML in musical gesture analysis and gestural sound control.We describe the ways in which machine learning is useful for creatingexpressive musical interaction, and in turn why live music performance presentsa pertinent and challenging use case for machine learning.},
address = {Daejeon, Republic of Korea},
author = {Caramiaux, Baptiste and Tanaka, Atau},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Analysis,Control,Data mining,Gesture,Machine Learning,Musical Expression,Musical Gestures,Sound},
pages = {513--518},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Machine Learning of Musical Gestures}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{tmays:2014,
abstract = {In this paper we expose the need to go beyond the composer/performer model of electronic instrument design and programming to encourage the transmission of compositions and the creation of a repertory via notation of repeatable performance practice. Drawing on 4 years of practice using the Karlax controller (Da Fact) as a base for new digital musical instruments, we present our notation system in detail and cite some mapping strategies and examples from to pieces in a growing repertory of chamber music compositions for electronic and acoustic instruments},
address = {London, United Kingdom},
author = {Mays, Tom and Faber, Francis},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {553--556},
publisher = {Goldsmiths, University of London},
title = {{A Notation System for the Karlax Controller}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}509.pdf},
year = {2014}
}
@inproceedings{Menzies2008,
address = {Genoa, Italy},
author = {Menzies, Dylan},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {audio synthesis,ing,musical expression,nime,nime08,physical model-,virtual reality},
pages = {71--76},
title = {{Virtual Intimacy : Phya as an Instrument}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}071.pdf},
year = {2008}
}
@inproceedings{Ustarroz2011,
address = {Oslo, Norway},
author = {Ustarroz, Paula},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {instrument,interface,musical,musical generation,network,network sniffer,telematics},
number = {June},
pages = {425--428},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{TresnaNet Musical Generation based on Network Protocols}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}425.pdf},
year = {2011}
}
@inproceedings{Taylor2005,
abstract = {We present a real-time system which allows musicians tointeract with synthetic virtual characters as they perform.Using Max/MSP to parameterize keyboard and vocal input, meaningful features (pitch, amplitude, chord information, and vocal timbre) are extracted from live performancein real-time. These extracted musical features are thenmapped to character behaviour in such a way that the musician's performance elicits a response from the virtual character. The system uses the ANIMUS framework to generatebelievable character expressions. Experimental results arepresented for simple characters.},
address = {Vancouver, BC, Canada},
author = {Taylor, Robyn and Torres, Daniel and Boulanger, Pierre},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Music,advanced man-machine inter- faces,artistic in- stallations,behavioural systems,immersive entertainment,interaction tech- niques,synthetic characters,virtual reality,visualization},
pages = {220--223},
title = {{Using Music to Interact with a Virtual Character}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}220.pdf},
year = {2005}
}
@inproceedings{Fencott2010,
address = {Sydney, Australia},
author = {Fencott, Robin and Bryan-Kinns, Nick},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {awareness,collaboration,design,en-,evaluation,gagement,group music making,interaction,music,nime10,privacy},
number = {June 2008},
pages = {198--203},
title = {{Hey Man, You're Invading my Personal Space ! Privacy and Awareness in Collaborative Music}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}198.pdf},
year = {2010}
}
@inproceedings{Franco2004,
address = {Hamamatsu, Japan},
author = {Franco, Enrique and Griffith, Niall J and Fernstr{\"{o}}m, Mikael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {audiovisual,composition,expressiveness,mapping,performance,representation},
pages = {165--168},
title = {{Issues for Designing a Flexible Expressive Audiovisual System for Real-time Performance {\&} Composition}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}165.pdf},
year = {2004}
}
@inproceedings{Tang:2013,
abstract = {CalliMusic, is a system developed for users to generate traditional Chinesemusic by writing Chinese ink brush calligraphy, turning the long-believedstrong linkage between the two art forms with rich histories into reality. Inaddition to traditional calligraphy writing instruments (brush, ink and paper),a camera is the only addition needed to convert the motion of the ink brushinto musical notes through a variety of mappings such as human-inspired,statistical and a hybrid. The design of the system, including details of eachmapping and research issues encountered are discussed. A user study of systemperformance suggests that the result is quite encouraging. The technique is,obviously, applicable to other related art forms with a wide range ofapplications.},
address = {Daejeon, Republic of Korea},
author = {Tang, Will W W and Chan, Stephen and Ngai, Grace and Leong, Hong-va},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Assisted Music Generation,Chinese Calligraphy,Chinese Music},
pages = {84--89},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Computer Assisted Melo-rhythmic Generation of Traditional Chinese Music from Ink Brush Calligraphy}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Blaine2002,
address = {Dublin, Ireland},
author = {Blaine, Tina and Forlines, Clifton},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {collaboration,computer graphics,embedded sensors,gam-,immersive musical gaming experiences,ing controller,mu-,multi-player,novice,sical controller,social interaction},
pages = {1--6},
title = {{JAM-O-WORLD: Evolution of the Jam-O-Drum Multi-player Musical Controller into the Jam-O-Whirl Gaming Interface}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}012.pdf},
year = {2002}
}
@inproceedings{vanTroyer2014,
abstract = {Acoustic feedback controllers (AFCs) are typically applied to solve feedback problems evident in applications such as public address (PA) systems, hearing aids, and speech applications. Applying the techniques of AFCs to different contexts, such as musical performance, sound installations, and product design, presents a unique insight into the research of embodied sonic interfaces and environments. This paper presents techniques that use digital acoustic feedback control algorithms to augment the sonic properties of environments and discusses approaches to the design of sonically playful experiences that apply such techniques. Three experimental prototypes are described to illustrate how the techniques can be applied to versatile environments and continuous coupling of users' audible actions with sonically augmented environments. The knowledge obtained from these prototypes has led to Acoustic Feedback Ecology System (AFES) design patterns. The paper concludes with some future research directions based on the prototypes and proposes several other potentially useful applications ranging from musical performance to everyday contexts.},
address = {London, United Kingdom},
author = {van Troyer, Akito},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {118--121},
publisher = {Goldsmiths, University of London},
title = {{Composing Embodied Sonic Play Experiences: Towards Acoustic Feedback Ecology}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}444.pdf},
year = {2014}
}
@inproceedings{Quessy2007,
address = {New York City, NY, United States},
author = {Quessy, Alexandre},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {433},
title = {{Human Sequencer}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}433.pdf},
year = {2007}
}
@inproceedings{Kleinberger:2013,
abstract = {PAMDI is an electromechanical music controller based on an expansion of thecommon metal music boxes. Our system enables an augmentation of the musicalproperties by adding different musical channels triggered and parameterized bynatural gestures during the ``performance''. All the channels are generatedform the original melody recorded once at the start.To capture and treat the different expressive parameters both natural andintentional, our platform is composed of a metallic structure supportingsensors. The measured values are processed by an arduino system that finallysends the results by serial communication to a Max/MSP patch for signaltreatment and modification. We will explain how our embedded instrument aims to bring a certain awarenessto the player of the mapping and the potential musical freedom of the veryspecific -- and not that much automatic - instrument that is a music box. Wewill also address how our design tackles the different questions of mapping,ergonomics and expressiveness while choosing the controller modalities and theparameters to be sensed.},
address = {Daejeon, Republic of Korea},
author = {Kleinberger, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Tangible interface,mapping.,mechanical and electronic coupling,music box,musical controller},
pages = {19--20},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{PAMDI Music Box: Primarily Analogico-Mechanical, Digitally Iterated Music Box}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Gurevich2006,
address = {Paris, France},
author = {Gurevich, Michael},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {collaborative interface,interaction design,media space,network music,novice,remote jamming},
pages = {118--123},
title = {{JamSpace: Designing A Collaborative Networked Music Space for Novices}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}118.pdf},
year = {2006}
}
@inproceedings{Molina2011,
abstract = {We present BeatJockey, a prototype interface which makesuse of Audio Mosaicing (AM), beat-tracking and machinelearning techniques, for supporting Diskjockeys (DJs) byproposing them new ways of interaction with the songs onthe DJ's playlist. This prototype introduces a new paradigmto DJing in which the user has the capability to mix songsinteracting with beat-units that accompany the DJ's mix.For this type of interaction, the system suggests song slicestaken from songs selected from a playlist, which could gowell with the beats of whatever master song is being played.In addition the system allows the synchronization of multiple songs, thus permitting flexible, coherent and rapid progressions in the DJ's mix. BeatJockey uses the Reactable,a musical tangible user interface (TUI), and it has beendesigned to be used by all DJs regardless of their level ofexpertise, as the system helps the novice while bringing newcreative opportunities to the expert.},
address = {Oslo, Norway},
author = {Molina, Pablo and Haro, Mart{\'{i}}n and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {DJ,audio mosaicing,beat-mash,interactive music interfaces,music information retrieval,percussion,reactable.,realtime,tabletop interaction,turntable},
pages = {288--291},
title = {{BeatJockey : A New Tool for Enhancing DJ Skills}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}288.pdf},
year = {2011}
}
@inproceedings{nime2014:fberthaut,
abstract = {This paper presents a collaborative digital musical instrument that uses the ephemeral and physical properties of soap bubbles to explore the complexity layers and oscillating parameters of electronic (bass) music. This instrument, called Wubbles, aims at encouraging both individual and collaborative musical manipulations.},
address = {London, United Kingdom},
author = {Berthaut, Florent and Knibbe, Jarrod},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {499--500},
publisher = {Goldsmiths, University of London},
title = {{Wubbles: A Collaborative Ephemeral Musical Instrument}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}334.pdf},
year = {2014}
}
@inproceedings{Chuchacz2007,
abstract = {A novel electronic percussion synthesizer prototype is presented. Our ambition is to design an instrument that will produce a high quality, realistic sound based on a physical modelling sound synthesis algorithm. This is achieved using a real-time Field Programmable Gate Array (FPGA) implementation of the model coupled to an interface that aims to make efficient use of all the subtle nuanced gestures of the instrumentalist. It is based on a complex physical model of the vibrating plate - the source of sound in the majority of percussion instruments. A Xilinx Virtex II pro FPGA core handles the sound synthesis computations with an 8 billion operations per second performance and has been designed in such a way to allow a high level of control and flexibility. Strategies are also presented to that allow the parametric space of the model to be mapped to the playing gestures of the percussionist. },
address = {New York City, NY, United States},
author = {Chuchacz, Katarzyna and O'Modhrain, Sile and Woods, Roger},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Electronic Percussion Instrument,FPGA.,Physical Model},
pages = {37--40},
title = {{Physical Models and Musical Controllers -- Designing a Novel Electronic Percussion Instrument}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}037.pdf},
year = {2007}
}
@inproceedings{Muller2009,
address = {Pittsburgh, PA, United States},
author = {M{\"{u}}ller, Alexander and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {gestural,interaction,intuitive interaction,midi controller,nime09,tactile feedback},
pages = {33--34},
title = {{Utilizing Tactile Feedback to Guide Movements Between Sounds}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}033.pdf},
year = {2009}
}
@inproceedings{Marinelli2005,
address = {Vancouver, BC, Canada},
author = {Marinelli, Maia and Lamenzo, Jared and Borissov, Liubo},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
number = {3},
pages = {272},
title = {{Mocean}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}272.pdf},
volume = {26},
year = {2005}
}
@inproceedings{Sinclair2007,
address = {New York City, NY, United States},
author = {Sinclair, Stephen and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {audio,control,force-feedback,haptics,multi-modal,nime07},
pages = {209--212},
title = {{Defining a Control Standard for Easily Integrating Haptic Virtual Environments with Existing Audio / Visual Systems}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}209.pdf},
year = {2007}
}
@inproceedings{Leslie2011,
address = {Oslo, Norway},
author = {Leslie, Grace and Mullen, Tim},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {bcmi,collaboration,eeg,sonification,visualization},
number = {June},
pages = {296--299},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{MoodMixer : EEG-based Collaborative Sonification}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}296.pdf},
year = {2011}
}
@inproceedings{McPherson2010,
address = {Sydney, Australia},
author = {McPherson, Andrew and Kim, Youngmoo},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {actuation,augmented instruments,electromagnetic,gesture measurement,interfaces,nime10,piano},
number = {Nime},
pages = {217--222},
title = {{Augmenting the Acoustic Piano with Electromagnetic String Actuation and Continuous Key Position Sensing}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}217.pdf},
year = {2010}
}
@inproceedings{Nakra2009,
abstract = {The UBS Virtual Maestro is an interactive conducting system designed by Immersion Music to simulate the experience of orchestral conducting for the general public attending a classical music concert. The system utilizes the Wii Remote, which users hold and move like a conducting baton to affect the tempo and dynamics of an orchestral video/audio recording. The accelerometer data from the Wii Remote is used to control playback speed and volume in real-time. The system is housed in a UBSbranded kiosk that has toured classical performing arts venues throughout the United States and Europe in 2007 and 2008. In this paper we share our experiences in designing this standalone system for thousands of users, and lessons that we learned from the project. },
address = {Pittsburgh, PA, United States},
author = {Nakra, Teresa M and Ivanov, Yuri and Smaragdis, Paris and Ault, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Wii Remote,conducting,gesture,interactive installations},
pages = {250--255},
title = {{The UBS Virtual Maestro : an Interactive Conducting System}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}250.pdf},
year = {2009}
}
@inproceedings{aplace:2014,
abstract = {This paper explores the design process of the AlphaSphere, an experimental new musical instrument that has transitioned into scale production and international distribution. Initially, the design intentions and engineering processes are covered. The paper continues by briefly evaluating the user testing process and outlining the ergonomics, communication protocol and software of the device. The paper closes by questioning what it takes to evaluate success as a musical instrument.},
address = {London, United Kingdom},
author = {Place, Adam and Lacey, Liam and Mitchell, Thomas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {399--402},
publisher = {Goldsmiths, University of London},
title = {{AlphaSphere from Prototype to Product}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}568.pdf},
year = {2014}
}
@inproceedings{Torchia2004,
address = {Hamamatsu, Japan},
author = {Torchia, Ryan H and Lippe, Cort},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
pages = {116--119},
title = {{Techniques for Multi-Channel Real-Time Spatial Distribution Using Frequency-Domain Processing}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}116.pdf},
year = {2004}
}
@inproceedings{nime2014:chonigman,
abstract = {This paper introduces a new technique for creating Swept Frequency Capacitive Sensing with open source technology for use in creating richer and more complex musical gestures. This new style of capacitive touch sensing is extremely robust compared to older versions and will allow greater implementation of gesture recognition and touch control in the development of NIMEs. Inspired by the Touch{\{}{\'{e}}{\}} project, this paper discusses how to implement this technique using the community standard hardware Arduino instead of custom designed electronics. The technique requires only passive components and can be used to enhance the touch sensitivity of many everyday objects and even biological materials and substances such as plants, which this paper will focus on as a case study through the project known as Cultivating Frequencies. This paper will discuss different techniques of filtering data captured by this system, different methods for creating gesture recognition unique to the object being used, and the implications of this technology as it pertains to the goal of ubiquitous sensing. Furthermore, this paper will introduce a new Arduino Library, SweepingCapSense, which simplifies the coding required to implement this technique.},
address = {London, United Kingdom},
author = {Honigman, Colin and Hochenbaum, Jordan and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {74--77},
publisher = {Goldsmiths, University of London},
title = {{Techniques in Swept Frequency Capacitive Sensing: An Open Source Approach}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}515.pdf},
year = {2014}
}
@inproceedings{Takahashi2007,
address = {New York City, NY, United States},
author = {Takahashi, Masato and Tanaka, Hiroya},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {429},
title = {{bog : Instrumental Aliens}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}429.pdf},
year = {2007}
}
@inproceedings{Lee:2012a,
abstract = {This paper describes recent extensions to LOLC, a text-based environment for collaborative improvisation for laptop ensembles, which integrate acoustic instrumental musicians into the environment. Laptop musicians author short commands to create, transform, and share pre-composed musical fragments, and the resulting notation is digitally displayed, in real time, to instrumental musicians to sight-read in performance. The paper describes the background and motivations of the project, outlines the design of the original LOLC environment and describes its new real-time notation components in detail, and explains the use of these new components in a musical composition, SGLC, by one of the authors.},
address = {Ann Arbor, Michigan},
author = {Lee, Sang Won and Freeman, Jason and Collela, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
publisher = {University of Michigan},
title = {{Real-Time Music Notation, Collaborative Improvisation, and Laptop Ensembles}},
year = {2012}
}
@inproceedings{Gallin2011,
address = {Oslo, Norway},
author = {Gallin, Emmanuelle and Sirguy, Marc},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Gallin, Sirguy - 2011 - Eobody3 a Ready-to-use Pre-mapped {\&} Multi-protocol Sensor Interface.pdf:pdf},
keywords = {A/D Converter,CV,Computer Music,Controller,DMX,Interface.,MIDI,OSC,Sensor,USB},
pages = {437--440},
title = {{Eobody3: a Ready-to-use Pre-mapped {\&} Multi-protocol Sensor Interface}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}437.pdf},
year = {2011}
}
@inproceedings{Hass2010,
address = {Sydney, Australia},
author = {Hass, Jeffrey},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {dance,keywords,lilypad,nime10,video processing,video tracking},
number = {Nime},
pages = {489--492},
title = {{Creating Integrated Music and Video for Dance : Lessons Learned and Lessons Ignored}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}489.pdf},
year = {2010}
}
@inproceedings{Overholt2009,
address = {Pittsburgh, PA, United States},
author = {Overholt, Dan and Lahey, Byron and {Skriver Hansen}, Anne-Marie and Burleson, Winslow and {Norrgaard Jensen}, Camilla},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {339},
title = {{Pendaphonics}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}339.pdf},
year = {2009}
}
@inproceedings{Oh:2013,
abstract = {"Significant progress in the domains of speech- and singing-synthesis hasenhanced communicative potential of machines. To make computers more vocallyexpressive, however, we need a deeper understanding of how nonlinguistic socialsignals are patterned and perceived. In this paper, we focus on laughterexpressions: how a phrase of vocalized notes that we call ""laughter"" may bemodeled and performed to implicate nuanced meaning imbued in the acousticsignal. In designing our model, we emphasize (1) using high-level descriptorsas control parameters, (2) enabling real-time performable laughter, and (3)prioritizing expressiveness over realism. We present an interactive systemimplemented in ChucK that allows users to systematically play with the musicalingredients of laughter. A crowdsourced study on the perception of synthesizedlaughter showed that our model is capable of generating a range of laughtertypes, suggesting an exciting potential for expressive laughter synthesis."},
address = {Daejeon, Republic of Korea},
author = {Oh, Jieun and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {interface for musical expression,laughter,real-time controller,synthesis model,vocalization},
month = {may},
pages = {190--195},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{LOLOL: Laugh Out Loud On Laptop}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Baath2011,
abstract = {The aim of this study was to investigate how well subjectsbeat out a rhythm using eye movements and to establishthe most accurate method of doing this. Eighteen subjectsparticipated in an experiment were five different methodswere evaluated. A fixation based method was found to bethe most accurate. All subjects were able to synchronizetheir eye movements with a given beat but the accuracywas much lower than usually found in finger tapping studies. Many parts of the body are used to make music but sofar, with a few exceptions, the eyes have been silent. The research presented here provides guidelines for implementingeye controlled musical interfaces. Such interfaces would enable performers and artists to use eye movement for musicalexpression and would open up new, exiting possibilities.},
address = {Oslo, Norway},
author = {B{\aa}$\backslash$aath, Rasmus and Strandberg, Thomas and Balkenius, Christian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Eye tapping,Eye tracking,Rhythm,Sensorimotor synchronization},
pages = {441--444},
title = {{Eye Tapping : How to Beat Out an Accurate Rhythm using Eye Movements}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}441.pdf},
year = {2011}
}
@inproceedings{Ferris2002,
address = {Dublin, Ireland},
author = {Ferris, Kieran and Bannon, Liam},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {assembly,augmented reality,cardboard box,disappearing computer,education,ing,pervasive comput-,play},
pages = {56--58},
title = {{The Musical Box Garden}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}056.pdf},
year = {2002}
}
@inproceedings{ckiefer:2014,
abstract = {Echo State Networks (ESNs), a form of recurrent neural network developed in the field of Reservoir Computing, show significant potential for use as a tool in the design of mappings for digital musical instruments. They have, however, seldom been used in this area, so this paper explores their possible uses. This project contributes a new open source library, which was developed to allow ESNs to run in the Pure Data dataflow environment. Several use cases were explored, focusing on addressing current issues in mapping research. ESNs were found to work successfully in scenarios of pattern classification, multiparametric control, explorative mapping and the design of nonlinearities and uncontrol. $\backslash$emph{\{}Un-trained{\}} behaviours are proposed, as augmentations to the conventional reservoir system that allow the player to introduce potentially interesting non-linearities and uncontrol into the reservoir. Interactive evolution style controls are proposed as strategies to help design these behaviours, which are otherwise dependent on arbitrary parameters. A study on sound classification shows that ESNs can reliably differentiate between two drum sounds, and also generalise to other similar input. Following evaluation of the use cases, heuristics are proposed to aid the use of ESNs in computer music scenarios.},
address = {London, United Kingdom},
author = {Kiefer, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {293--298},
publisher = {Goldsmiths, University of London},
title = {{Musical Instrument Mapping Design with Echo State Networks}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}530.pdf},
year = {2014}
}
@inproceedings{Weinberg2007,
address = {New York City, NY, United States},
author = {Weinberg, Gil and Driscoll, Scott},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {human-machine interaction,improvisation,nime07,perceptual modeling,robotic musicianship},
pages = {228--233},
title = {{The Design of a Robotic Marimba Player -- Introducing Pitch into Robotic Musicianship}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}228.pdf},
year = {2007}
}
@inproceedings{Yuksel:2012,
abstract = {In this work, a comprehensive study is performed on the relationship between audio, visual and emotion by applying the principles of cognitive emotion theory into digital creation. The study is driven by an audiovisual emotion library project that is named AVIEM, which provides an interactive interface for experimentation and evaluation of the perception and creation processes of audiovisuals. AVIEM primarily consists of separate audio and visual libraries and grows with user contribution as users explore different combinations between them. The library provides a wide range of experimentation possibilities by allowing users to create audiovisual relations and logging their emotional responses through its interface. Besides being a resourceful tool of experimentation, AVIEM aims to become a source of inspiration, where digitally created abstract virtual environments and soundscapes can elicit target emotions at a preconscious level, by building genuine audiovisual relations that would engage the viewer on a strong emotional stage. Lastly, various schemes are proposed to visualize information extracted through AVIEM, to improve the navigation and designate the trends and dependencies among audiovisual relations.},
address = {Ann Arbor, Michigan},
author = {Yuksel, Kamer Ali and Buyukbas, Sinan and Ayiter, Elif},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Designing emotive audiovisuals,audiovisual perception and interaction,cognitive emotion theory,synaesthesia},
publisher = {University of Michigan},
title = {{An Interface for Emotional Expression in Audio-Visuals}},
year = {2012}
}
@inproceedings{Caceres2005,
abstract = {The Self-Contained Unified Bass Augmenter (SCUBA) is a new augmentative OSC (Open Sound Control) [5] controller for the tuba. SCUBA adds new expressive possibilities to the existing tuba interface through onboard sensors. These sensors provide continuous and discrete user-controlled parametric data to be mapped at will to signal processing parameters, virtual instrument control parameters, sound playback, and various other functions. In its current manifestation, control data is mapped to change the processing of the instrument's natural sound in Pd (Pure Data) [3]. SCUBA preserves the unity of the solo instrument interface by acoustically mixing direct and processed sound in the instrument's bell via mounted satellite speakers, which are driven by a subwoofer below the performer's chair. The end result augments the existing interface while preserving its original unity and functionality. },
address = {Vancouver, BC, Canada},
author = {C{\'{a}}ceres, Juan Pablo and Mysore, Gautham J and Trevi{\~{n}}o, Jeffrey},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Interactive music,Open Sound Control (OSC),electro-acoustic musical instruments,human computer interface,musical instrument design,signal processing},
pages = {38--41},
title = {{{\{}SC{\}}UBA: The Self-Contained Unified Bass Augmenter}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}038.pdf},
year = {2005}
}
@inproceedings{Kirk2007,
abstract = {The FrankenPipe project is an attempt to convert a traditionalHighland Bagpipe into a controller capable of driving both realtime synthesis on a laptop as well as a radio-controlled (RC) car.Doing so engages musical creativity while enabling novel, oftenhumorous, performance art. The chanter is outfitted withphotoresistors (CdS photoconductive cells) underneath each hole,allowing a full range of MIDI values to be produced with eachfinger and giving the player a natural feel. An air-pressure sensoris also deployed in the bag to provide another element of controlwhile capturing a fundamental element of bagpipe performance.The final product navigates the realm of both musical instrumentand toy, allowing the performer to create a novel yet richperformance experience for the audience.},
address = {New York City, NY, United States},
author = {Kirk, Turner and Leider, Colby},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {FrankenPipe,MIDI,alternate controller,bagpipe,chanter.,photoresistor},
pages = {301--304},
title = {{The FrankenPipe : A Novel Bagpipe Controller}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}301.pdf},
year = {2007}
}
@inproceedings{Nakanishi:2013,
abstract = {In this paper, the authors introduce an interactive device, ``POWDER BOX''for use by novices in musical sessions. ``POWDER BOX'' is equipped withsensor-based replaceable interfaces, which enable participants to discover andselect their favorite playing styles of musical instruments during a musicalsession. In addition, it has a wireless communication function thatsynchronizes musical scale and BPM between multiple devices. To date, various kinds of ``inventive'' electronic musical instruments havebeen created in the field of Computer Music field. The authors are interestedin formations of musical sessions, aiming for a balance between simpleinteraction and musical expression. This study focuses on the development ofperformance playing styles.Musicians occasionally change their playing styles (e.g., guitar pluckingstyle) during a musical session. Generally, it is difficult for nonmusicians toachieve this kind of smooth changing depends on levels of their skillacquisition. However, it is essentially important for enjoying musical sessionswhether people could acquire these skills. Here, the authors attempted to develop the device that supports nonmusicians toconquer this point using replaceable interfaces. The authors expected thatchanging interfaces would bring similar effect as changing playing style by theskillful player. This research aims to establish an environment in whichnonmusicians and musicians share their individual musical ideas easily. Here,the interaction design and configuration of the ``POWDER BOX'' is presented.},
address = {Daejeon, Republic of Korea},
author = {Nakanishi, Yoshihito and Matsumura, Seiichiro and Arakawa, Chuichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Musical instrument,replaceable interface,sensors,synthesizer},
pages = {373--376},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{POWDER BOX: An Interactive Device with Sensor Based Replaceable Interface For Musical Session}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Lee2011,
address = {Oslo, Norway},
author = {Lee, Jeong-seob and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Doppler effect,Stereotypical transducers,audible sound,handfree interface,interactive performance,musical instrument},
pages = {24--27},
title = {{Sonicstrument : A Musical Interface with Stereotypical Acoustic Transducers}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}024.pdf},
year = {2011}
}
@inproceedings{Couturier2003,
address = {Montreal, QC, Canada},
author = {Couturier, Jean-Michel and Arfib, Daniel},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {184--187},
title = {{Pointing Fingers: Using Multiple Direct Interactions with Visual Objects to Perform Music}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}184.pdf},
year = {2003}
}
@inproceedings{Dannenberg2005,
address = {Vancouver, BC, Canada},
author = {Dannenberg, Roger B and Brown, Ben and Zeglin, Garth and Lupish, Ron},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {80--84},
title = {{McBlare: A Robotic Bagpipe Player}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}080.pdf},
year = {2005}
}
@inproceedings{eberdahl:2014,
abstract = {An embedded acoustic instrument is an embedded musical instrument that provides a direct acoustic output. This paper describes how to make embedded acoustic instruments using laser cutting for digital fabrication. Several tips are given for improving the acoustic quality including: employing maximally stiff material, placing loudspeaker drivers in the corners of enclosure faces, increasing the stiffness of ``loudspeaker" faces by doubling their thickness, choosing side-lengths with non-integer ratios, and incorporating bracing. Various versions of an open design of the ``LapBox" are provided to help community members replicate and extend the work. A procedure is suggested for testing and optimizing the acoustic quality.},
address = {London, United Kingdom},
author = {Berdahl, Edgar},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Berdahl - 2014 - How to Make Embedded Acoustic Instruments.pdf:pdf},
month = {jun},
pages = {140--143},
publisher = {Goldsmiths, University of London},
title = {{How to Make Embedded Acoustic Instruments}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}551.pdf},
year = {2014}
}
@inproceedings{Ramkissoon2011,
address = {Oslo, Norway},
author = {Ramkissoon, Izzi},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {augmented instruments,controllers,electric bass,gesture,interactive music,interactive performance systems,video},
number = {June},
pages = {224--227},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{The Bass Sleeve: A Real-time Multimedia Gestural Controller for Augmented Electric Bass Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}224.pdf},
year = {2011}
}
@inproceedings{Garcia2011,
address = {Oslo, Norway},
author = {Garc{\'{i}}a, Francisco and Vinceslas, Leny and Tubau, Josep and Maestre, Esteban},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {blowing,instrumental gesture,multi-modal data,pressure,recorder,wind instrument},
number = {June},
pages = {124--127},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Acquisition and Study of Blowing Pressure Profiles in Recorder Playing}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}124.pdf},
year = {2011}
}
@inproceedings{Socolofsky2005,
address = {Vancouver, BC, Canada},
author = {Socolofsky, Eric},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {271},
title = {{Contemplace}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}271.pdf},
year = {2005}
}
@inproceedings{Bisig:2013,
abstract = {This paper presents a proof of concept implementation of an interface entitledCoral. The interface serves as a physical and haptic extension of a simulatedcomplex system, which will be employed as an intermediate mechanism for thecreation of generative music and imagery. The paper discusses the motivationand conceptual context that underly the implementation, describes its technicalrealisation and presents some first interaction experiments. The paper focuseson the following two aspects: the interrelation between the physical andvirtual behaviours and properties of the interface and simulation, and thecapability of the interface to enable an intuitive and tangible exploration ofthis hybrid dynamical system.},
address = {Daejeon, Republic of Korea},
author = {Bisig, Daniel and Schiesser, S{\'{e}}bastien},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {generative art,haptic interface,swarm simulation},
pages = {385--388},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Coral - a Physical and Haptic Extension of a Swarm Simulation}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Cadoz2003,
address = {Montreal, QC, Canada},
author = {Cadoz, Claude and Luciani, Annie and Florens, Jean-Loup and Castagn{\'{e}}, Nicolas},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {235--246},
title = {{ACROE - ICA Artistic Creation and Computer Interactive Multisensory Simulation Force Feedback Gesture Transducers}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}235.pdf},
year = {2003}
}
@inproceedings{Murphy2010,
address = {Sydney, Australia},
author = {Murphy, Jim and Kapur, Ajay and Burgin, Carl},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {nime10},
number = {June},
pages = {15--18},
title = {{The Helio : A Study of Membrane Potentiometers and Long Force Sensing Resistors for Musical Interfaces}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}459.pdf},
year = {2010}
}
@inproceedings{nime2014:dmazzanti,
abstract = {Designing a collaborative performance requires the use of paradigms and technologies which can deeply influence the whole piece experience. In this paper we define a set of six variables, and use them to describe and evaluate a number of platforms for participatory performances. Based on this evaluation, the Augmented Stage is introduced. Such concept describes how Augmented Reality techniques can be used to superimpose a performance stage with a virtual environment, populated with interactive elements. The manipulation of these objects allows spectators to contribute to the visual and sonic outcome of the performance through their mobile devices, while keeping their freedom to focus on the stage. An interactive acoustic rock performance based on this concept was staged. Questionnaires distributed to the audience and performers' comments have been analyzed, contributing to an evaluation of the presented concept and platform done through the defined variables.},
address = {London, United Kingdom},
author = {Mazzanti, Dario and Zappi, Victor and Caldwell, Darwin and Brogni, Andrea},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {29--34},
publisher = {Goldsmiths, University of London},
title = {{Augmented Stage for Participatory Performances}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}413.pdf},
year = {2014}
}
@inproceedings{Freed2009,
address = {Pittsburgh, PA, United States},
author = {Freed, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {both kirchoff,circuit laws and ohms,current steering multitouch,nime09,piezoresistive touch sensor pressure,reader is encouraged to,resistive networks so the,review,s voltage and current,sensing},
pages = {230--235},
title = {{Novel and Forgotten Current-steering Techniques for Resistive Multitouch, Duotouch, and Polytouch Position Sensing with Pressure}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}230.pdf},
year = {2009}
}
@inproceedings{Murphy2010,
abstract = {This paper describes a study of membrane potentiometers and long force sensing resistors as tools to enable greater interaction between performers and audiences. This is accomplished through the building of a new interface called the Helio. In preparation for the Helio's construction, a variety of brands of membrane potentiometers and long force sensing resistors were analyzed for their suitability for use in a performance interface. Analog and digital circuit design considerations are discussed. We discuss in detail the design process and performance scenarios explored with the Helio. },
address = {Sydney, Australia},
author = {Murphy, Jim and Kapur, Ajay and Burgin, Carl},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Force Sensing Resistors,Haptic Feedback,Helio,Membrane Potentiometers},
pages = {459--462},
title = {{The Helio : A Study of Membrane Potentiometers and Long Force Sensing Resistors for Musical Interfaces}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}459.pdf},
year = {2010}
}
@inproceedings{Bonardi2006,
abstract = {In this article, we present the first step of our research work todesign a Virtual Assistant for Performers and Stage Directors,able to give a feedback from performances. We use amethodology to automatically construct fuzzy rules in a FuzzyRule-Based System that detects contextual emotions from anactor's performance during a show.We collect video data from a lot of performances of the sameshow from which it should be possible to visualize all theemotions and intents or more precisely "intent graphs". Toperform this, the collected data defining low-level descriptorsare aggregated and converted into high-level characterizations.Then, depending on the retrieved data and on their distributionon the axis, we partition the universes into classes. The last stepis the building of the fuzzy rules that are obtained from theclasses and that permit to give conclusions to label the detectedemotions.},
address = {Paris, France},
author = {Bonardi, Alain and Truck, Isis and Akdag, Herman},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Emotion detector,Fuzzy Classes,Intents,Performance.,Stage Director,Virtual Assistant},
pages = {326--329},
title = {{Towards a Virtual Assistant for Performers and Stage Directors}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}326.pdf},
year = {2006}
}
@inproceedings{Trump2014,
abstract = {This paper describes the concept and design of Orphion, a new digital musical instrument based on the Apple iPad. We begin by outlining primary challenges associated with DMI design, focussing on the specific problems Orphion seeks to address such as requirements for haptic feedback from the device. Orphion achieves this by incorporating an interaction model based on tonally tuned virtual ``pads" in user-configurable layouts, where the pitch and timbre associated with each pad depends on the initial point of touch, touch point size and size variation, and position after the initial touch. These parameters control a physical model for sound generation with visual feedback provided via the iPad display. We present findings from the research and development process including design revisions made in response to user testing. Finally, conclusions are made about the effectiveness of the instrument based on large-scale user feedback.},
address = {London, United Kingdom},
author = {Trump, Sebastian and Bullock, Jamie},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {159--162},
publisher = {Goldsmiths, University of London},
title = {{Orphion: A Gestural Multi-Touch Instrument for the iPad}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}277.pdf},
year = {2014}
}
@inproceedings{DeJong2009,
address = {Pittsburgh, PA, United States},
author = {de Jong, Staas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {musical controller,nime09,tactile interface},
pages = {163--164},
title = {{Developing the Cyclotactor}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}163.pdf},
year = {2009}
}
@inproceedings{Park:2013b,
abstract = {SSN (Sound Surfing Network) is a performance system that provides a new musicalexperience by incorporating mobile phone-based spatial sound control tocollaborative music performance. SSN enables both the performer and theaudience to manipulate the spatial distribution of sound using the smartphonesof the audience as distributed speaker system. Proposing a new perspective tothe social aspect music appreciation, SSN will provide a new possibility tomobile music performances in the context of interactive audience collaborationas well as sound spatialization.},
address = {Daejeon, Republic of Korea},
author = {Park, Saebyul and Ban, Seonghoon and Hong, Dae Ryong and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Park et al. - 2013 - Sound Surfing Network (SSN) Mobile Phone-based Sound Spatialization with Audience Collaboration.pdf:pdf},
keywords = {Mobile music,audience participation,digital performance,smartphone,spatial sound control},
pages = {111--114},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Sound Surfing Network (SSN): Mobile Phone-based Sound Spatialization with Audience Collaboration}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{nime2014:jeaton,
abstract = {The Space Between Us is a live performance piece for vocals, piano and live electronics using a Brain-Computer Music Interface system for emotional control of the score. The system not only aims to reflect emotional states but to direct and induce emotional states through the real-time generation of the score, highlighting the potential of direct neural-emotional manipulation in live performance. The EEG of the vocalist and one audience member is measured throughout the performance and the system generates a real-time score based on mapping the emotional features within the EEG. We measure the two emotional descriptors, valence and arousal, within EEG and map the two-dimensional correlate of averaged windows to musical phrases. These pre-composed phrases contain associated emotional content based on the KTH Performance Rules System (Director Musices). The piece is in three movements, the first two are led by the emotions of each subject respectively, whilst the third movement interpolates the combined response of the performer and audience member. The system not only aims to reflect the individuals' emotional states but also attempts to induce a shared emotional experience by drawing the two responses together. This work highlights the potential available in affecting neural-emotional manipulation within live performance and demonstrates a new approach to real-time, affectively-driven composition.},
address = {London, United Kingdom},
author = {Eaton, Joel and Jin, Weiwei and Miranda, Eduardo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {593--596},
publisher = {Goldsmiths, University of London},
title = {{The Space Between Us. A Live Performance with Musical Score Generated via Emotional Levels Measured in EEG of One Performer and an Audience Member}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}418.pdf},
year = {2014}
}
@inproceedings{Dobrian2006,
abstract = {Is there a distinction between New Interfaces for MusicalExpression and New Interfaces for Controlling Sound? Thisarticle begins with a brief overview of expression in musicalperformance, and examines some of the characteristics ofeffective "expressive" computer music instruments. Itbecomes apparent that sophisticated musical expressionrequires not only a good control interface but also virtuosicmastery of the instrument it controls. By studying effectiveacoustic instruments, choosing intuitive but complexgesture-sound mappings that take advantage of establishedinstrumental skills, designing intelligent characterizationsof performance gestures, and promoting long-term dedicatedpractice on a new interface, computer music instrumentdesigners can enhance the expressive quality of computermusic performance.},
address = {Paris, France},
author = {Dobrian, Christopher and Koppelman, Daniel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Expression,instrument design,performance,virtuosity.},
pages = {277--282},
title = {{The E in NIME: Musical Expression with New Computer Interfaces}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}277.pdf},
year = {2006}
}
@inproceedings{Jorda:2002,
abstract = {In this paper we present Afasia, an interactive multimedia performance based in Homer's Odyssey [2]. Afasia is a one-man digital theater play in which a lone performer fitted with a sensor-suit conducts, like Homer, the whole show by himself, controlling 2D animations, DVD video and conducting the music mechanically performed by a robot quartet. After contextualizing the piece, all of its technical elements, starting with the hardware input and output components, are described. A special emphasis is given to the interactivity strategies and the subsequent software design. Since its first version premiered in Barcelona in 1998, Afasia has been performed in many European and American countries and has received several international awards. },
address = {Dublin},
author = {Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Multimedia interaction,musical robots,real-time musical systems.},
pages = {102--107},
title = {{Afasia: the Ultimate Homeric One-man-multimedia-band}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}102.pdf},
year = {2002}
}
@inproceedings{Kim2011a,
address = {Oslo, Norway},
author = {Kim, Seunghun and Yeo, Woon Seung},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {amplified,design of the interface,figure 1,filtered,in which the measured,mapping,pipe,resonance,sound signal is low-pass},
number = {June},
pages = {217--219},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Musical Control of a Pipe Based on Acoustic Resonance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}217.pdf},
year = {2011}
}
@inproceedings{Tsiros2014,
abstract = {This paper presents the findings of two exploratory studies. In these studies participants performed a series of image-sound association tasks. The aim of the studies was to investigate the perceived similarity and the efficacy of two multidimensional mappings each consisting of three audio-visual associations. The purpose of the mappings is to enable visual control of corpus-based concatenative synthesis. More specifically the stimuli in the first study was designed to test the perceived similarity of six audio-visual associations, between the two mappings using three corpora resulting in 18 audio-visual stimuli. The corpora differ in terms of two sound characteristics: harmonic contain and continuity. Data analysis revealed no significant differences in the participant's responses between the three corpora, or between the two mappings. However highly significant differences were revealed between the individual audio-visual association pairs. The second study investigates the affects of the mapping and the corpus in the ability of the participants to detect which image out of three similar images was used to generate six audio stimuli. The data analysis revealed significant differences in the ability of the participants' to detect the correct image depending on which corpus was used. Less significant was the effect of the mapping in the success rate of the participant responses.},
address = {London, United Kingdom},
author = {Tsiros, Augoustinos},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {421--426},
publisher = {Goldsmiths, University of London},
title = {{Evaluating the Perceived Similarity Between Audio-Visual Features Using Corpus-Based Concatenative Synthesis}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}484.pdf},
year = {2014}
}
@inproceedings{nime2014:jmathew,
abstract = {Recent technological improvements in audio reproduction systems increased the possibilities to spatialize sources in a listening environment. The spatialization of reproduced audio is however highly dependent on the recording technique, the rendering method, and the loudspeaker configuration. While object-based audio production has proven to reduce the dependency on loudspeaker configurations, authoring tools are still considered to be difficult to interact with in current production environments. In this paper, we investigate the issues of spatialization techniques for object-based audio production and introduce the Spatial Audio Design Spaces (SpADS) framework, that provides insights into the spatial manipulation of object-based audio. Based on interviews with professional sound engineers, this morphological analysis clarifies the relationships between recording and rendering techniques that define audio-objects for 3D speaker configurations, allowing the analysis and the design of advanced object-based controllers as well.},
address = {London, United Kingdom},
author = {Mathew, Justin and Huot, St{\'{e}}phane and Blum, Alan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {415--420},
publisher = {Goldsmiths, University of London},
title = {{A Morphological Analysis of Audio-Objects and their Control Methods for 3D Audio}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}420.pdf},
year = {2014}
}
@inproceedings{Takegawa2008,
address = {Genoa, Italy},
author = {Takegawa, Yoshinari and Tsukamoto, Masahiko},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {automatic,block interface,nime08,portable keyboard instruments},
pages = {289--292},
title = {{UnitKeyboard : An Easily Configurable Compact Clavier}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}289.pdf},
year = {2008}
}
@inproceedings{Berdahl2011,
address = {Oslo, Norway},
author = {Berdahl, Edgar and Chafe, Chris},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {arduino,autonomous,satellite ccrma,standalone},
number = {June},
pages = {322--323},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Autonomous New Media Artefacts ( AutoNMA )}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}322.pdf},
year = {2011}
}
@inproceedings{Lugo2005,
address = {Vancouver, BC, Canada},
author = {Lugo, Robert and Damondrick, Jack},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
isbn = {6507233811},
pages = {246--247},
title = {{Beat Boxing : Expressive Control for Electronic Music Performance and Musical Applications}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}246.pdf},
year = {2005}
}
@inproceedings{Holm2006,
abstract = {This paper discusses the concept of using background music to control video game parameters and thus actions on the screen. Each song selected by the player makes the game look different and behave variedly. The concept is explored by modifying an existing video game and playtesting it with different kinds of MIDI music. Several examples of mapping MIDI parameters to game events are presented. As mobile phones' MIDI players do not usually have a dedicated callback API, a real-time MIDI analysis software for Symbian OS was implemented. Future developments including real-time group performance as a way to control game content are also considered. },
address = {Paris, France},
author = {Holm, Jukka and Arrasvuori, Juha and Havukainen, Kai},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Games,MIDI,MIDI-controlled games,Virtual Sequencer.,background music reactive games,music,musically controlled games,rhythm games},
pages = {65--70},
title = {{Using MIDI to Modify Video Game Content}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}065.pdf},
year = {2006}
}
@inproceedings{Schlessinger2009,
address = {Pittsburgh, PA, United States},
author = {Schlessinger, Daniel and Smith, Julius O},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {accordion,electro-acoustic instruments,kalichord,kalimba,nime09,physical model,piezo,plucked,string,tine},
pages = {98--101},
title = {{The Kalichord : A Physically Modeled Electro-Acoustic Plucked String Instrument}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}098.pdf},
year = {2009}
}
@inproceedings{nime2014:cmckinney,
abstract = {With the growing adoption of internet connectivity across the world, online collaboration is still a difficult and slow endeavor. Many amazing languages and tools such as SuperCollider, ChucK, and Max/MSP all facilitate networking and collaboration, however these languages and tools were not created explicitly to make group performances simple and intuitive. New web standards such as Web Audio and Web GL introduce the capability for web browsers to duplicate many of the features in computer music tools. This paper introduces Lich.js, an effort to bring musicians together over the internet with minimal effort by leveraging web technologies.},
address = {London, United Kingdom},
author = {McKinney, Chad},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {379--382},
publisher = {Goldsmiths, University of London},
title = {{Quick Live Coding Collaboration In The Web Browser}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}519.pdf},
year = {2014}
}
@inproceedings{Kim-Boyle2005,
abstract = {The ,
,
author describes a recent composition for piano andcomputer in which the score performed by the pianist, readfrom a computer monitor, is generated in real-time from avocabulary of predetermined scanned score excerpts. The,
,
author outlines the algorithm used to choose and display aparticular excerpt and describes some of the musicaldifficulties faced by the pianist in a performance of the work. },
address = {Vancouver, BC, Canada},
author = {Kim-Boyle, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Jitter.,Score generation},
pages = {238--239},
title = {{Musical Score Generation in Valses and Etudes}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}238.pdf},
year = {2005}
}
@inproceedings{Ventura2003,
address = {Montreal, QC, Canada},
author = {Ventura, David and Mase, Kenji},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {91--94},
title = {{Duet Musical Companion: Improvisational Interfaces for Children}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}091.pdf},
year = {2003}
}
@inproceedings{Steiner2007,
address = {New York City, NY, United States},
author = {Steiner, Hans-Christoph},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {375--378},
title = {{A Unified Toolkit for Accessing Human Interface Devices in Pure Data and Max / MSP}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}375.pdf},
year = {2007}
}
@inproceedings{Fohl:2013,
abstract = {This paper presents the design and implementation of agesture control interface for a wave field synthesis system.The user's motion is tracked by a IR-camera-based trackingsystem. The developed connecting software processes thetracker data to modify the positions of the virtual soundsources of the wave field synthesis system. Due to the mod-ular design of the software, the triggered actions of the ges-tures may easily be modified. Three elementary gestureswere designed and implemented: Select / deselect, circularmovement and radial movement. The guidelines for gesturedesign and detection are presented, and the user experiencesare discussed.},
address = {Daejeon, Republic of Korea},
author = {Fohl, Wolfgang and Nogalski, Malte},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Wave field synthesis,gesture control},
pages = {341--346},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Gesture Control Interface for a Wave Field Synthesis System}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Lai2009,
address = {Pittsburgh, PA, United States},
author = {Lai, Chi-Hsia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {as a solo performance,audiovisual,designed to project,for the,hands on stage is,instead of projecting,instrument,interface design,nime09,performance,performer,surface to provide feedback,user,visuals onto the table},
pages = {39--40},
title = {{Hands On Stage : A Sound and Image Performance Interface}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}039.pdf},
year = {2009}
}
@inproceedings{Burt2010,
abstract = {This paper describes a series of mathematical functions implemented by the ,
,
author in the commercial algorithmic software language ArtWonk, written by John Dunn, which are offered with that language as resources for composers. It gives a history of the development of the functions, with an emphasis on how I developed them for use in my compositions. },
address = {Sydney, Australia},
author = {Burt, Warren},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Algorithmic composition,additive sequences,fractals,mathematical composition,probability distributions},
pages = {493--496},
title = {{Packages for ArtWonk : New Mathematical Tools for Composers}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}493.pdf},
year = {2010}
}
@inproceedings{McPherson:2012a,
abstract = {There is growing interest in the field of augmented musical instruments, which extend traditional acoustic instruments using new sensors and actuators. Several designs use electromagnetic actuation to induce vibrations in the acoustic mechanism, manipulating the traditional sound of the in-strument without external speakers. This paper presents techniques and guidelines for the use of electromagnetic actuation in augmented instruments, including actuator design and selection, interfacing with the instrument, and cir-cuits for driving the actuators. The material in this pa-per forms the basis of the magnetic resonator piano, an electromagnetically-augmented acoustic grand piano now in its second design iteration. In addition to discussing applications to the piano, this paper aims to provide a toolbox to accelerate the design of new hybrid acoustic-electronic instruments.},
address = {Ann Arbor, Michigan},
author = {McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {augmented instruments,circuit design,electromagnetic actuation,hardware},
month = {may},
publisher = {University of Michigan},
title = {{Techniques and Circuits for Electromagnetic Instrument Actuation}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}117.pdf},
year = {2012}
}
@inproceedings{Jacquemin2006,
address = {Paris, France},
author = {Jacquemin, Christian and de Laubier, Serge},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {audio-visual composition,feedback,transmodality},
pages = {156--161},
title = {{Transmodal Feedback as a New Perspective for Audio-visual Effects}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}156.pdf},
year = {2006}
}
@inproceedings{nime2014:btaylor,
abstract = {Developing for mobile and multimodal platforms is more important now than ever, as smartphones and tablets proliferate and mobile device orchestras become commonplace. We detail NexusUI, a JavaScript framework that enables rapid prototyping and development of expressive multitouch electronic instrument interfaces within a web browser. Extensions of this project assist in easily creating dynamic user interfaces. NexusUI contains several novel encapsulations of creative interface objects, each accessible with one line of code. NexusUp assists in one-button duplication of Max interfaces into mobile-friendly web pages that transmit to Max automatically via Open Sound Control. NexusDrop enables drag-and-drop interface building and saves interfaces to a central Nexus database. Finally, we provide an overview of several projects made with NexusUI, including mobile instruments, art installations, sound diffusion tools, and iOS games, and describe Nexus' possibilities as an architecture for our future Mobile App Orchestra.},
address = {London, United Kingdom},
author = {Taylor, Benjamin and Allison, Jesse and Conlin, William and Oh, Yemin and Holmes, Daniel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {257--262},
publisher = {Goldsmiths, University of London},
title = {{Simplified Expressive Mobile Development with NexusUI, NexusUp, and NexusDrop}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}480.pdf},
year = {2014}
}
@inproceedings{Bencina2005,
address = {Vancouver, BC, Canada},
author = {Bencina, Ross},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
keywords = {computational geometry,design,design support,high-level control,interpolation,mapping,of interpo-,this section reviews related,user interface,work in the field},
pages = {101--104},
title = {{The Metasurface  Applying Natural Neighbour Interpolation to Two-to-Many Mapping}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}101.pdf},
year = {2005}
}
@inproceedings{Guaus2010,
address = {Sydney, Australia},
author = {Guaus, Enric and Ozaslan, Tan and Palacios, Eric and Arcos, Josep L},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {capacitive sensors,gesture acquisition,guitar,nime10},
number = {Nime},
pages = {238--243},
title = {{A Left Hand Gesture Caption System for Guitar Based on Capacitive Sensors}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}238.pdf},
year = {2010}
}
@inproceedings{Pan2010,
address = {Sydney, Australia},
author = {Pan, Ye and Kim, Min-Gyu and Suzuki, Kenji},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {human-robot interaction,initiative exchange,nime10,prediction},
number = {Nime},
pages = {15--18},
title = {{A Robot Musician Interacting with a Human Partner through Initiative Exchange}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}166.pdf},
year = {2010}
}
@inproceedings{Astrinaki:2013,
abstract = {This paper describes the recent progress in our approach to generateperformative and controllable speech. The goal of the performative HMM-basedspeech and singing syn- thesis library, called Mage, is to have the ability togenerate natural sounding speech with arbitrary speaker's voicecharacteristics, speaking styles and expressions and at the same time to haveaccurate reactive user control over all the available production levels. Mageallows to arbitrarily change between voices, control speaking style or vocalidentity, manipulate voice characteristics or alter the targeted contexton-the-fly and also maintain the naturalness and intelligibility of the output.To achieve these controls, it was essential to redesign and improve the initiallibrary. This paper focuses on the improvements of the architectural design,the additional user controls and provides an overview of a prototype, where aguitar is used to reactively control the generation of a synthetic voice invarious levels.},
address = {Daejeon, Republic of Korea},
author = {Astrinaki, Maria and D'Alessandro, Nicolas and Reboursi{\`{e}}re, Lo{\"{i}}c and Moinet, Alexis and Dutoit, Thierry},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {augmented guitar,hexaphonic guitar,speech synthesis},
pages = {547--550},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{MAGE 2.0: New Features and its Application in the Development of a Talking Guitar}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Choi2010,
abstract = {We propose an environment that allows users to create music by leveraging playful visualization and organic interaction. Our attempt to improve ideas drawn from traditional sequencer paradigm has been made in terms of extemporizing music and associating with visualization in real-time. In order to offer different user experience and musical possibility, this system incorporates many techniques, including; flocking simulation, nondeterministic finite automata (NFA), score file analysis, vector calculation, OpenGL animation, and networking. We transform a sequencer into an audiovisual platform for composition and performance, which is furnished with artistry and ease of use. Thus we believe that it is suitable for not only artists such as algorithmic composers or audiovisual performers, but also anyone who wants to play music and imagery in a different way. },
address = {Sydney, Australia},
author = {Choi, Hongchan and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {algorithmic composition,audiovisual,automata,behavior simulation,music,music sequencer,musical interface,nime10,visualization},
pages = {112--115},
title = {{LUSH : An Organic Eco + Music System}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}112.pdf},
year = {2010}
}
@inproceedings{Cook2009,
address = {Pittsburgh, PA, United States},
author = {Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {1,batteries,composed instruments,hci,i will restate the,laptop orchestras,nime09,original 13 principles for,sensas,the original principles,to begin,voice synthesis,wireless},
pages = {218--221},
title = {{Re-Designing Principles for Computer Music Controllers : a Case Study of SqueezeVox Maggie}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}218.pdf},
volume = {11},
year = {2009}
}
@inproceedings{Lopez2011,
address = {Oslo, Norway},
author = {Lopez, Pedro and Ferreira, Alfredo and Pereira, J A Madeiras},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {djing,expert user evaluation,hci,multitouch interaction},
number = {June},
pages = {367--372},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Battle of the DJs: an HCI Perspective of Traditional, Virtual, Hybrid and Multitouch DJing}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}367.pdf},
year = {2011}
}
@inproceedings{ldonovan:2014,
abstract = {This paper presents the Talking Guitar, an electric guitar augmented with a system which tracks the position of the headstock in real time and uses that data to control the parameters of a formant-filtering effect which impresses upon the guitar sound a sense of speech. A user study is conducted with the device to establish an indication of the practicality of using headstock tracking to control effect parameters and to suggest natural and useful mapping strategies. Individual movements and gestures are evaluated in order to guide further development of the system.},
address = {London, United Kingdom},
author = {Donovan, Liam and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {351--354},
publisher = {Goldsmiths, University of London},
title = {{The Talking Guitar: Headstock Tracking and Mapping Strategies}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}407.pdf},
year = {2014}
}
@inproceedings{Pardue:2013,
abstract = {The Hand Controller is a new interface designed to enable a performer toachieve detailed control of audio and visual parameters through a tangibleinterface combined with motion tracking of the hands to capture large scalephysical movement. Such movement empowers an expressive dynamic for bothperformer and audience. However tracking movements in free space isnotoriously difficult for virtuosic performance. The lack of tactile feedbackleads to difficulty learning the repeated muscle movements required for precisecontrol. In comparison, the hands have shown an impressive ability to mastercomplex motor tasks through feel. The hand controller uses both modes ofinteraction. Electro-magnetic field tracking enables 6D hand motion trackingwhile two options provide tactile interaction- a set of tracks that providelinear positioning and applied finger pressure, or a set of trumpet like sliderkeys that provide continuous data describing key depth. Thumbs actuateadditional pressure sensitive buttons. The two haptic interfaces are mountedto a comfortable hand grip that allows a significant range of reach, andpressure to be applied without restricting hand movement highly desirable inexpressive motion.},
address = {Daejeon, Republic of Korea},
author = {Pardue, Laurel and Sebastian, William},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {force sensing resistor,free gesture,hand,interface,new musical instrument,position tracking,tactile feedback},
pages = {90--93},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Hand-Controller for Combined Tactile Control and Motion Tracking}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Funk2005,
abstract = {The central role of the face in social interaction and non-verbal communication suggest we explore facial action as a means of musical expression. This paper presents the design, implementation, and preliminary studies of a novel system utilizing face detection and optic flow algorithms to associate facial movements with sound synthesis in a topographically specific fashion. We report on our experience with various gesture-to-sound mappings and applications, and describe our preliminary experiments at musical performance using the system. },
address = {Vancouver, BC, Canada},
author = {Funk, Mathias and Kuwabara, Kazuhiro and Lyons, Michael J},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Video-based musical interface; gesture-based inter},
pages = {127--131},
title = {{Sonification of Facial Actions for Musical Expression}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}127.pdf},
year = {2005}
}
@inproceedings{Sioros2011,
address = {Oslo, Norway},
author = {Sioros, George and Guedes, Carlos},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {1 calculating the weights,2,automatic music generation,generative,indispensability,max,max4live,metric,msp,stochastic,syncopation},
number = {June},
pages = {88--91},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Automatic Rhythmic Performance in Max/MSP: the kin.rhythmicator}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}088.pdf},
year = {2011}
}
@inproceedings{nime2014:astark,
abstract = {Real-time audio analysis has great potential for being used to create musically responsive applications in live performances. There have been many examples of such use, including sound-responsive visualisations, adaptive audio effects and machine musicianship. However, at present, using audio analysis algorithms in live performance requires either some detailed knowledge about the algorithms themselves, or programming - or both. Those wishing to use audio analysis in live performances may not have either of these as their strengths. Rather, they may instead wish to focus upon systems that respond to audio analysis data, such as visual projections or sound generators. In response, this paper introduces the Sound Analyser - an audio plug-in allowing users to a) select a custom set of audio analyses to be performed in real-time and b) send that information via OSC so that it can easily be used by other systems to develop responsive applications for live performances and installations. A description of the system architecture and audio analysis algorithms implemented in the plug-in is presented before moving on to two case studies where the plug-in has been used in the field with artists.},
address = {London, United Kingdom},
author = {Stark, Adam},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {183--186},
publisher = {Goldsmiths, University of London},
title = {{Sound Analyser: A Plug-In for Real-Time Audio Analysis in Live Performances and Installations}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}348.pdf},
year = {2014}
}
@inproceedings{Vamvakousis:2012a,
address = {Ann Arbor, Michigan},
author = {Vamvakousis, Zacharias},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
publisher = {University of Michigan},
title = {{From the Eyes to the Ears.}},
year = {2012}
}
@inproceedings{Laurenzo2009,
address = {Pittsburgh, PA, United States},
author = {Laurenzo, Tomas and Rodr{\'{i}}guez, Ernesto and Castro, Juan Fabrizio},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {augmented,collaborative music,instruments,interactive music instruments,interfaces,musical sequencer,networked musical,nime09,real-time musical systems,reality,tangible interfaces,visual,visual feedback},
pages = {268--269},
title = {{YARMI : an Augmented Reality Musical Instrument}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}268.pdf},
year = {2009}
}
@inproceedings{Paine:2002,
abstract = {Interactivity has become a major consideration in the development of a contemporary art practice that engages with the proliferation of computer based technologies. Keywords },
address = {Dublin},
author = {Paine, Garth},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {are your choice.},
pages = {143--144},
title = {{GESTATION}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}143.pdf},
year = {2002}
}
@inproceedings{Ojanen2007,
address = {New York City, NY, United States},
author = {Ojanen, Mikko and Suominen, Jari and Kallio, Titti and Lassfolk, Kai},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
doi = {10.1145/1279740.1279756},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {digital electronics,dimi,erkki kurenniemi,nime07,synthesizer},
pages = {88--93},
title = {{Design Principles and User Interfaces of Erkki Kurenniemi's Electronic Musical Instruments of the 1960's and 1970's}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}088.pdf},
year = {2007}
}
@inproceedings{Hashida2007,
address = {New York City, NY, United States},
author = {Hashida, Mitsuyo and Nagata, Noriko and Katayose, Haruhiro},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
number = {1},
pages = {313--316},
title = {{jPop-E : An Assistant System for Performance Rendering of Ensemble Music}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}313.pdf},
year = {2007}
}
@inproceedings{Dannenberg2005,
abstract = {McBlare is a robotic bagpipe player developed by the Robotics Institute at Carnegie Mellon University. McBlare plays a standard set of bagpipes, using a custom air compressor to supply air and electromechanical ``fingers'' to control the chanter. McBlare is MIDI controlled, allowing for simple interfacing to a keyboard, computer, or hardware sequencer. The control mechanism exceeds the measured speed of expert human performers. On the other hand, human performers surpass McBlare in their ability to compensate for limitations and imperfections in reeds, and we discuss future enhancements to address these problems. McBlare has been used to perform traditional bagpipe music as well as experimental computer generated music. },
address = {Vancouver, BC, Canada},
author = {Dannenberg, Roger B and Brown, Ben and Zeglin, Garth and Lupish, Ron},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {MIDI,bagpipes,instrument,music,robot},
pages = {80--84},
title = {{McBlare: A Robotic Bagpipe Player}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}080.pdf},
year = {2005}
}
@inproceedings{Court2007,
address = {New York City, NY, United States},
author = {Dearraindrop, Collective},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {421},
title = {{Miller}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}421.pdf},
year = {2007}
}
@inproceedings{nime2014:ihattwick,
abstract = {Moving new DMIs from the research lab to professional artistic contexts places new demands on both their design and manufacturing. Through a discussion of the Prosthetic Instruments, a family of digital musical instruments we de- signed for use in an interactive dance performance, we dis- cuss four different approaches to manufacturing -- artisanal, building block, rapid prototyping, and industrial. We dis- cuss our use of these different approaches as we strove to reconcile the many conflicting constraints placed upon the instruments' design due to their use as hypothetical prosthetic extensions to dancers' bodies, as aesthetic objects, and as instruments used in a professional touring context. Experiences and lessons learned during the design and manufacturing process are discussed in relation both to these manufacturing approaches as well as to Bill Buxton's concept of artist-spec design.},
address = {London, United Kingdom},
author = {Hattwick, Ian and Malloch, Joseph and Wanderley, Marcelo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {443--448},
publisher = {Goldsmiths, University of London},
title = {{Forming Shapes to Bodies: Design for Manufacturing in the Prosthetic Instruments}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}370.pdf},
year = {2014}
}
@inproceedings{Toyoda2007,
address = {New York City, NY, United States},
author = {Toyoda, Shinichiro},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {composition,gesture,improvisation,interactive systems,nime07},
number = {Fig 2},
pages = {254--255},
title = {{Sensillum : An Improvisational Approach to Composition}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}254.pdf},
year = {2007}
}
@inproceedings{Mitchell2011,
address = {Oslo, Norway},
author = {Mitchell, Thomas and Heap, Imogen},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {data glove,gestural music,imogen heap,live music composition,looping,music controller,neural net-,work},
number = {June},
pages = {465--468},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{SoundGrasp : A Gestural Interface for the Performance of Live Music}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}465.pdf},
year = {2011}
}
@inproceedings{Fyans2009,
address = {Pittsburgh, PA, United States},
author = {Fyans, A Cavan and Gurevich, Michael and Stapleton, Paul},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {design,hci,nime09,performance,skill,transparency},
pages = {171--172},
title = {{Where Did It All Go Wrong ? A Model of Error From the Spectator's Perspective}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}171.pdf},
year = {2009}
}
@inproceedings{Caramiaux2011a,
address = {Oslo, Norway},
author = {Caramiaux, Baptiste and Susini, Patrick and Bianco, Tommaso and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Houix, Olivier and Schnell, Norbert and Misdariis, Nicolas},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {embodiment,environmental sound perception,gesture sound interaction,listening},
number = {June},
pages = {144--148},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Gestural Embodiment of Environmental Sounds : an Experimental Study}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}144.pdf},
year = {2011}
}
@inproceedings{Lemouton2006,
address = {Paris, France},
author = {Lemouton, Serge and Stroppa, Marco and Sluchin, Benny},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {1,augmented instrument,chamber electronics,computer,interaction,musical motivation,performer,trombone},
pages = {304--307},
title = {{Using the Augmented Trombone in "I will not kiss your f.ing flag"}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}304.pdf},
year = {2006}
}
@inproceedings{Poepel2004,
address = {Hamamatsu, Japan},
author = {Poepel, Cornelius},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {controlled sound synthesis,electronic bowed string instrument,human computer interface,instrument design,musical,oscillation,playability},
pages = {150--153},
title = {{Synthesized Strings for String Players}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}150.pdf},
year = {2004}
}
@inproceedings{Hattwick:2012,
abstract = {The configurability and networking abilities of digital musical instruments increases the possibilities for collaboration in musical performances. Computer music ensembles such as laptop orchestras are becoming increasingly common and provide laboratories for the exploration of these possibilities. However, much of the literature regarding the creation of DMIs has been focused on individual expressivity, and their potential for collaborative performance has been under-utilized. This paper makes the case for the benefits of an approach to digital musical instrument design that begins with their collaborative potential, examines several frameworks and sets of principles for the creation of digital musical instruments, and proposes a dimension space representation of collaborative approaches which can be used to evaluate and guide future DMI creation. Several examples of DMIs and compositions are then evaluated and discussed in the context of this dimension space.},
address = {Ann Arbor, Michigan},
author = {Hattwick, Ian and Wanderley, Marcelo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {collaborative,digital music ensemble,digital musical instrument,dimension space,dme,dmi},
publisher = {University of Michigan},
title = {{A Dimension Space for Evaluating Collaborative Musical Performance Systems}},
year = {2012}
}
@inproceedings{Kitani2010,
address = {Sydney, Australia},
author = {Kitani, Kris M and Koike, Hideki},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {algorithmic composition,context-free grammars,grammatical induction,machine improvisation,nime10,stochastic},
number = {Nime},
pages = {469--472},
title = {{ImprovGenerator : Online Grammatical Induction for On-the-Fly Improvisation Accompaniment}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}469.pdf},
year = {2010}
}
@inproceedings{Gao2009,
abstract = {This paper explores a rapidly developed, new musical interface involving a touch-screen, 32 pressure sensitive button pads, infrared sensor, 8 knobs and cross-fader. We provide a versatile platform for computer-based music performance and production using a human computer interface that has strong visual and tactile feedback as well as robust software that exploits the strengths of each individual system component. },
address = {Pittsburgh, PA, United States},
author = {Gao, Mingfei and Hanson, Craig},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {live performance interface,lumi,nime09,pressure},
pages = {58--59},
title = {{LUMI : Live Performance Paradigms Utilizing Software Integrated Touch Screen and Pressure Sensitive Button Matrix}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}058.pdf},
year = {2009}
}
@inproceedings{Blaine2005,
address = {Vancouver, BC, Canada},
author = {Blaine, Tina},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
isbn = {0114122682980},
pages = {27--33},
title = {{The Convergence of Alternate Controllers and Musical Interfaces in Interactive Entertainment}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}027.pdf},
year = {2005}
}
@inproceedings{Everman:2013,
abstract = {Few formal methods exist for evaluating digital musical instruments (DMIs) .This paper proposes a novel method of DMI evaluation using crowd-sourcedtagging. One of the challenges in devising such methods is that the evaluationof a musical instrument is an inherently qualitative task. While previouslyproposed methods have focused on quantitative methods and largely ignored thequalitative aspects of the task, tagging is well-suited to this and is alreadyused to classify things such as websites and musical genres. These, like DMIs,do not lend themselves to simple categorization or parameterization. Using the social tagging method, participating individuals assign descriptivelabels, or tags, to a DMI. A DMI can then be evaluated by analyzing the tagsassociated with it. Metrics can be generated from the tags assigned to theinstrument, and comparisons made to other instruments. This can give thedesigner valuable insight into the where the strengths of the design lie andwhere improvements may be needed. A prototype system for testing the method is proposed in the paper and iscurrently being implemented as part of an ongoing DMI evaluation project. It isexpected that results from the prototype will be available to report by thetime of the conference in May.},
address = {Daejeon, Republic of Korea},
author = {Everman, Michael and Leider, Colby},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Evaluation,digital musical instrument,tagging},
month = {may},
pages = {437--440},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Toward DMI Evaluation Using Crowd-Sourced Tagging Techniques}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Bisig2011,
address = {Oslo, Norway},
author = {Bisig, Daniel and Schacher, Jan C and Neukom, Martin},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {generative art,hybrid ecosystem,immersive,installation,interactive environment,swarm simulation},
number = {June},
pages = {260--263},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Flowspace  A Hybrid Ecosystem}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}260.pdf},
year = {2011}
}
@inproceedings{Miranda2005,
abstract = { Musicians and composers have been using brainwaves as generative sources in music for at least 40 years and the possibility of a brain-computer interface for direct communication and control was first seriously investigated in the early 1970s. Work has been done by many artists and technologists in the intervening years to attempt to control music systems with brainwaves and - indeed - many other biological signals. Despite the richness of EEG, fMRI and other data which can be read from the human brain, there has up to now been only limited success in translating the complex encephalographic data into satisfactory musical results. We are currently pursuing research which we believe will lead to the possibility of direct brain-computer interfaces for rich and expressive musical control. This report will outline the directions of our current research and results. },
address = {Vancouver, BC, Canada},
author = {Miranda, Eduardo and Brouse, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {216--219},
title = {{Toward Direct Brain-Computer Musical Interfaces}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}216.pdf},
year = {2005}
}
@inproceedings{nime2014:rtubb,
abstract = {This paper outlines a theoretical framework for creative technology based on two contrasting processes: divergent exploration and convergent optimisation. We claim that these two cases require different gesture-to-parameter mapping properties. Results are presented from a user experiment that motivates this theory. The experiment was conducted using a publicly available iPad app: ``Sonic Zoom''. Participants were encouraged to conduct an open ended exploration of synthesis timbre using a combination of two different interfaces. The first was a standard interface with ten sliders, hypothesised to be suited to the ``convergent'' stage of creation. The second was a mapping of the entire 10-D combinatorial space to a 2-D surface using a space filling curve. This novel interface was intended to support the ``divergent'' aspect of creativity. The paths of around 250 users through both 2-D and 10-D space were logged and analysed. Both the interaction data and questionnaire results show that the different interfaces tended to be used for different aspects of sound creation, and a combination of these two navigation styles was deemed to be more useful than either individually. The study indicates that the predictable, separate parameters found in most music technology are more appropriate for convergent tasks.},
address = {London, United Kingdom},
author = {Tubb, Robert and Dixon, Simon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {227--232},
publisher = {Goldsmiths, University of London},
title = {{The Divergent Interface: Supporting Creative Exploration of Parameter Spaces}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}415.pdf},
year = {2014}
}
@inproceedings{Corness2008,
address = {Genoa, Italy},
author = {Corness, Greg},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {hci,interactive performance,nime08,perception},
pages = {265--268},
title = {{Performer Model : Towards a Framework for Interactive Performance Based on Perceived Intention}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}265.pdf},
year = {2008}
}
@inproceedings{Roberts:2013a,
abstract = {Web technologies provide an incredible opportunity to present new musicalinterfaces to new audiences. Applications written in JavaScript and designed torun in the browser offer remarkable performance, mobile/desktop portability andlongevity due to standardization. Our research examines the use and potentialof native web technologies for musical expression. We introduce two librariestowards this end: Gibberish.js, a heavily optimized audio DSP library, andInterface.js, a GUI toolkit that works with mouse, touch and motion events.Together these libraries provide a complete system for defining musicalinstruments that can be used in both desktop and mobile browsers. Interface.jsalso enables control of remote synthesis applications by including anapplication that translates the socket protocol used by browsers into both MIDIand OSC messages.},
address = {Daejeon, Republic of Korea},
author = {Roberts, Charles and Wakefield, Graham and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {browser-based NIMEs,javascript,mobile devices,web audio,websockets},
pages = {313--318},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{The Web Browser As Synthesizer And Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Modler2008,
address = {Genoa, Italy},
author = {Modler, Paul and Myatt, Tony},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {nime08},
pages = {358--359},
title = {{Video Based Recognition of Hand Gestures by Neural Networks for the Control of Sound and Music}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}358.pdf},
year = {2008}
}
@inproceedings{nime2014:jfrancoise1,
abstract = {We present a set of probabilistic models that support the design of movement and sound relationships in interactive sonic systems. We focus on a mapping--by--demonstration approach in which the relationships between motion and sound are defined by a machine learning model that learns from a set of user examples. We describe four probabilistic models with complementary characteristics in terms of multimodality and temporality. We illustrate the practical use of each of the four models with a prototype application for sound control built using our Max implementation.},
address = {London, United Kingdom},
author = {Fran{\c{c}}oise, Jules and Schnell, Norbert and Borghesi, Riccardo and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {287--292},
publisher = {Goldsmiths, University of London},
title = {{Probabilistic Models for Designing Motion and Sound Relationships}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}482.pdf},
year = {2014}
}
@inproceedings{Kang2010,
address = {Sydney, Australia},
author = {Kang, Laewoo and Chien, Hsin-Yi},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {nime10},
number = {Nime},
pages = {352--355},
title = {{H{\'{e}} : Calligraphy as a Musical Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}352.pdf},
year = {2010}
}
@inproceedings{Vogt2002,
address = {Dublin, Ireland},
author = {Vogt, Florian and McCaig, Graeme and Ali, Mir Adnan and Fels, Sidney},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {music synthesis,real-time,tongue model,ultrasound},
pages = {181--185},
title = {{Tongue n' Groove: An Ultrasound based Music Controller}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}181.pdf},
year = {2002}
}
@inproceedings{Hindle:2013,
abstract = {Audience participation in computer music has long been limited byresources such as sensor technology or the material goods necessary toshare such an instrument. A recent paradigm is to take advantageof the incredible popularity of the smart-phone, a pocket sizedcomputer, and other mobile devices, to provide the audience aninterface into a computer music instrument. In this paper we discuss amethod of sharing a computer music instrument's interface with anaudience to allow them to interact via their smartphone. We propose amethod that is relatively cross-platform and device-agnostic, yetstill allows for a rich user-interactive experience. By emulating acaptive-portal or hotspot we reduce the adoptability issues and configurationproblems facing performers and their audience. We share ourexperiences with this system, as well as an implementation of thesystem itself.},
address = {Daejeon, Republic of Korea},
author = {Hindle, Abram},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Adoption,Audience Interaction,Captive Portal,Hotspot,Multi-User,Smartphone,Wifi},
month = {may},
pages = {174--179},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{{\{}SW{\}}ARMED: Captive Portals, Mobile Devices, and Audience Participation in Multi-User Music Performance}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Carlson2011,
address = {Oslo, Norway},
author = {Carlson, Chris and Marschner, Eli and Mccurry, Hunter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {arduino,beagleboard,ccrma,force feedback,haptics,jack,linux audio,multi-channel audio,nime,pd,pure data,satellite ccrma,sound spatialization},
pages = {138--139},
title = {{The Sound Flinger : A Haptic Spatializer}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}138.pdf},
year = {2011}
}
@inproceedings{Kapur2002,
address = {Dublin, Ireland},
author = {Kapur, Ajay and Essl, Georg and Davidson, Philip L and Cook, Perry R},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {and the variety of,electronic tabla,graphical feedback,head,indian drum controller,models,physical,strokes by which the,tabla can be},
pages = {108--112},
title = {{The Electronic Tabla Controller}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}108.pdf},
year = {2002}
}
@inproceedings{DArcangelo2004,
address = {Hamamatsu, Japan},
author = {D'Arcangelo, Gideon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {call and response,dj tools,oral culture,turntablism},
pages = {55--58},
title = {{Recycling Music, Answering Back: Toward an Oral Tradition of Electronic Music}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}055.pdf},
year = {2004}
}
@inproceedings{Donald2011,
address = {Oslo, Norway},
author = {Donald, Erika and Duinker, Ben and Britton, Eliot},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {chamber,digital performance,ensemble,instrument identity,live electronics,mapping,music},
number = {June},
pages = {491--494},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Designing the EP Trio: Instrument Identities, Control and Performance Practice in an Electronic Chamber Music Ensemble}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}491.pdf},
year = {2011}
}
@inproceedings{El-Shimy:2013,
abstract = {For a number of years, musicians in different locations have been able toperform with one another over a network as though present on the same stage.However, rather than attempt to re-create an environment for Network MusicPerformance (NMP) that mimics co-present performance as closely as possible, wepropose focusing on providing musicians with additional controls that can helpincrease the level of interaction between them. To this end, we have developeda reactive environment for distributed performance that provides participantsdynamic, real-time control over several aspects of their performance, enablingthem to change volume levels and experience exaggerated stereo panning. Inaddition, our reactive environment reinforces a feeling of a ``shared space''between musicians. It differs most notably from standard ventures into thedesign of novel musical interfaces and installations in its reliance onuser-centric methodologies borrowed from the field of Human-ComputerInteraction (HCI). Not only does this research enable us to closely examine thecommunicative aspects of performance, it also allows us to explore newinterpretations of the network as a performance space. This paper describes themotivation and background behind our project, the work that has been undertakentowards its realization and the future steps that have yet to be explored.},
address = {Daejeon, Republic of Korea},
author = {El-Shimy, Dalia and Cooperstock, Jeremy R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
pages = {158--163},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Reactive Environment for Network Music Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Ferguson2009,
address = {Pittsburgh, PA, United States},
author = {Ferguson, Sam and Beilharz, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Auditory Display.,Interactive Sonification,Sonification},
pages = {35--36},
title = {{An Interface for Live Interactive Sonification}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}035.pdf},
year = {2009}
}
@inproceedings{nime2014:croberts,
abstract = {We describe research enabling the rapid creation of digital musical instruments and their publication to the Internet. This research comprises both high-level abstractions for making continuous mappings between audio, interactive, and graphical elements, as well as a centralized database for storing and accessing instruments. Published instruments run in most devices capable of running a modern web browser. Notation of instrument design is optimized for readability and expressivity.},
address = {London, United Kingdom},
author = {Roberts, Charlie and Wright, Matthew and Kuchera-Morin, JoAnn and H{\"{o}}llerer, Tobias},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {239--242},
publisher = {Goldsmiths, University of London},
title = {{Rapid Creation and Publication of Digital Musical Instruments}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}373.pdf},
year = {2014}
}
@inproceedings{Solis2009,
abstract = {During several decades, the research at Waseda University has been focused on developing anthropomorphic robots capable performing musical instruments. As a result of our research efforts, the Waseda Flutist Robot WF-4RIV and the Waseda Saxophonist Robot WAS-1 have been designed to reproduce the human player performance. As a long-term goal, we are proposing to enable the interaction between musical performance robots as well as with human players. In general the communication of humans within a band is a special case of conventional human social behavior. Rhythm, harmony and timbre of the music played represent the emotional states of the musicians. So the development of an artificial entity that participates in such an interaction may contribute to the better understanding of some of the mechanisms that enable the communication of humans in musical terms. Therefore, we are not considering a musical performance robot (MPR) just as a mere sophisticated MIDI instrument. Instead, its human-like design and the integration of perceptual capabilities may enable to act on its own autonomous initiative based on models which consider its own physical constrains. In this paper, we present an overview of our research approaches towards enabling the interaction between musical performance robots as well as with musicians. },
address = {Pittsburgh, PA, United States},
author = {Solis, Jorge and Ninomiya, Takeshi and Petersen, Klaus and Takeuchi, Masaki and Takanishi, Atsuo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {64--69},
title = {{Anthropomorphic Musical Performance Robots at Waseda University : Increasing Understanding of the Nature of Human Musical Interaction Abstract}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}064.pdf},
year = {2009}
}
@inproceedings{Pardue:2013a,
abstract = {This paper explores the potential of near-field optical reflective sensing formusical instrument gesture capture. Near-field optical sensors are inexpensive,portable and non-intrusive, and their high spatial and temporal resolutionmakes them ideal for tracking the finer motions of instrumental performance.The paper discusses general optical sensor performance with detailedinvestigations of three sensor models. An application is presented to violinbow position tracking using reflective sensors mounted on the stick. Bowtracking remains a difficult task, and many existing solutions are expensive,bulky, or offer limited temporal resolution. Initial results indicate that bowposition and pressure can be derived from optical measurements of thehair-string distance, and that similar techniques may be used to measure bowtilt.},
address = {Daejeon, Republic of Korea},
author = {Pardue, Laurel and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {LED,bow tracking,gesture,near-field sensing,optical sensor,photodiode,phototransistor,reflectance,violin},
month = {may},
pages = {363--368},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Near-Field Optical Reflective Sensing for Bow Tracking}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Vinjar2008,
abstract = {A general CAC1-environment charged with physical-modelling capabilities is described. It combines CommonMusic,ODE and Fluxus in a modular way, making a powerful andflexible environment for experimenting with physical modelsin composition.Composition in this respect refers to the generation andmanipulation of structure typically on or above a note, phrase or voice-level. Compared to efforts in synthesisand performance little work has gone into applying physicalmodels to composition. Potentials in composition-applications are presumably large.The implementation of the physically equipped CAC-environment is described in detail.},
address = {Genoa, Italy},
author = {Vinjar, Anders},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {CommonMusic,Musical mapping,Physical Models in composition},
pages = {335--338},
title = {{Bending Common Music with Physical Models}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}335.pdf},
year = {2008}
}
@inproceedings{Zadel2006,
address = {Paris, France},
author = {Zadel, Mark and Scavone, Gary},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
keywords = {dynamic simulation,freehand input,graphical interfaces,laptop performance,music,software control of computer},
pages = {168--171},
title = {{Different Strokes: a Prototype Software System for Laptop Performance and Improvisation}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}168.pdf},
year = {2006}
}
@inproceedings{Skogstad:2013,
abstract = {In this paper we present some custom designed filters for real-time motion capture applications. Our target application is motion controllers, i.e. systems that interpret hand motion for musical interaction. In earlier research we found effective methods to design nearly optimal filters for realtime applications. However, to be able to design suitable filters for our target application, it is necessary to establish the typical frequency content of the motion capture data we want to filter. This will again allow us to determine a reasonable cutoff frequency for the filters. We have therefore conducted an experiment in which we recorded the hand motion of 20 subjects. The frequency spectra of these data together with a method similar to the residual analysis method were then used to determine reasonable cutoff frequencies. Based on this experiment, we propose three cutoff frequencies for different scenarios and filtering needs: 5, 10 and 15 Hz, which correspond to heavy, medium and light filtering, respectively. Finally, we propose a range of real-time filters applicable to motion controllers. In particular, low-pass filters and low-pass differentiators of degrees one and two, which in our experience are the most useful filters for our target application.},
address = {Daejeon, Korea},
annote = {Edited by:
Woon Seung Yeo Kyogu Lee Alexander Sigman Haru Ji
Graham Wakefield


Proceedings published by
Graduate School of Culture Technology,
Korea Advance Institute of Science and Technology (KAIST) 291 Daehak-ro, Yuseong-gu, Daejeon 305-701,
Republic of Korea
All copyrights remain with the authors.
Websites
www.nime2013.org / nime2013.kaist.ac.kr www.nime.org
Cover design
Samuel Lee
ISSN 2220-4806 (Online) ISSN 2220-4814 (USB)
ISBN 9788989453598 95600},
author = {Skogstad, St{\aa}le A and Nymoen, Kristian and H{\o}vin, Mats and Holm, Sverre and Jensenius, Alexander Refsum},
booktitle = {Proceedings of the International Conference on New Interfaces For Musical Expression},
file = {::},
pages = {196--197},
title = {{Filtering Motion Capture Data for Real-Time Applications}},
url = {http://www.arj.no/wp-content/2013/05/Skogstad{\_}2013.pdf},
year = {2013}
}
@inproceedings{Lympouridis:2012,
abstract = {Through a series of collaborative research projects usingOrient, a wireless, inertial sensor-based motion capture system,I have studied the requirements of musicians, dancers,performers and choreographers and identified various designstrategies for the realization of Whole Body Interactive (WBI)performance systems. The acquired experience and knowledgeled to the design and development of EnActor, prototypeWhole Body Interaction Design software. The software hasbeen realized as a collection of modules that were provedvaluable for the design of interactive performance systems thatare directly controlled by the body.This paper presents EnActor's layout as a blueprint for thedesign and development of more sophisticated descendants.Complete video archive of my research projects in WBIperformance systems at: http://www.inter-axions.com},
address = {Ann Arbor, Michigan},
author = {Lympouridis, Vangelis},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Interaction Design,Interactive Performance Systems,Motion Capture,Software Prototype,Whole Body Interaction},
publisher = {University of Michigan},
title = {{EnActor: A Blueprint for a Whole Body Interaction Design Software Platform}},
year = {2012}
}
@inproceedings{Gillian2009,
address = {Pittsburgh, PA, United States},
author = {Gillian, Nicholas and Knapp, Benjamin and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {gesture recognition,nime09,sarc eyesweb catalog},
pages = {60--61},
title = {{The SARC EyesWeb Catalog : A Pattern Recognition Toolbox for Musician-Computer Interaction}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}060.pdf},
year = {2009}
}
@inproceedings{Janer2005,
address = {Vancouver, BC, Canada},
author = {Janer, Jordi},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {132--135},
title = {{Voice-controlled plucked bass guitar through two synthesis techniques}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}132.pdf},
year = {2005}
}
@inproceedings{Birnbaum2005,
address = {Vancouver, BC, Canada},
author = {Birnbaum, David and Fiebrink, Rebecca and Malloch, Joseph and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Birnbaum et al. - 2005 - Towards a Dimension Space for Musical Devices.pdf:pdf},
keywords = {design space analysis,human-computer interaction,interfaces for musical expression,new},
pages = {192--195},
title = {{Towards a Dimension Space for Musical Devices}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}192.pdf},
year = {2005}
}
@inproceedings{Nagashima2006,
abstract = {This is a studio report of researches and projects in SUAC(Shizuoka University of Art and Culture). SUAC was foundedin April 2000, and organized NIME04 as you know. SUAChas "Faculty of Design" and "Department of Art and Science"and all students study interactive systems and media arts.SUAC has organized Media Art Festival (MAF) from 2001 to2005. Domestic/overseas artists participated in SUAC MAF,and SUAC students' projects also joined and exhibited theirworks in MAF. I will introduce the production cases withinteractive media-installations by SUAC students' projectsfrom the aspect "experiences with novel interfaces ineducation and entertainment" and "reports on students projectsin the framework of NIME related courses".},
address = {Paris, France},
author = {Nagashima, Yoichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Interactive Installation,Media Arts,Sensors,Studio Reports},
pages = {330--333},
title = {{Students' Projects of Interactive Media-installations in SUAC}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}330.pdf},
year = {2006}
}
@inproceedings{Zannos2008,
address = {Genoa, Italy},
author = {Zannos, Iannis},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {nime08},
pages = {261--264},
title = {{Multi-Platform Development of Audiovisual and Kinetic Installations}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}261.pdf},
year = {2008}
}
@inproceedings{Bevilacqua2006,
address = {Paris, France},
author = {Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Rasamimanana, Nicolas and Fl{\'{e}}ty, Emmanuel and Lemouton, Serge and Baschet, Florence},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
pages = {402--406},
title = {{The Augmented Violin Project: Research, Composition and Performance Report}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}402.pdf},
volume = {9},
year = {2006}
}
@inproceedings{Fiebrink2007,
address = {New York City, NY, United States},
author = {Fiebrink, Rebecca and Wang, Ge and Cook, Perry R},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {164--167},
title = {{Don ' t Forget the Laptop : Using Native Input Capabilities for Expressive Musical Control}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}164.pdf},
year = {2007}
}
@inproceedings{Refsum:2012,
abstract = {We report on the Music Ball Project, a longterm, exploratory project focused on creating novel instruments/controllers with a spherical shape as the common denominator. Be- sides a simple and attractive geometrical shape, balls afford many different types of use, including play. This has made our music balls popular among widely different groups of people, from toddlers to seniors, including those that would not otherwise engage with a musical instrument. The pa- per summarises our experience of designing, constructing and using a number of music balls of various sizes and with di{\{}{\O}{\}}erent types of sound-producing elements.},
address = {Ann Arbor, Michigan},
author = {Refsum, Jensenius Alexander and Arve, Voldsund},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {controllers,inexpensive,instruments,music balls},
publisher = {University of Michigan},
title = {{The Music Ball Project: Concept, Design, Development, Performance}},
year = {2012}
}
@inproceedings{Carrillo2010,
abstract = {This paper presents a virtual violin for real-time performances consisting of two modules: a violin spectral modeland a control interface. The interface is composed by asensing bow and a tube with drawn strings in substitutionof a real violin. The spectral model is driven by the bowingcontrols captured with the control interface and it is ableto predict spectral envelopes of the sound corresponding tothose controls. The envelopes are filled with harmonic andnoisy content and given to an additive synthesizer in orderto produce violin sounds. The sensing system is based ontwo motion trackers with 6 degrees of freedom. One trackeris attached to the bow and the other to the tube. Bowingcontrols are computed after a calibration process where theposition of virtual strings and the hair-ribbon of the bowis obtained. A real time implementation was developed asa MAX/MSP patch with external objects for each of themodules.},
address = {Sydney, Australia},
author = {Carrillo, Alfonso P and Bonada, Jordi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {control,spectral,synthesis,violin,virtual},
pages = {229--232},
title = {{The Bowed Tube : a Virtual Violin}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}229.pdf},
year = {2010}
}
@inproceedings{Place:2013,
abstract = {A two-page demonstration paper.Abstract:The AlphaSphere is an electronic musical instrument featuring a series oftactile, pressure sensitive touch pads arranged in a spherical form. It isdesigned to offer a new playing style, while allowing for the expressivereal-time modulation of sound available in electronic-based music. It is alsodesigned to be programmable, enabling the flexibility to map a series ofdifferent notational arrangements to the pad-based interface.The AlphaSphere functions as an HID, MIDI and OSC device, which connects to acomputer and/or independent MIDI device, and its control messages can be mappedthrough the AlphaLive software. Our primary motivations for creating theAlphaSphere are to design an expressive music interface which can exploit thesound palate of synthesizers in a design which allows for the mapping ofnotational arrangements.},
address = {Daejeon, Republic of Korea},
author = {Place, Adam and Lacey, Liam and Mitchell, Thomas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {AlphaSphere,HID,MIDI,open source,polyphonic aftertouch},
month = {may},
pages = {491--492},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{AlphaSphere}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{nime2014:sfavilla,
abstract = {This paper presents new touch-screen collaborative music interaction for people with dementia. The authors argue that dementia technology has yet to focus on collaborative multi-user group musical interactions. The project aims to contribute to dementia care while addressing a significant gap in current literature. Two trials explore contrasting musical scenarios: the performance of abstract electronic music and the distributed performance of J.S. Bach's Goldberg Variations. Findings presented in this paper; demonstrate that people with dementia can successfully perform and engage in collaborative music performance activities with little or no scaffolded instruction. Further findings suggest that people with dementia can develop and retain musical performance skill over time. This paper proposes a number of guidelines and design solutions.},
address = {London, United Kingdom},
author = {Favilla, Stu and Pedell, Sonja},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {35--39},
publisher = {Goldsmiths, University of London},
title = {{Touch Screen Collaborative Music: Designing NIME for Older People with Dementia}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}417.pdf},
year = {2014}
}
@inproceedings{Buch2010,
address = {Sydney, Australia},
author = {Buch, Benjamin and Coussement, Pieter and Schmidt, L{\"{u}}der},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {dynamic mapping,embodiment,finite state au-,human-robot interaction,new media art,nime10,structured,tomata},
number = {June},
pages = {15--18},
title = {{"playing robot" : An Interactive Sound Installation in Human-Robot Interaction Design for New Media Art}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}411.pdf},
year = {2010}
}
@inproceedings{Leeuw:2012a,
abstract = {This position paper likes to stress the role and importance of performance based education in NIME like subjects. It describes the `klankontwerp' learning line at the `school of the arts Utrecht' in its department Music Technology.
Our educational system also reflects the way that we could treat performance in the NIME community as a whole. The importance of performing with our instruments other then in the form of a mere demonstration should get more emphasis.},
address = {Ann Arbor, Michigan},
author = {Leeuw, Hans and Tamminga, Jorrit},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {NIME,education,live electronics,performance,position paper},
publisher = {University of Michigan},
title = {{NIME Education at the HKU, Emphasizing performance}},
year = {2012}
}
@inproceedings{Clay2007,
address = {New York City, NY, United States},
author = {Clay, Arthur and Majoe, Dennis},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::;::},
keywords = {collaborative performance,controller,gps,mobile music,nime07},
pages = {242--245},
title = {{The Wrist-Conductor}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}242.pdf},
year = {2007}
}
@inproceedings{Lee:2013a,
abstract = {We introduce a form of networked music performance where a performer plays amobile music instrument while it is being implemented on the fly by a livecoder. This setup poses a set of challenges in performing a music instrumentwhich changes over time and we suggest design guidelines such as making asmooth transition, varying adoption of change, and sharing information betweenthe pair of two performers. A proof-of-concept instrument is implemented on amobile device using UrMus, applying the suggested guidelines. We wish that thismodel would expand the scope of live coding to the distributed interactivesystem, drawing existing performance ideas of NIMEs.},
address = {Daejeon, Republic of Korea},
author = {Lee, Sang Won and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {live coding,mobile music,network music,on-the-fly instrument},
month = {may},
pages = {493--498},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Live Coding The Mobile Music Instrument}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Stearns2009,
address = {Pittsburgh, PA, United States},
author = {Stearns, Phillip},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {341},
title = {{AANN: Artificial Analog Neural Network}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}341.pdf},
year = {2009}
}
@inproceedings{Bukvic2009,
address = {Pittsburgh, PA, United States},
author = {Bukvic, Ivika and Standley, Eric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {337},
title = {{Elemental {\&} Cyrene Reefs}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}337.pdf},
year = {2009}
}
@inproceedings{nime2014:ynakanishi,
abstract = {In this paper, the authors introduce a stand-alone synthesizer, ``B.O.M.B. -- Beat Of Magic Box --" for electronic music sessions and live performances. ``B.O.M.B." has a wireless communication system that synchronizes musical scale and tempo (BPM) between multiple devices. In addition, participants can change master/slave role between performers immediately. Our primary motivation is to provide musicians and nonmusicians with opportunities to experience a collaborative electronic music performance. Here, the hardware and interaction design of the device is presented. To date, numerous collaborative musical instruments have been developed in electronic music field [1][2][3]. The authors are interested in formations of musical sessions using stand-alone devices and leader/follower relationship in musical sessions. The authors specify three important requirements of instrument design for musical session. They are as follows: (1) Simple Interface: Interface that enables performers to control three sound elements (pitch, timbre, and amplitude) with simple interaction. (2) Portable Stand-alone System: System that runs standalone (with sound generators, speakers, and butteries). Because musical sessions can be improvised at any place and time, the authors consider that portability is essential in designing musical instruments for sessions. (3) Wireless Synchronization: System that supports ensembles by automatically synchronizing tempo (BPM) and tonality between multiple devices by air because of portability. In addition, performers can switch master/slave roles smoothly such as leader/follower relationship during a musical session. The authors gave ten live performances using this device at domestic and international events. In these events, the authors confirmed that our proposed wireless synchronization system worked stable. It is suggested that our system demonstrate the practicality of wireless synchronization. In future, the authors will evaluate the device in terms of its stability in multi-performer musical sessions.},
address = {London, United Kingdom},
author = {Nakanishi, Yoshihito and Matsumura, Seiichiro and Arakawa, Chuichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {80--81},
publisher = {Goldsmiths, University of London},
title = {{B.O.M.B. -- Beat Of Magic Box -: Stand-Alone Synthesizer Using Wireless Synchronization System For Musical Session and Performance}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}327.pdf},
year = {2014}
}
@inproceedings{Muller:2012,
abstract = {This paper discusses the utilization of human skin as a tangible interface for musical expression and collaborative performance. We present an overview of existing different instrument designs that include the skin as the main input. As a further development of a previous exploration [16] we outline the setup and interaction methods of `Skintimacy', an instrument that appropriates the skin for low voltage power transmission in multi-player interaction. Observations deriving from proof-of- concept exploration and performances using the instrument are brought into the reflection and discussion concerning the capabilities and limitations of skin as an input surface.},
address = {Ann Arbor, Michigan},
author = {M{\"{u}}ller, Alexander and Fuchs, Jochen},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Skin-based instruments,collaborative interfaces,embodiment,intimacy,multi-player performance,skin conductivity},
publisher = {University of Michigan},
title = {{The Human Skin as an Interface for Musical Expression}},
year = {2012}
}
@inproceedings{Burtner2002,
address = {Dublin, Ireland},
author = {Burtner, Matthew},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Burtner - 2002 - Noisegate 67 for Metasaxophone Composition and Performance Considerations of a New Computer Music Controller.pdf:pdf},
number = {VCCM},
pages = {24--29},
title = {{Noisegate 67 for Metasaxophone: Composition and Performance Considerations of a New Computer Music Controller}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}024.pdf},
volume = {1842},
year = {2002}
}
@inproceedings{Nymoen:2012a,
abstract = {The paper presents an analysis of the quality of motion data from an iPod Touch (4th gen.). Acceleration and orientation data derived from internal sensors of an iPod is compared to data from a high end optical infrared marker-based motion capture system (Qualisys) in terms of latency, jitter, accuracy and precision. We identify some rotational drift in the iPod, and some time lag between the two systems. Still, the iPod motion data is quite reliable, especially for describing relative motion over a short period of time.},
address = {Ann Arbor, Michigan},
author = {Nymoen, Kristian and Voldsund, Arve and Skogstad, St{\aa}le Andreas van Dorp and Jensenius, Alexander Refsum and T{\o}rresen, Jim},
booktitle = {Proceedings of the International Conference on New Interfaces For Musical Expression},
pages = {88--91},
title = {{Comparing Motion Data from an i{\{}P{\}}od Touch to a High-End Optical Infrared Marker-Based Motion Capture System}},
url = {http://www.duo.uio.no/sok/work.html?WORKID=165865{\&}fid=99230},
year = {2012}
}
@inproceedings{Lui:2013,
abstract = {Music is expressive and hard to be described by words. Learning music istherefore not a straightforward task especially for vocal music such as humanbeatboxing. People usually learn beatboxing in the traditional way of imitatingaudio sample without steps and instructions. Spectrogram contains a lot ofinformation about audio, but it is too complicated to be understood inreal-time. Reinforcement learning is a psychological method, which makes use ofreward and/or punishment as stimulus to train the decision-making process ofhuman. We propose a novel music learning approach based on the reinforcementlearning method, which makes use of compact and easy-to-read spectruminformation as visual clue to assist human beatboxing learning on smartphone.Experimental result shows that the visual information is easy to understand inreal-time, which improves the effectiveness of beatboxing self-learning.},
address = {Daejeon, Republic of Korea},
author = {Lui, Simon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Audio analysis,audio information retrieval.,music learning tool,reinforcement learning,smartphone app},
pages = {25--28},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Compact Spectrum-Assisted Human Beatboxing Reinforcement Learning Tool On Smartphone}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Overholt2005,
address = {Vancouver, BC, Canada},
author = {Overholt, Dan},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {34--37},
title = {{The Overtone Violin}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}034.pdf},
year = {2005}
}
@inproceedings{Lo:2013,
abstract = {Mobile DJ is a music-listening system that allows multiple users to interactand collaboratively contribute to a single song over a social network. Activelistening through a tangible interface facilitates users to manipulate musicaleffects, such as incorporating chords or ``scratching'' the record. Acommunication and interaction server further enables multiple users to connectover the Internet and collaborate and interact through their music. User testsindicate that the device is successful at facilitating user immersion into theactive listening experience, and that users enjoy the added sensory input aswell as the novel way of interacting with the music and each other.},
address = {Daejeon, Republic of Korea},
author = {Lo, Kenneth W K and Lau, Chi Kin and Huang, Michael Xuelin and Tang, Wai Wa and Ngai, Grace and Chan, Stephen C F},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Mobile,interaction design,music,tangible user interface},
pages = {217--222},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Mobile DJ: a Tangible, Mobile Platform for Active and Collaborative Music Listening}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Nymoen2009,
address = {Pittsburgh, PA, United States},
author = {Nymoen, Kristian and Jensenius, Alexander R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {94--97},
title = {{A Discussion of Multidimensional Mapping in Nymophone2}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}094.pdf},
year = {2009}
}
@inproceedings{Caceres2005,
address = {Vancouver, BC, Canada},
author = {C{\'{a}}ceres, Juan Pablo and Mysore, Gautham J and Trevi{\~{n}}o, Jeffrey},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {38--41},
title = {{SCUBA: The Self-Contained Unified Bass Augmenter}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}038.pdf},
year = {2005}
}
@inproceedings{Cartwright2007,
address = {New York City, NY, United States},
author = {Cartwright, Mark and Jones, Matt and Terasawa, Hiroko},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {audience participation,inflatable,instrume nt design,instrume nt size,mapping,musical,new musical instrument,nime07,physical systems,sound scultpure},
pages = {224--227},
title = {{Rage in Conjunction with the Machine}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}224.pdf},
year = {2007}
}
@inproceedings{Essl2010,
abstract = {We discuss how the environment urMus was designed to allow creation of mobile musical instruments on multi-touch smartphones. The design of a mobile musical instrument consists of connecting sensory capabilities to output modalities through various means of processing. We describe how the default mapping interface was designed which allows to set up such a pipeline and how visual and interactive multi-touch UIs for musical instruments can be designed within the system. },
address = {Sydney, Australia},
author = {Essl, Georg and M{\"{u}}ller, Alexander},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Mobile music making,design,mapping,meta-environment,user interface},
pages = {76--81},
title = {{Designing Mobile Musical Instruments and Environments with urMus}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}076.pdf},
year = {2010}
}
@inproceedings{Camurri2008,
address = {Genoa, Italy},
author = {Camurri, Antonio and Canepa, Corrado and Coletta, Paolo and Mazzarino, Barbara and Volpe, Gualtiero},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {active listening of music,and performing arts applications,expressive interfaces,full-body,gesture processing,interactive systems for music,motion analysis and expressive,multimodal,nime08},
pages = {134--139},
title = {{Mappe per Affetti Erranti : a Multimodal System for Social Active Listening and Expressive Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}134.pdf},
year = {2008}
}
@inproceedings{Burns2006,
address = {Paris, France},
author = {Burns, Anne-Marie and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {finger-tracking,gesture,guitar fingering,hough transform},
pages = {196--199},
title = {{Visual Methods for the Retrieval of Guitarist Fingering}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}196.pdf},
year = {2006}
}
@inproceedings{Taylor2010a,
address = {Sydney, Australia},
author = {Taylor, Stuart and Hook, Jonathan},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {ferrofluid,ferromagnetic sensing,midi controller,nime10,reconfigurable user interface,wave terrain synthesis},
number = {Nime},
pages = {463--466},
title = {{FerroSynth : A Ferromagnetic Music Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}463.pdf},
year = {2010}
}
@inproceedings{Humphrey2009,
address = {Pittsburgh, PA, United States},
author = {Humphrey, Eric and Leider, Colby},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {musical,nime09},
pages = {31--32},
title = {{The Navi Activity Monitor : Toward Using Kinematic Data to Humanize Computer Music}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}031.pdf},
year = {2009}
}
@inproceedings{DArcangelo2001,
address = {Seattle, WA, United States},
author = {D'Arcangelo, Gideon},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
file = {::},
pages = {42--45},
title = {{Creating Contexts of Creativity : Musical Composition with Modular Components}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}042.pdf},
year = {2001}
}
@inproceedings{Loewenstein2007,
address = {New York City, NY, United States},
author = {Loewenstein, Stefan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {405--406},
title = {{"Acoustic Map" -- An Interactive Cityportrait}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}405.pdf},
year = {2007}
}
@inproceedings{Levin2005,
address = {Vancouver, BC, Canada},
author = {Levin, Golan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {2--3},
title = {{A Personal Chronology of Audiovisual Systems Research}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}002.pdf},
year = {2005}
}
@inproceedings{Bullock2009,
address = {Pittsburgh, PA, United States},
author = {Bullock, Jamie and Coccioli, Lamberto},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {266--267},
title = {{Towards a Humane Graphical User Interface for Live Electronic Music}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}266.pdf},
year = {2009}
}
@inproceedings{Zaborowski:2004,
abstract = {This paper describes ThumbTEC, a novel general purposeinput device for the thumb or finger that is useful in a widevariety of applications from music to text entry. The device i smade up of three switches in a row and one miniature joystickon top of the middle switch. The combination of joystickdirection and switch(es) controls what note or alphanumericcharacter is selected by the finger. Several applications aredetailed.},
address = {Hamamatsu, Japan},
author = {Zaborowski, Philippe S},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {HCI,Handheld Devices,Isometric Joystick,Mobile Computing,Musical Instrument.,One-Thumb Input Device},
pages = {112--115},
title = {{ThumbTec: A New Handheld Input Device}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}112.pdf},
year = {2004}
}
@inproceedings{Fabiani2011,
address = {Oslo, Norway},
author = {Fabiani, Marco and Dubus, Ga{\"{e}}l and Bresin, Roberto},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {collaborative performance,expressive performance,gesture},
number = {June},
pages = {116--119},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{MoodifierLive : Interactive and Collaborative Expressive Music Performance on Mobile Devices}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}116.pdf},
year = {2011}
}
@inproceedings{Hauert2007,
address = {New York City, NY, United States},
author = {Hauert, Sibylle and Reichmuth, Daniel and B{\"{o}}hm, Volker},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {422},
title = {{Instant City, a Music Building Game Table}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}422.pdf},
year = {2007}
}
@inproceedings{Takegawa2008,
abstract = {Musical keyboard instruments have a long history, whichresulted in many kinds of keyboards (claviers) today. Sincethe hardware of conventional musical keyboards cannot bechanged, such as the number of keys, musicians have tocarry these large keyboards for playing music that requiresonly a small diapason. To solve this problem, the goal ofour study is to construct UnitKeyboard, which has only 12keys (7 white keys and 5 black keys) and connectors fordocking with other UnitKeyboards. We can build variouskinds of musical keyboard configurations by connecting oneUnitKeyboard to others, since they have automatic settingsfor multiple keyboard instruments. We discuss the usabilityof the UnitKeyboard from reviews by several amateur andprofessional pianists who used the UnitKeyboard.},
address = {Genoa, Italy},
author = {Takegawa, Yoshinari and Tsukamoto, Masahiko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Automatic settings,Portable keyboard instruments,block interface},
pages = {289--292},
title = {{UnitKeyboard : An Easily Configurable Compact Clavier}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}289.pdf},
year = {2008}
}
@inproceedings{Franinovic2011,
address = {Oslo, Norway},
author = {Franinovic, Karmen},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {exploration,gesture,habit,sonic interaction design},
pages = {448--452},
title = {{The Flo)(ps : Negotiating Between Habitual and Explorative Gestures}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}448.pdf},
year = {2011}
}
@inproceedings{Liang2011,
address = {Oslo, Norway},
author = {Liang, Dawen and Xia, Guangyu and Dannenberg, Roger B},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {automatic accompaniment,interactive,music display,popular music,real-time,synchronization},
number = {June},
pages = {167--172},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{A Framework for Coordination and Synchronization of Media}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}167.pdf},
year = {2011}
}
@inproceedings{Nagashima2003,
address = {Montreal, QC, Canada},
author = {Nagashima, Yoichi},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {48--53},
title = {{Bio-Sensing Systems and Bio-Feedback Systems for Interactive Media Arts}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}048.pdf},
year = {2003}
}
@inproceedings{Schacher2007,
abstract = {This paper presents a methodology and a set of tools for gesture control of sources in 3D surround sound. The techniques for rendering acoustic events on multi-speaker or headphone-based surround systems have evolved considerably, making it possible to use them in real-time performances on light equipment. Controlling the placement of sound sources is usually done in idiosyncratic ways and has not yet been fully explored and formalized. This issue is addressed here with the proposition of a methodical approach. The mapping of gestures to source motion is implemented by giving the sources physical object properties and manipulating these characteristics with standard geometrical transforms through hierarchical or emergent relationships. },
address = {New York City, NY, United States},
author = {Schacher, Jan C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Emergent Structures.,Gesture,Mapping,Surround Sound,Trajectory,Transform Matrix,Tree Hierarchy},
pages = {358--362},
title = {{Gesture Control of Sounds in {\{}3D{\}} Space}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}358.pdf},
year = {2007}
}
@inproceedings{Bozzolan2008,
abstract = {The continuous evolutions in the human-computer interfaces field have allowed the development of control devicesthat let have a more and more intuitive, gestural and noninvasive interaction.Such devices find a natural employment also in the musicapplied informatics and in particular in the electronic music,always searching for new expressive means.This paper presents a prototype of a system for the realtime control of sound spatialization in a multichannel configuration with a multimodal interaction interface. The spatializer, called SMuSIM, employs interaction devices thatrange from the simple and well-established mouse and keyboard to a classical gaming used joystick (gamepad), finallyexploiting more advanced and innovative typologies basedon image analysis (as a webcam).},
address = {Genoa, Italy},
author = {Bozzolan, Matteo and Cospito, Giovanni},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {EyesWeb,Pure data.,Sound spatialization,interaction interfaces,multimodal interaction},
pages = {24--27},
title = {{SMuSIM : a Prototype of Multichannel Spatialization System with Multimodal Interaction Interface}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}024.pdf},
year = {2008}
}
@inproceedings{Meier2010,
address = {Sydney, Australia},
author = {Meier, Max and Schranner, Max},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {algorithmic composition,nime10,soft constraints,tangible interaction},
pages = {501--504},
title = {{The Planets}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}501.pdf},
year = {2010}
}
@inproceedings{nime2014:mdavies,
abstract = {In this paper we present Improvasher a real-time musical accompaniment system which creates an automatic mashup to accompany live musical input. Improvasher is built around two music processing modules, the first, a performance following technique, makes beat-synchronous predictions of chroma features from a live musical input. The second, a music mashup system, determines the compatibility between beat-synchronous chromagrams from different pieces of music. Through the combination of these two techniques, a real-time time predict mashup can be generated towards a new form of automatic accompaniment for interactive musical performance.},
address = {London, United Kingdom},
author = {Davies, Matthew and Stark, Adam and Gouyon, Fabien and Goto, Masataka},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {541--544},
publisher = {Goldsmiths, University of London},
title = {{Improvasher: A Real-Time Mashup System for Live Musical Input}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}405.pdf},
year = {2014}
}
@inproceedings{Bell2007,
address = {New York City, NY, United States},
author = {Bell, Bo and Kleban, Jim and Overholt, Dan and Putnam, Lance and Thompson, John and Morin-Kuchera, JoAnn},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {computer vision,e-field sensing,interactivity,multimodal,nime07},
pages = {62--65},
title = {{The Multimodal Music Stand}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}062.pdf},
year = {2007}
}
@inproceedings{Nash:2012,
abstract = {This paper presents concepts, models, and empirical findings relating to liveness and flow in the user experience of systems mediated by notation. Results from an extensive two-year field study of over 1,000 sequencer and tracker users, combining interaction logging, user surveys, and a video study, are used to illustrate the properties of notations and interfaces that facilitate greater immersion in musical activities and domains, borrowing concepts from programming to illustrate the role of visual and musical feedback, from the notation and domain respectively. The Cognitive Dimensions of Notations framework and Csikszentmihalyi's flow theory are combined to demonstrate how non-realtime, notation-mediated interaction can support focused, immersive, energetic, and intrinsically-rewarding musical experiences, and to what extent they are supported in the interfaces of music production software. Users are shown to maintain liveness through a rapid, iterative edit-audition cycle that integrates audio and visual feedback.},
address = {Ann Arbor, Michigan},
author = {Nash, Chris and Blackwell, Alan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {DAWs,composition,feedback,flow,liveness,notation,performance,programming,sequencers,soundtracking,user studies},
month = {may},
publisher = {University of Michigan},
title = {{Liveness and Flow in Notation Use}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}217.pdf},
year = {2012}
}
@inproceedings{Burtner:2004,
abstract = {This paper describes a theory for modulated objects based onobservations of recent musical interface design trends. Thetheory implies extensions to an object-based approach tocontroller design. Combining NIME research withethnographic study of shamanic traditions. The ,
,
authordiscusses the creation of new controllers based on theshamanic use of ritual objects.},
address = {Hamamatsu, Japan},
author = {Burtner, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {Ecoacoustics,Multimedia,Music Composition,Music and Video Controllers,Mythology,New Interface Design,Shamanism},
pages = {193--196},
title = {{A Theory of Modulated Objects for New Shamanic Controller Design}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}193.pdf},
year = {2004}
}
@inproceedings{Vallis2010,
address = {Sydney, Australia},
author = {Vallis, Owen and Hochenbaum, Jordan and Kapur, Ajay},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {arduino,arduinome,iterative design,monome,nime10},
number = {Nime},
pages = {1--6},
title = {{A Shift Towards Iterative and Open-Source Design for Musical Interfaces}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}001.pdf},
year = {2010}
}
@inproceedings{Schacher2008,
abstract = {Moving out of doors with digital tools and electronic music and creating musically rich experiences is made possible by the increased availability of ever smaller and more powerful mobile computers. Composing music for and in a landscape instead of for a closed architectural space offers new perspectives but also raises questions about interaction and composition of electronic music. The work we present here was commissioned by a festival and ran on a daily basis over a period of three months. A GPS-enabled embedded Linux system is assembled to serve as a location-aware sound platform. Several challenges have to be overcome both technically and artistically to achieve a seamless experience and provide a simple device to be handed to the public. By building this interactive experience, which relies as much on the user's willingness to explore the invisible sonic landscape as on the ability to deploy the technology, a number of new avenues for exploring electronic music and interactivity in location-based media open up. New ways of composing music for and in a landscape and for creating audience interaction are explored. },
address = {Genoa, Italy},
author = {Schacher, Jan C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {GPS,Location-based,Pure Data,composition,electronic music,embedded Linux,interaction,mapping,soundscape},
pages = {168--171},
title = {{Davos Soundscape, a Location Based Interactive Composition}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}168.pdf},
year = {2008}
}
@inproceedings{Mullen2011,
address = {Oslo, Norway},
author = {Mullen, Tim and Warp, Richard and Jansch, Adam},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {arduino,brain-computer music interface,eeg,internet},
number = {June},
pages = {469--472},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Minding the (Transatlantic) Gap: An Internet-Enabled Acoustic Brain-Computer Music Interface}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}469.pdf},
year = {2011}
}
@inproceedings{Ward2008,
abstract = {This paper presents a comparison of the movement styles of two theremin players based on observation and analysis of video recordings. The premise behind this research is that a consideration of musicians' movements could form the basis for a new framework for the design of new instruments. Laban Movement Analysis is used to qualitatively analyse the movement styles of the musicians and to argue that the Recuperation phase of their phrasing is essential to achieve satisfactory performance.},
address = {Genoa, Italy},
author = {Ward, Nicholas and Penfield, Kedzie and O'Modhrain, Sile and Knapp, Benjamin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Effort Phrasing,Laban Movement Analysis,Recuperation,Theremin},
pages = {117--121},
title = {{A Study of Two Thereminists : Towards Movement Informed Instrument Design}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}117.pdf},
year = {2008}
}
@inproceedings{Thiebaut2008,
abstract = {This research focuses on real-time gesture learning and recognition. Events arrive in a continuous stream without explicitly given boundaries. To obtain temporal accuracy, weneed to consider the lag between the detection of an eventand any effects we wish to trigger with it. Two methodsfor real time gesture recognition using a Nintendo Wii controller are presented. The first detects gestures similar to agiven template using either a Euclidean distance or a cosinesimilarity measure. The second method uses novel information theoretic methods to detect and categorize gestures inan unsupervised way. The role of supervision, detection lagand the importance of haptic feedback are discussed.},
address = {Genoa},
author = {Thiebaut, Jean-Baptiste and Abdallah, Samer and Robertson, Andrew and Bryan-Kinns, Nick and Plumbley, Mark D},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Gesture recognition,HMMs,haptic feedback,information dynamics,interaction,supervised and unsupervised learning},
pages = {215--218},
title = {{Real Time Gesture Learning and Recognition : Towards Automatic Categorization}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}215.pdf},
year = {2008}
}
@inproceedings{Nymoen2010,
address = {Sydney, Australia},
author = {Nymoen, Kristian and Glette, Kyrre and Skogstad, St{\aa}le A and Torresen, Jim and Jensenius, Alexander R},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {nime10},
number = {Nime},
pages = {259--262},
title = {{Searching for Cross-Individual Relationships between Sound and Movement Features using an SVM Classifier}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}259.pdf},
year = {2010}
}
@inproceedings{Carlson:2012,
abstract = {Borderlands is a new interface for composing and performing with granular synthesis. The software enables flexible, realtime improvisation and is designed to allow users to engage with sonic material on a fundamental level, breaking free of traditional paradigms for interaction with this technique. The user is envisioned as an organizer of sound, simultaneously assuming the roles of curator, performer, and listener. This paper places the software within the context of painterly interfaces and describes the user interaction design and synthesis methodology.},
address = {Ann Arbor, Michigan},
author = {Carlson, Chris and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {CCRMA,Granular synthesis,NIME,improvisation,organized sound,painterly interfaces},
publisher = {University of Michigan},
title = {{Borderlands - An Audiovisual Interface for Granular Synthesis}},
year = {2012}
}
@inproceedings{Rodet2005,
address = {Vancouver, BC, Canada},
author = {Rodet, Xavier and Lambert, Jean-Philippe and Cahen, Roland and Gaudy, Thomas and Guedy, Fabrice and Gosselin, Florian and Mobuchon, Pascal},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {109--114},
title = {{Study of haptic and visual interaction for sound and music control in the Phase project}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}109.pdf},
year = {2005}
}
@inproceedings{nime2014:seloul,
abstract = {We use Max/MSP framework to create a reliable but flexible approach for managing live performances of music bands who rely on live playing with digital music. This approach utilizes Max/MSP to allow any player an easy and low cost way to apply and experiment innovative music interfaces for live performance, without losing the professionalism required on stage. In that approach, every 1-3 players is plugged to a unit consisting of a standard sound-card and laptop. This unit is controlled by an interface that schedules and manages all the digital sounds made by each player (VST effects, VST instruments and 'home-made' interactive interfaces). All the player's units are then remotely controlled by a conduc- tor patch which is in charge of the synchronization of all the players and background samples in real time, as well as pro- viding sensitive metronome and scheduling visual enhance- ment. Moreover, and not less important, we can take the advantage of using virtual instruments and virtual effects in Max environment to manage the mix, and routing the au- dio. This providing monitors and metronome to the players ears, and virtual mixing via Max/MSP patch. This privi- lege almost eliminates the dependency in the venue's equipment, and in that way, the sound quality and music ideas can be taken out directly from the studio to the stage.},
address = {London, United Kingdom},
author = {Amo, Yehiel and Zissu, Gil and Eloul, Shaltiel and Shlomi, Eran and Schukin, Dima and Kalifa, Almog},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {94--97},
publisher = {Goldsmiths, University of London},
title = {{A Max/MSP Approach for Incorporating Digital Music via Laptops in Live Performances of Music Bands}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}340.pdf},
year = {2014}
}
@inproceedings{Dimitrov2010,
address = {Sydney, Australia},
author = {Dimitrov, Smilen},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {dc,isa,nime10,sensors,soundcard},
number = {Nime},
pages = {303--308},
title = {{Extending the Soundcard for Use with Generic DC Sensors Demonstrated by Revisiting a Vintage ISA Design}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}303.pdf},
year = {2010}
}
@inproceedings{Trento:2013,
abstract = {This paper describes the development of a prototype of a sonic toy forpre-scholar kids. The device, which is a mod- ified version of a footballratchet, is based on the spinning gesture and it allows to experience fourdifferent types of auditory feedback. These algorithms let a kid play withmusic rhythm, generate a continuous sound feedback and control the pitch of apiece of music. An evaluation test of the device has been performed withfourteen kids in a kindergarten. Results and observations showed that kidspreferred the algorithms based on the exploration of the music rhythm and onpitch shifting.},
address = {Daejeon, Republic of Korea},
author = {Trento, Stefano and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Sonic toy,auditory feedback.,children},
pages = {456--459},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Flag beat: a novel interface for rhythmic musical expression for kids}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Sa2007,
address = {New York City, NY, United States},
author = {Sa, Adriana},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {428},
title = {{Thresholds}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}428.pdf},
year = {2007}
}
@inproceedings{Hadjakos2010,
address = {Sydney, Australia},
author = {Hadjakos, Aristotelis and M{\"{u}}hlh{\"{a}}user, Max},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {nime10},
number = {Nime},
pages = {335--338},
title = {{Analysis of Piano Playing Movements Spanning Multiple Touches}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}335.pdf},
year = {2010}
}
@inproceedings{Freed:2013,
abstract = {We describe ``o.expr'' an expression language for dynamic, object- andagent-oriented computation of gesture signal processing workflows using OSCbundles. We illustrate the use of o.expr for a range of gesture processingtasks showing how stateless programming and homoiconicity simplify applicationsdevelopment and provide support for heterogeneous computational networks.},
address = {Daejeon, Republic of Korea},
author = {Freed, Adrian and MacCallum, John and Wessel, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Functional Programming,Gesture Signal Processing,Homoiconicity,Open Sound Control,Process Migration.},
pages = {347--351},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Agile Interface Development using OSC Expressions and Process Migration}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{doverholt:2014,
abstract = {We introduce and describe the initial evaluation of a new low-cost augmented violin prototype, with research focused on the user experience when playing such hybrid physical-digital instruments, and the exploration of novel interactive performance techniques. Another goal of this work is wider platform accessibility for players, via a simple `do-it-yourself' approach described by the design herein. While the hardware and software elements are open source, the build process can nonetheless require non-insignificant investments of time and money, as well as basic electronics construction skills. These have been kept to a minimum wherever possible. Our initial prototype is based upon an inexpensive electric violin that is widely available online for approximately 200 USD. This serves as the starting point for construction, to which the design adds local Digital Signal Processing (DSP), gestural sensing, and sound output. Real-time DSP algorithms are running on a mobile device, which also incorporates orientation/gesture sensors for parameter mapping, with the resulting sound amplified and rendered via small loudspeakers mounted on the instrument. The platform combines all necessary elements for digitally-mediated interactive performance; the need for a traditional computer only arises when developing new DSP algorithms for the platform. An initial exploratory evaluation with users is presented, in which performers explore different possibilities with the proposed platform (various DSP implementations, mapping schemes, physical setups, etc.) in order to better establish the needs of the performing artist. Based on these results, future work is outlined leading towards the development of a complete quartet of instruments.},
address = {London, United Kingdom},
author = {Overholt, Dan and Gelineck, Steven},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {122--125},
publisher = {Goldsmiths, University of London},
title = {{Design {\&} Evaluation of an Accessible Hybrid Violin Platform}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}470.pdf},
year = {2014}
}
@inproceedings{Argo2004,
address = {Hamamatsu, Japan},
author = {Argo, Mark},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {control,controller,human computer interaction,live performance,open sound,sample manipulation},
pages = {189--192},
title = {{The Slidepipe: A Timeline-Based Controller for Real-Time Sample Manipulation}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}189.pdf},
year = {2004}
}
@inproceedings{Johnston2011,
address = {Oslo, Norway},
author = {Johnston, Andrew},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {evaluation,human-computer in-,practice-based research,research methods,teraction,user studies},
number = {June},
pages = {280--283},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Beyond Evaluation : Linking Practice and Theory in New Musical Interface Design}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}280.pdf},
year = {2011}
}
@inproceedings{Wulfson2007,
abstract = {This article presents various custom software tools called Automatic Notation Generators (ANG's) developed by the authors to aid in the creation of algorithmic instrumental compositions. The unique possibilities afforded by ANG software are described, along with relevant examples of their compositional output. These avenues of exploration include: mappings of spectral data directly into notated music, the creation of software transcribers that enable users to generate multiple realizations of algorithmic compositions, and new types of spontaneous performance with live generated screen-based music notation. The authors present their existing software tools along with suggestions for future research and artistic inquiry.},
address = {New York City, NY, United States},
author = {Wulfson, Harris and Barrett, G Douglas and Winter, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {346--351},
title = {{Automatic Notation Generators}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}346.pdf},
year = {2007}
}
@inproceedings{nime2014:tresch,
abstract = {This paper proposes a simple architecture for creating (indoor) audio walks by using a server running Max/MSP together with the external object fhnw.audiowalk.state and smartphone clients running either under Android or iOS using LibPd. Server and smartphone clients communicate over WLAN by exchanging OSC messages. Server and client have been designed in a way that allows artists with only little programming skills to create position-based audio walks.},
address = {London, United Kingdom},
author = {Resch, Thomas and Krebs, Matthias},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {269--272},
publisher = {Goldsmiths, University of London},
title = {{A Simple Architecture for Server-based (Indoor) Audio Walks}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}491.pdf},
year = {2014}
}
@inproceedings{Smallwood2009a,
address = {Pittsburgh, PA, United States},
author = {Smallwood, Scott and Cook, Perry R and Trueman, Dan and McIntyre, Lawrence},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {display systems,hemispherical speakers,laptop orchestras,loudspeakers,nime09,sonic},
pages = {110--115},
title = {{Don ' t Forget the Loudspeaker  A History of Hemispherical Speakers at Princeton , Plus a DIY Guide}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}110.pdf},
year = {2009}
}
@inproceedings{Gunther2002,
address = {Dublin, Ireland},
author = {Gunther, Eric and Davenport, Glorianna and O'Modhrain, Sile},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {multi-modal,music,tactile composition,vibrotactile},
pages = {73--79},
title = {{Cutaneous Grooves: Composing for the Sense of Touch}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}073.pdf},
year = {2002}
}
@inproceedings{Britt:2012,
abstract = {The EMvibe is an augmented vibraphone that allows for continuous control over the amplitude and spectrum of in- dividual notes. The system uses electromagnetic actuators to induce vibrations in the vibraphone's aluminum tone bars. The tone bars and the electromagnetic actuators are coupled via neodymium magnets affixed to each bar. The acoustic properties of the vibraphone allowed us to develop a very simple, low-cost and powerful amplification solution that requires no heat sinking. The physical design is meant to be portable and robust, and the system can be easily installed on any vibraphone without interfering with normal performance techniques. The system supports multiple in- terfacing solutions, affording the performer and composer the ability to interact with the EMvibe in different ways depending on the musical context.},
address = {Ann Arbor, Michigan},
author = {Britt, N Cameron and Snyder, Jeff and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Vibraphone,augmented instrument,electromagnetic actuation},
publisher = {University of Michigan},
title = {{The EMvibe: An Electromagnetically Actuated Vibraphone}},
year = {2012}
}
@inproceedings{Hashida:2004,
abstract = {This paper proposes an interface for improvisational ensemble plays which synthesizes musical sounds and graphical images on the floor from people's act of "walking." The aim of this paper is to develop such a system that enables nonprofessional people in our public spaces to play good contrapuntal music without any knowledge of music theory. The people are just walking. This system is based on the i-trace system [1] which can capture the people's behavior and give some visual feedback. },
address = {Hamamatsu, Japan},
author = {Hashida, Tomoko and Kakehi, Yasuaki and Naemura, Takeshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {Contrapuntal Music,Human Tracking,Improvisational Ensemble Play,Spatially Augmented Reality,Traces},
pages = {215--216},
title = {{Ensemble System with i-trace}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}215.pdf},
year = {2004}
}
@inproceedings{Jorda2005,
address = {Vancouver, BC, Canada},
author = {Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
keywords = {collaborative music,multi-user instruments,new instruments},
pages = {23--26},
title = {{Multi-user Instruments: Models, Examples and Promises}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}023.pdf},
year = {2005}
}
@inproceedings{Hamano:2013,
abstract = {Electroencephalography (EEG) has been used to generate music for over 40 years,but the most recent developments in brain--computer interfaces (BCI) allowgreater control and more flexible expression for using new musical instrumentswith EEG. We developed a real-time musical performance system using BCItechnology and sonification techniques to generate imagined musical chords withorganically fluctuating timbre. We aim to emulate the expressivity oftraditional acoustic instruments. The BCI part of the system extracts patternsfrom the neural activity while a performer imagines a score of music. Thesonification part of the system captures non-stationary changes in the brainwaves and reflects them in the timbre by additive synthesis. In this paper, wediscuss the conceptual design, system development, and the performance of thisinstrument.},
address = {Daejeon, Republic of Korea},
author = {Hamano, Takayuki and Rutkowski, Tomasz and Terasawa, Hiroko and Okanoya, Kazuo and Furukawa, Kiyoshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Brain-computer interface (BCI),classification,qualitative and quantitative information,sonification},
pages = {49--54},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Generating an Integrated Musical Expression with a Brain--Computer Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Sosnick2011,
address = {Oslo, Norway},
author = {Sosnick, Marc and Hsu, William},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {cuda,finite difference,gpu,synthesis},
number = {June},
pages = {264--267},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Implementing a Finite Difference-Based Real-time Sound Synthesizer using GPUs}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}264.pdf},
year = {2011}
}
@inproceedings{Kaneko:2013,
abstract = {In this paper, a function-oriented musical interface, named the sound wheel{\_}x0011{\_},is presented. This interface is designed to manipulate musical functions likepitch class sets, tonal centers and scale degrees, rather than the {\_}x0010{\_}musicalsurface{\_}x0011{\_}, i.e. the individual notes with concrete note heights. The sound wheelhas an interface summarizing harmony theory, and the playing actions haveexplicit correspondencewith musical functions. Easy usability is realized by semi-automatizing theconversion process from musical functions into the musical surface. Thus, theplayer can use this interface with concentration on the harmonic structure,without having his attention caught by manipulating the musical surface.Subjective evaluation indicated the e{\_}x001B{\_}ffectiveness of this interface as a toolhelpful for understanding the music theory. Because of such features, thisinterface can be used for education and interactive training of tonal musictheory.},
address = {Daejeon, Republic of Korea},
author = {Kaneko, Shoken},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Interactive tonal music generation,Music education},
pages = {202--205},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Function-Oriented Interface for Music Education and Musical Expressions: ``the Sound Wheel''}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Caramiaux:2013,
abstract = {We present an overview of machine learning (ML) techniques and theirapplication in interactive music and new digital instruments design. We firstgive to the non-specialist reader an introduction to two ML tasks,classification and regression, that are particularly relevant for gesturalinteraction. We then present a review of the literature in current NIMEresearch that uses ML in musical gesture analysis and gestural sound control.We describe the ways in which machine learning is useful for creatingexpressive musical interaction, and in turn why live music performance presentsa pertinent and challenging use case for machine learning.},
address = {Daejeon, Republic of Korea},
author = {Caramiaux, Baptiste and Tanaka, Atau},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Analysis,Control,Data mining,Gesture,Machine Learning,Musical Expression,Musical Gestures,Sound},
pages = {513--518},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Machine Learning of Musical Gestures}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Macrae2008,
address = {Genoa, Italy},
author = {Macrae, Robert and Dixon, Simon},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {nime08},
pages = {364--365},
title = {{From Toy to Tutor : Note-Scroller is a Game to Teach Music}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}364.pdf},
year = {2008}
}
@inproceedings{Reus2011,
address = {Oslo, Norway},
author = {Reus, Jonathan},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {cracklebox,dynamic mapping,interaction,interface topology,iphone,mobile music,paradigm,touchscreen},
number = {June},
pages = {377--380},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Crackle: A Dynamic Mobile Multitouch Topology for Exploratory Sound Interaction}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}377.pdf},
year = {2011}
}
@inproceedings{nime2014:aberndt,
abstract = {We present the digital musical instrument TouchNoise that is based on multitouch interaction with a particle system. It implements a novel interface concept for modulating noise spectra. Each particle represents a sine oscillator that moves through the two-dimensional frequency and stereo panning domain via Brownian motion. Its behavior can be affected by multitouch gestures allowing the shaping of the resulting sound in many different ways. Particles can be dragged, attracted, repelled, accentuated, and their autonomous behavior can be manipulated. In this paper we introduce the concepts behind this instrument, describe its implementation and discuss the sonic design space emerging from it.},
address = {London, United Kingdom},
author = {Berndt, Axel and Al-Kassab, Nadia and Dachselt, Raimund},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {323--326},
publisher = {Goldsmiths, University of London},
title = {{TouchNoise: A Particle-based Multitouch Noise Modulation Interface}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}325.pdf},
year = {2014}
}
@inproceedings{Baath2011,
address = {Oslo, Norway},
author = {B{\aa}{\aa}th, Rasmus and Strandberg, Thomas and Balkenius, Christian},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {eye,eye tracking,rhythm,sensorimotor synchronization},
number = {June},
pages = {441--444},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Eye Tapping : How to Beat Out an Accurate Rhythm using Eye Movements}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}441.pdf},
year = {2011}
}
@inproceedings{Kleinsasser2003,
address = {Montreal, QC, Canada},
author = {Kleinsasser, William},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {213--215},
title = {{Dsp.rack: Laptop-based Modular, Programmable Digital Signal Processing and Mixing for Live Performance}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}213.pdf},
year = {2003}
}
@inproceedings{Min2009,
address = {Pittsburgh, PA, United States},
author = {Min, Hye Ki},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {game interfaces,mathematics and,mathematics in music,nime09,numbers,puzzles,sound,tangible user},
pages = {82--85},
title = {{SORISU : Sound with Numbers}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}082.pdf},
year = {2009}
}
@inproceedings{Bryan2010,
address = {Sydney, Australia},
author = {Bryan, Nicholas J and Herrera, Jorge and Oh, Jieun and Wang, Ge},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {instrument design,iphone,mobile music,nime10,software develop-},
number = {Nime},
pages = {174--177},
title = {{MoMu : A Mobile Music Toolkit}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}174.pdf},
year = {2010}
}
@inproceedings{Bevilacqua2007,
abstract = {We present in this paper a complete gestural interface built to support music pedagogy. The development of this prototype concerned both hardware and software components: a small wireless sensor interface including accelerometers and gyroscopes, and an analysis system enabling gesture following and recognition. A first set of experiments was conducted with teenagers in a music theory class. The preliminary results were encouraging concerning the suitability of these developments in music education. },
address = {New York City, NY, United States},
author = {Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Gu{\'{e}}dy, Fabrice and Schnell, Norbert and Fl{\'{e}}ty, Emmanuel and Leroy, Nicolas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Technology-enhanced learning,gesture recognition,gesture-follower,music pedagogy,wireless interface},
pages = {124--129},
title = {{Wireless Sensor Interface and Gesture-Follower for Music Pedagogy}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}124.pdf},
year = {2007}
}
@inproceedings{Nagashima2007,
address = {New York City, NY, United States},
author = {Nagashima, Yoichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {kendang,media arts,new instruments,nime07,sound and light},
pages = {250--253},
title = {{GHI project and "Cyber Kendang"}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}250.pdf},
year = {2007}
}
@inproceedings{Katayose2004,
address = {Hamamatsu, Japan},
author = {Katayose, Haruhiro and Okudaira, Keita},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {interfaces for musical expression,rencon,visualization},
pages = {124--129},
title = {{Using an Expressive Performance Template in a Music Conducting Interface}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}124.pdf},
year = {2004}
}
@inproceedings{Anderson2010,
abstract = {This project aims at studying how recent interactive and in- teractions technologies would help extend how we play the guitar, thus defining the multimodal guitar. Our contri- butions target three main axes: audio analysis, gestural con- trol and audio synthesis. For this purpose, we designed and developed a freely-available toolbox for augmented guitar performances, compliant with the PureData and Max/MSP environments, gathering tools for: polyphonic pitch estima- tion, fretboard visualization and grouping, pressure sensing, modal synthesis, infinite sustain, rearranging looping and smart harmonizing.},
author = {{John Anderson, Mills}, III},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {audio synthesis,augmented guitar,digital audio effects,gestural sensing,hexaphonic guitar,multimodal interaction,polyphonic tran,scription},
number = {Nime},
pages = {415--418},
title = {{Multimodal Guitar : A Toolbox For Augmented Guitar Performances}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}415.pdf},
year = {2010}
}
@inproceedings{Lee2006,
abstract = {Designing a conducting gesture analysis system for public spacesposes unique challenges. We present conga, a software framework that enables automatic recognition and interpretation ofconducting gestures. conga is able to recognize multiple types ofgestures with varying levels of difficulty for the user to perform,from a standard four-beat pattern, to simplified up-down conducting movements, to no pattern at all. conga provides an extendablelibrary of feature detectors linked together into a directed acyclicgraph; these graphs represent the various conducting patterns asgesture profiles. At run-time, conga searches for the best profileto match a user's gestures in real-time, and uses a beat prediction algorithm to provide results at the sub-beat level, in additionto output values such as tempo, gesture size, and the gesture'sgeometric center. Unlike some previous approaches, conga doesnot need to be trained with sample data before use. Our preliminary user tests show that conga has a beat recognition rate ofover 90{\%}. conga is deployed as the gesture recognition systemfor Maestro!, an interactive conducting exhibit that opened in theBetty Brinn Children's Museum in Milwaukee, USA in March2006.},
address = {Paris, France},
author = {Lee, Eric and Gr{\"{u}}ll, Ingo and Keil, Henning and Borchers, Jan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {conducting,gesture recognition,software gesture frameworks},
pages = {260--265},
title = {{conga: A Framework for Adaptive Conducting Gesture Analysis}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}260.pdf},
year = {2006}
}
@inproceedings{Rigler2007,
abstract = {The Music Cre8tor is an interactive music composition systemcontrolled by motion sensors specifically designed forchildren with disabilities although not exclusively for thispopulation. The player(s) of the Music Cre8tor can either holdor attach accelerometer sensors to trigger a variety ofcomputer-generated sounds, MIDI instruments and/or prerecorded sound files. The sensitivity of the sensors can bemodified for each unique individual so that even the smallestmovement can control a sound. The flexibility of the systemis such that either four people can play simultaneously and/orone or more players can use up to four sensors. The originalgoal of this program was to empower students with disabilitiesto create music and encourage them to perform with othermusicians, however this same goal has expanded to includeother populations.},
address = {New York City, NY, United States},
author = {Rigler, Jane and Seldess, Zachary},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Music Education,disabilities,interactive performance.,motion sensors,music composition,special education},
pages = {415--416},
title = {{The Music Cre8tor : an Interactive System for Musical Exploration and Education}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}415.pdf},
year = {2007}
}
@inproceedings{Popp2011,
address = {Oslo, Norway},
author = {Popp, Phillip and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Gesture Recognition,Machine Learning,Spectral Model Synthesis,Synthesis Control,Wacom Tablet},
pages = {284--287},
title = {{Intuitive Real-Time Control of Spectral Model Synthesis}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}284.pdf},
year = {2011}
}
@inproceedings{Kawahara2004,
address = {Hamamatsu, Japan},
author = {Kawahara, Hideki and Banno, Hideki and Morise, Masanori},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
number = {45},
pages = {130--131},
title = {{Acappella Synthesis Demonstrations using RWC Music Database}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}130.pdf},
year = {2004}
}
@inproceedings{Lee2007a,
address = {New York City, NY, United States},
author = {Lee, Eric and Wolf, Marius and Jansen, Yvonne and Borchers, Jan},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {augmented instru-,e-learning,education,interactive music exhibits,medieval music,ments,nime07},
pages = {172--177},
title = {{REXband : A Multi-User Interactive Exhibit for Exploring Medieval Music}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}172.pdf},
year = {2007}
}
@inproceedings{Breinbjerg2006,
address = {Paris, France},
author = {Breinbjerg, Morten and Caprani, Ole and Lunding, Rasmus and Kramhoft, Line},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {acousmatic listening,aesthetics,tangible interfaces},
pages = {334--337},
title = {{An Acousmatic Composition Environment}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}334.pdf},
year = {2006}
}
@inproceedings{Bevilacqua2006,
address = {Paris, France},
author = {Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Rasamimanana, Nicolas and Fl{\'{e}}ty, Emmanuel and Lemouton, Serge and Baschet, Florence},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
pages = {402--406},
title = {{The Augmented Violin Project: Research, Composition and Performance Report}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}402.pdf},
year = {2006}
}
@inproceedings{Schacher:2013a,
abstract = {This article describes the motivations and reflections that led to thedevelopment of a gestural sensor instrument called the Quarterstaff. In aniterative design and fabrication process, several versions of this interfacewere build, tested and evaluated in performances. A detailed explanation of thedesign choices concerning the shape but also the sensing capabilities of theinstrument illustrates the emphasis on establishing an `enactive'instrumental relationship. A musical practice for this type of instrument isshown by discussing the methods used in the exploration of the gesturalpotential of the interface and the strategies deployed for the development ofmappings and compositions. Finally, to gain more information about how thisinstrument compares with similar designs, two dimension-space analyses are madethat show a clear positioning in relation to instruments that precede theQuarterstaff.},
address = {Daejeon, Republic of Korea},
author = {Schacher, Jan C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Gestural sensor interface,body-object relation,composition and performance practice,dimension space analysis,instrument design},
pages = {535--540},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{The Quarterstaff, a Gestural Sensor Instrument}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@article{OSullivan2012,
abstract = {Development of new musical interfaces often requires experimentation with the mapping of available controller inputs to output parameters. Useful mappings for a particular application may be complex in nature, with one or more inputs being linked to one or more outputs. Existing development environments are commonly used to program such mappings, while code libraries provide powerful data-stream manipulation. However, room exists for a standalone application with a simpler graphical user interface for dynamically patching between inputs and outputs. This paper presents an early prototype version of a software tool that allows the user to route control signals in real time, using various messaging formats. It is cross-platform and runs as a standalone application in desktop and Android OS versions. The latter allows the users of mobile devices to experiment with mapping signals to and from physical computing components using the inbuilt multi-touch screen. Potential uses therefore include real-time mapping during performance in a more expressive manner than facilitated by existing tools.},
author = {O'Sullivan, Liam and Furlong, Dermot and Boland, Frank},
journal = {NIME 2012 Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {android,mapping,software tools},
pages = {230--233},
title = {{Introducing CrossMapper: Another Tool for Mapping Musical Control Parameters}},
year = {2012}
}
@inproceedings{Martin2011,
address = {Oslo, Norway},
author = {Martin, Charles and Lai, Chi-Hsia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {computer vision,media performance,percussion},
pages = {142--143},
title = {{Strike on Stage : a Percussion and Media Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}142.pdf},
year = {2011}
}
@inproceedings{Gaye2004,
address = {Hamamatsu, Japan},
author = {Gaye, Lalya and Holmquist, Lars E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {context-awareness,expression,figure 1,interactive,mobility,music,new interface for musical,user in action,user study,video-filming a sonic city,wearable computing},
pages = {161--164},
title = {{In Duet with Everyday Urban Settings: A User Study of Sonic City}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}161.pdf},
year = {2004}
}
@inproceedings{Jakovich2007,
address = {New York City, NY, United States},
author = {Jakovich, Joanne and Beilharz, Kirsty},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {1,1 intermodal mapping,adaptation,architecture,engagement,granular synthesis,installation,interaction,intermodal space,nime07},
pages = {185--190},
title = {{ParticleTecture : Interactive Granular Soundspaces for Architectural Design}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}185.pdf},
year = {2007}
}
@inproceedings{Nagashima2010,
address = {Sydney, Australia},
author = {Nagashima, Yoichi},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {distance sensor,nime10,propeller processor,theremin,untouchable},
number = {Nime},
pages = {387--390},
title = {{Untouchable Instrument "Peller-Min"}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}387.pdf},
year = {2010}
}
@inproceedings{Tormoen2014,
abstract = {This paper introduces an extension of the Rubato Composer software's BigBang rubette module for gestural composition. The extension enables composers and improvisers to operate BigBang using the Leap Motion controller, which uses two cameras to detect hand motions in three-dimensional space. The low latency and high precision of the device make it a good fit for BigBang's functionality, which is based on immediate visual and auditive feedback. With the new extensions, users can define an infinite variety of musical objects, such as oscillators, pitches, chord progressions, or frequency modulators, in real-time and transform them in order to generate more complex musical structures on any level of abstraction.},
address = {London, United Kingdom},
author = {Tormoen, Daniel and Thalmann, Florian and Mazzola, Guerino},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
keywords = {Leap Motion,hand gesture,improvisation,meta-composition,rubato composer},
pages = {207--212},
publisher = {Goldsmiths, University of London},
title = {{The Composing Hand: Musical Creation with Leap Motion and the BigBang Rubette}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}536.pdf},
year = {2014}
}
@inproceedings{Kleinberger:2013,
abstract = {PAMDI is an electromechanical music controller based on an expansion of thecommon metal music boxes. Our system enables an augmentation of the musicalproperties by adding different musical channels triggered and parameterized bynatural gestures during the ``performance''. All the channels are generatedform the original melody recorded once at the start.To capture and treat the different expressive parameters both natural andintentional, our platform is composed of a metallic structure supportingsensors. The measured values are processed by an arduino system that finallysends the results by serial communication to a Max/MSP patch for signaltreatment and modification. We will explain how our embedded instrument aims to bring a certain awarenessto the player of the mapping and the potential musical freedom of the veryspecific -- and not that much automatic - instrument that is a music box. Wewill also address how our design tackles the different questions of mapping,ergonomics and expressiveness while choosing the controller modalities and theparameters to be sensed.},
address = {Daejeon, Republic of Korea},
author = {Kleinberger, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Tangible interface,mapping.,mechanical and electronic coupling,music box,musical controller},
pages = {19--20},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{PAMDI Music Box: Primarily Analogico-Mechanical, Digitally Iterated Music Box}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Goto2006,
abstract = {This paper is intended to introduce the system, whichcombines "BodySuit" and "RoboticMusic," as well as itspossibilities and its uses in an artistic application."BodySuit" refers to a gesture controller in a Data Suit type."RoboticMusic" refers to percussion robots, which are appliedto a humanoid robot type. In this paper, I will discuss theiraesthetics and the concept, as well as the idea of the "ExtendedBody".},
address = {Paris, France},
author = {Goto, Suguru},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Artificial Intelligence,Gesture Controller,Humanoid Robot,Interaction,Robot},
pages = {292--295},
title = {{The Case Study of An Application of The System, {\{}$\backslash$textquotedblleft{\}}BodySuit{\{}$\backslash$textquotedblright{\}} and {\{}$\backslash$textquotedblleft{\}}RoboticMusic{\{}$\backslash$textquotedblright{\}}: Its Introduction and Aesthetics}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}292.pdf},
year = {2006}
}
@inproceedings{Freeman2007,
abstract = {Graph Theory links the creative music-making activities of web site visitors to the dynamic generation of an instrumental score for solo violin. Participants use a web-based interface to navigate among short, looping musical fragments to create their own unique path through the open-form composition. Before each concert performance, the violinist prints out a new copy of the score that orders the fragments based on the decisions made by web visitors. },
address = {New York City, NY, United States},
author = {Freeman, Jason},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Audience Interaction,Collaboration,Composition,Flash,Graph,Internet,Music,Residency,Traveling Salesman.,Violin},
pages = {260--263},
title = {{Graph Theory : Interfacing Audiences Into the Compositional Process}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}260.pdf},
year = {2007}
}
@inproceedings{Camurri2002,
address = {Dublin, Ireland},
author = {Camurri, Antonio and Trocca, Riccardo and Volpe, Gualtiero},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
pages = {30--37},
title = {{Interactive Systems Design: A KANSEI-based Approach}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}030.pdf},
volume = {20410},
year = {2002}
}
@inproceedings{Lehrman:2012,
abstract = {George Antheil's notorious Ballet m{\{}{\'{e}}{\}}canique (1924-1925) was originally scored for percussion ensemble, sound effects, and 16 pianolas. He was never able to perform the piece with those forces, however, due to his inability to synchronize multiple pianolas. Thus all performances of the piece in his lifetime, and for decades after, were done with a single pianola or player piano.*
The author traces the origin of the concept of synchronizing multiple pianolas, and explains the attendant technological issues. He examines attempts to synchronize mechanical pianos and other time-based devices at the time of Ballet m{\{}{\'{e}}{\}}canique's composition, and suggests that Antheil's vision for his piece was not as farfetched as has long been thought.},
address = {Ann Arbor, Michigan},
author = {Lehrman, Paul},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Antheil,Stravinsky,mechanical instruments,pianola,player piano,synchronization},
publisher = {University of Michigan},
title = {{Multiple Pianolas in Antheil's Ballet m{\{}{\'{e}}{\}}canique}},
year = {2012}
}
@inproceedings{Fornari2007,
abstract = {We present an interactive sound spatialization and synthesis system based on Interaural Time Difference (ITD) model and Evolutionary Computation. We define a Sonic Localization Field using sound attenuation and ITD azimuth angle parameters and, in order to control an adaptive algorithm, we used pairs of these parameters as Spatial Sound Genotypes (SSG). They are extracted from waveforms which are considered individuals of a Population Set. A user-interface receives input from a generic gesture interface (such as a NIME device) and interprets them as ITD cues. Trajectories provided by these signals are used as Target Sets of an evolutionary algorithm. A Fitness procedure optimizes locally the distance between the Target Set and the SSG pairs. Through a parametric score the user controls dynamic changes in the sound output. },
address = {New York City, NY, United States},
author = {Fornari, Jose and Maia, Adolfo Jr. and Manzolli, Jonatas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {adaptation.,evolutionary,interactive,sound,spatialization},
pages = {293--298},
title = {{Interactive Spatialization and Sound Design using an Evolutionary System}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}293.pdf},
year = {2007}
}
@inproceedings{Wiriadjaja2013,
abstract = {The Gamelan Sampul is a laptop sleeve with embedded circuitry that allows usersto practice playing Javanese gamelan instruments without a full set ofinstruments. It is part of a larger project that aims to develop a set ofportable and mobile tools for learning, recording and performing classicalJavanese gamelan music.The accessibility of a portable Javanese gamelan set introduces the musicalgenre to audiences who have never experienced this traditional music before,passing down long established customs to future generations. But it also raisesthe question of what is and what isn't appropriate to the musical tradition.The Gamelan Sampul attempts to introduce new technology to traditional folkmusic while staying sensitive to cultural needs.},
address = {Daejeon, Republic of Korea},
author = {Wiriadjaja, Antonius},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Physical computing,gamelan,product design,traditional folk arts},
pages = {469--470},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Gamelan Sampul: Laptop Sleeve Gamelan}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Schacher:2013a,
abstract = {This article describes the motivations and reflections that led to thedevelopment of a gestural sensor instrument called the Quarterstaff. In aniterative design and fabrication process, several versions of this interfacewere build, tested and evaluated in performances. A detailed explanation of thedesign choices concerning the shape but also the sensing capabilities of theinstrument illustrates the emphasis on establishing an `enactive'instrumental relationship. A musical practice for this type of instrument isshown by discussing the methods used in the exploration of the gesturalpotential of the interface and the strategies deployed for the development ofmappings and compositions. Finally, to gain more information about how thisinstrument compares with similar designs, two dimension-space analyses are madethat show a clear positioning in relation to instruments that precede theQuarterstaff.},
address = {Daejeon, Republic of Korea},
author = {Schacher, Jan C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Gestural sensor interface,body-object relation,composition and performance practice,dimension space analysis,instrument design},
month = {may},
pages = {535--540},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{The Quarterstaff, a Gestural Sensor Instrument}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{McGee:2013,
abstract = {VOSIS is an interactive image sonification interface that creates complexwavetables by raster scanning greyscale image pixel data. Using a multi-touchscreen to play image regions of unique frequency content rather than a linearscale of frequencies, it becomes a unique performance tool for experimental andvisual music. A number of image filters controlled by multi-touch gestures addvariation to the sound palette. On a mobile device, parameters controlled bythe accelerometer add another layer expressivity to the resulting audio-visualmontages.},
address = {Daejeon, Republic of Korea},
author = {McGee, Ryan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {image sonification,multi-touch,visual music},
pages = {460--463},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{VOSIS: a Multi-touch Image Sonification Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Hansen2008,
address = {Genoa, Italy},
author = {Hansen, Kjetil F and Alonso, Marcos},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {dj scratch techniques,interfaces,nime08,playability,reactable},
pages = {207--210},
title = {{More DJ Techniques on the reactable}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}207.pdf},
year = {2008}
}
@inproceedings{Bergsland2011,
address = {Oslo, Norway},
author = {Bergsland, Andreas},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {analysis,csound,lpc,modeling,software instrument},
number = {June},
pages = {523--526},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{phrases from Paul Lansky ' s Six Fantasies}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}523.pdf},
year = {2011}
}
@inproceedings{Brown2010,
address = {Sydney, Australia},
author = {Brown, Andrew R},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {collaborative,ensemble,generative,interaction,network,nime10},
number = {Nime},
title = {{Network Jamming : Distributed Performance using Generative Music}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}283.pdf},
year = {2010}
}
@inproceedings{Dimitrov2010,
abstract = {The sound card anno 2010, is an ubiquitous part of almostany personal computing system; what was once considereda high-end, CD-quality audio fidelity, is today found in mostcommon sound cards. The increased presence of multichannel devices, along with the high sampling frequency, makesthe sound card desirable as a generic interface for acquisition of analog signals in prototyping of sensor-based musicinterfaces. However, due to the need for coupling capacitorsat a sound card's inputs and outputs, the use as a genericsignal interface of a sound card is limited to signals not carrying information in a constant DC component. Through arevisit of a card design for the (now defunct) ISA bus, thispaper proposes use of analog gates for bypassing the DCfiltering input sections, controllable from software - therebyallowing for arbitrary choice by the user, if a soundcardinput channel is to be used as a generic analog-to-digitalsensor interface. Issues regarding use of obsolete technology and educational aspects are discussed as well.},
address = {Sydney, Australia},
author = {Dimitrov, Smilen},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {dc,isa,nime10,sensors,soundcard},
pages = {303--308},
title = {{Extending the Soundcard for Use with Generic {\{}DC{\}} Sensors Demonstrated by Revisiting a Vintage ISA Design}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}303.pdf},
year = {2010}
}
@inproceedings{Albin2011,
address = {Oslo},
author = {Albin, Aaron and Sent{\"{u}}rk, Sertan and {Van Troyer}, Akito and Blosser, Brian and Jan, Oliver and Weinberg, Gil},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {ensemble,ization,mixed media,processing,reactivision,sample,tangible,virtual-},
number = {June},
pages = {112--115},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Beatscape , a Mixed Virtual-Physical Environment for Musical Ensembles}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}112.pdf},
year = {2011}
}
@inproceedings{Choi:2012,
abstract = {The Deckle Group1 is an ensemble that designs, builds and performs on electroacoustic drawing boards. These draw- ing surfaces are augmented with Satellite CCRMA Beagle- Boards and Arduinos2.[1] Piezo microphones are used in conjunction with other sensors to produce sounds that are coupled tightly to mark-making gestures. Position tracking is achieved with infra-red object tracking, conductive fabric and a magnetometer.},
address = {Ann Arbor, Michigan},
author = {Choi, Hongchan and Granzow, John and Sadler, Joel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Audiovisual,BeagleBoard,Deckle,Drawing,Gestural Interface,Performance,Sonification},
publisher = {University of Michigan},
title = {{The Deckle Project : A Sketch of Three Sensors}},
year = {2012}
}
@inproceedings{Fabiani2011,
abstract = {This paper presents MoodifierLive, a mobile phone application for interactive control of rule-based automatic musicperformance. Five different interaction modes are available,of which one allows for collaborative performances with upto four participants, and two let the user control the expressive performance using expressive hand gestures. Evaluations indicate that the application is interesting, fun touse, and that the gesture modes, especially the one based ondata from free expressive gestures, allow for performanceswhose emotional content matches that of the gesture thatproduced them.},
address = {Oslo, Norway},
author = {Fabiani, Marco and Dubus, Ga{\"{e}}l and Bresin, Roberto},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Expressive performance,collaborative performance,gesture,mobile phone},
pages = {116--119},
title = {{MoodifierLive : Interactive and Collaborative Expressive Music Performance on Mobile Devices}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}116.pdf},
year = {2011}
}
@inproceedings{nime2014:aplace,
abstract = {This paper explores the design process of the AlphaSphere, an experimental new musical instrument that has transitioned into scale production and international distribution. Initially, the design intentions and engineering processes are covered. The paper continues by briefly evaluating the user testing process and outlining the ergonomics, communication protocol and software of the device. The paper closes by questioning what it takes to evaluate success as a musical instrument.},
address = {London, United Kingdom},
author = {Place, Adam and Lacey, Liam and Mitchell, Thomas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {399--402},
publisher = {Goldsmiths, University of London},
title = {{AlphaSphere - from Prototype to Product}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}568.pdf},
year = {2014}
}
@inproceedings{Bouenard2008,
address = {Genoa, Italy},
author = {Bou{\"{e}}nard, Alexandre and Gibet, Sylvie and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {character,gesture and sound,interaction,interface,nime08,percussion gesture,virtual},
pages = {38--43},
title = {{Enhancing the Visualization of Percussion Gestures by Virtual Character Animation}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}038.pdf},
year = {2008}
}
@inproceedings{Sarkar2007,
abstract = {Playing music over the Internet, whether for real-time jamming, network performance or distance education, is constrained by the speed of light which introduces, over long distances, time delays unsuitable for musical applications. Current musical collaboration systems generally transmit compressed audio streams over low-latency and high-bandwidthnetworks to optimize musician synchronization. This paperproposes an alternative approach based on pattern recognition and music prediction. Trained for a particular typeof music, here the Indian tabla drum, the system calledTablaNet identifies rhythmic patterns by recognizing individual strokes played by a musician and mapping them dynamically to known musical constructs. Symbols representing these musical structures are sent over the network toa corresponding computer system. The computer at thereceiving end anticipates incoming events by analyzing previous phrases and synthesizes an estimated audio output.Although such a system may introduce variants due to prediction approximations, resulting in a slightly different musical experience at both ends, we find that it demonstratesa high level of playability with an immediacy not present inother systems, and functions well as an educational tool.},
address = {New York City, NY, United States},
author = {Sarkar, Mihir and Vercoe, Barry},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Indian percussions,music prediction,network music performance,real-time online musical collab- oration,strokes recognition,tabla bols},
pages = {317--320},
title = {{Recognition and Prediction in a Network Music Performance System for {\{}India{\}}n Percussion}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}317.pdf},
year = {2007}
}
@inproceedings{Cannon2003,
abstract = {In this paper we present a design for the EpipE, a newexpressive electronic music controller based on the IrishUilleann Pipes, a 7-note polyphonic reeded woodwind. Thecore of this proposed controller design is a continuouselectronic tonehole-sensing arrangement, equally applicableto other woodwind interfaces like those of the flute, recorder orJapanese shakuhachi. The controller will initially be used todrive a physically-based synthesis model, with the eventualgoal being the development of a mapping layer allowing theEpipE interface to operate as a MIDI-like controller of arbitrarysynthesis models.},
address = {Montreal},
author = {Cannon, Cormac and Hughes, Stephen and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
keywords = {Controllers,Irish bagpipe,conical bore,continuous woodwind tonehole sensor,double reed,physical modelling,tonehole.,uilleann pipes},
pages = {3--8},
title = {{EpipE: Exploration of the Uilleann Pipes as a Potential Controller for Computer-based Music}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}003.pdf},
year = {2003}
}
@inproceedings{Remus2006,
address = {Paris, France},
author = {R{\'{e}}mus, Jacques},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {camera musicale,interface,jacques r{\'{e}}mus,machines,musical camera,musical hand,non haptic instrument,s mappings,sculptures and mechanical musical,sound},
pages = {250--253},
title = {{Non Haptic Control of Music by Video Analysis of Hand Movements: 14 Years of Experience with the Cam{\'{e}}ra Musicale}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}250.pdf},
year = {2006}
}
@inproceedings{Smyth:2002,
abstract = {The cicada uses a rapid sequence of buckling ribs to initiate and sustain vibrations in its tymbal plate (the primary mechanical resonator in the cicada's sound production system). The tymbalimba, a music controller based on this same mechanism, has a row of 4 convex aluminum ribs (ason the cicada's tymbal) arranged much like the keys on a calimba. Each rib is spring loaded and capable of snapping down into a V-shape (a motion referred to as buckling), under the downward force of the user's finger. This energy generated by the buckling motion is measured by an accelerometer located under each rib and used as the input to a physical model.},
address = {Dublin},
author = {Smyth, Tamara and Smith, Julius O},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Bioacoustics,Buckling mechanism.,Cicada,Controllers,Physical Modeling},
pages = {24--27},
title = {{Creating Sustained Tones with the Cicada's Rapid Sequential Buckling Mechanism}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}161.pdf},
year = {2002}
}
@inproceedings{Han:2013a,
abstract = {This paper presents a framework that transforms fingerprint patterns intoaudio. We describe Digiti Sonus, an interactive installation performingfingerprint sonification and visualization, including novel techniques forrepresenting user-intended fingerprint expression as audio parameters. In orderto enable personalized sonification and broaden timbre of sound, theinstallation employs sound synthesis based on various visual feature analysissuch as minutiae extraction, area, angle, and push pressure of fingerprints.The sonification results are discussed and the diverse timbres of soundretrieved from different fingerprints are compared.},
address = {Daejeon, Republic of Korea},
author = {Han, Yoon Chung and Han, Byeong-jun and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Fingerprint,Fingerprint sonification,biometric data,interactive sonification,sound synthesis},
month = {may},
pages = {136--141},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Digiti Sonus: Advanced Interactive Fingerprint Sonification Using Visual Feature Analysis}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{nime2014:mrodrigues,
abstract = {Digital Musical Instruments (DMIs) have difficulties establishing themselves after their creation. A huge number of DMIs is presented every year and few of them actually remain in use. Several causes could explain this reality, among them the lack of a proper instrumental technique, inadequacy of the traditional musical notation and the non-existence of a repertoire dedicated to the instrument. In this paper we present Entoa, the first written music for Intonaspacio, a DMI we designed in our research project. We propose some strategies for mapping data from sensors to sound processing, in order to accomplish an expressive performance. Entoa is divided in five different sections that corresponds to five movements. For each, a different mapping is designed, introducing subtle alterations that progressively explore the ensemble of features of the instrument. The performer is then required to adapt his repertoire of gestures along the piece. Indications are expressed through a gestural notation, where freedom is give to performer to control certain parameters at specific moments in the music.},
address = {London, United Kingdom},
author = {Mamedes, Clayton and Rodrigues, Mailis and Wanderley, Marcelo M and Manzolli, J{\^{o}}natas and Garcia, Denise H L and Ferreira-Lopes, Paulo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {509--512},
publisher = {Goldsmiths, University of London},
title = {{Composing for DMIs - Entoa, a Dedicate Piece for Intonaspacio}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}411.pdf},
year = {2014}
}
@inproceedings{Barri2009,
address = {Pittsburgh, PA, United States},
author = {Barri, Tarik},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {audiovisual,collaboration,expectations based upon their,having the audience create,make all musical and,nime09,on the contrary,predictable,sequencing,the goal is to,understanding of the,visual events},
pages = {264--265},
title = {{Versum : Audiovisual Composing in 3d}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}264.pdf},
year = {2009}
}
@inproceedings{McPherson:2013,
abstract = {This paper presents a portable optical measurement system for capturingcontinuous key motion on any piano. Very few concert venues have MIDI-enabledpianos, and many performers depend on the versatile but discontinued MoogPianoBar to provide MIDI from a conventional acoustic instrument. The scannerhardware presented in this paper addresses the growing need for alternativesolutions while surpassing existing systems in the level of detail measured.Continuous key position on both black and white keys is gathered at 1kHz samplerate. Software extracts traditional and novel features of keyboard touch fromeach note, which can be flexibly mapped to sound using MIDI or Open SoundControl. RGB LEDs provide rich visual feedback to assist the performer ininteracting with more complex sound mapping arrangements. An application ispresented to the magnetic resonator piano, an electromagnetically-augmentedacoustic grand piano which is performed using continuous key positionmeasurements.},
address = {Daejeon, Republic of Korea},
author = {McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Piano,gesture sensing,keyboard,magnetic resonator piano,mapping,optical sensing,visual feedback},
pages = {152--157},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Portable Measurement and Mapping of Continuous Piano Gesture}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Blaine2005,
abstract = {This paper will investigate a variety of alternate controllers that are making an impact in interactive entertainment, particularly in the video game industry. Since the late 1990's, the surging popularity of rhythmic and musical performance games in Japanese arcades has led to the development of new interfaces and alternate controllers for the consumer market worldwide. Rhythm action games such as Dance Dance Revolution, Taiko No Tatsujin (Taiko: Drum Master), and Donkey Konga are stimulating collaborative gameplay and exposing consumers to custom controllers designed specifically for musical and physical interaction. We are witnessing the emergence and acceptance of these breakthrough controllers and models for gameplay as an international cultural phenomenon penetrating the video game and toy markets in record numbers. Therefore, it is worth considering the potential benefits to developers of musical interfaces, electronic devices and alternate controllers in light of these new and emerging opportunities, particularly in the realm of video gaming, toy development, arcades, and other interactive entertainment experiences. },
address = {Vancouver, BC, Canada},
author = {Blaine, Tina},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {27--33},
title = {{The Convergence of Alternate Controllers and Musical Interfaces in Interactive Entertainment}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}027.pdf},
year = {2005}
}
@inproceedings{Clay:2012,
abstract = {The augmented ballet project aims at gathering research from several fields and directing them towards a same application case: adding virtual elements (visual and acoustic) to a dance live performance, and allowing the dancer to interact with them. In this paper, we describe a novel interaction that we used in the frame of this project: using the dancer's movements to recognize the emotions he expresses, and use these emotions to generate musical audio flows evolving in real-time. The originality of this interaction is threefold. First, it covers the whole interaction cycle from the input (the dancer's movements) to the output (the generated music). Second, this interaction isn't direct but goes through a high level of abstraction: dancer's emotional expression is recognized and is the source of music generation. Third, this interaction has been designed and validated through constant collaboration with a choreographer, culminating in an augmented ballet performance in front of a live audience.},
address = {Ann Arbor, Michigan},
author = {Clay, Alexis and Couture, Nadine and Desainte-Catherine, Myriam and Vulliard, Pierre-Henri and Larralde, Joseph and Decarsin, Elodie},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Interactive sonification,gesture and music,interaction,live performance,motion,musical human-computer interaction},
month = {may},
publisher = {University of Michigan},
title = {{Movement to emotions to music: using whole body emotional expression as an interaction for electronic music generation}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}180.pdf},
year = {2012}
}
@inproceedings{Leslie2011,
abstract = {MoodMixer is an interactive installation in which participants collaboratively navigate a two-dimensional music spaceby manipulating their cognitive state and conveying thisstate via wearable Electroencephalography (EEG) technology. The participants can choose to actively manipulateor passively convey their cognitive state depending on theirdesired approach and experience level. A four-channel electronic music mixture continuously conveys the participants'expressed cognitive states while a colored visualization oftheir locations on a two-dimensional projection of cognitive state attributes aids their navigation through the space.MoodMixer is a collaborative experience that incorporatesaspects of both passive and active EEG sonification andperformance art. We discuss the technical design of the installation and place its collaborative sonification aestheticdesign within the context of existing EEG-based music andart.},
address = {Oslo, Norway},
author = {Leslie, Grace and Mullen, Tim},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {BCMI,EEG,collaboration,sonification,visualization},
pages = {296--299},
title = {{MoodMixer : {\{}EEG{\}}-based Collaborative Sonification}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}296.pdf},
year = {2011}
}
@inproceedings{Smith2011,
abstract = {Supervised machine learning enables complex many-to-manymappings and control schemes needed in interactive performance systems. One of the persistent problems in theseapplications is generating, identifying and choosing inputoutput pairings for training. This poses problems of scope(limiting the realm of potential control inputs), effort (requiring significant pre-performance training time), and cognitive load (forcing the performer to learn and remember thecontrol areas). We discuss the creation and implementationof an automatic "supervisor," using unsupervised machinelearning algorithms to train a supervised neural networkon the fly. This hierarchical arrangement enables networktraining in real time based on the musical or gestural control inputs employed in a performance, aiming at freeing theperformer to operate in a creative, intuitive realm, makingthe machine control transparent and automatic. Three implementations of this self supervised model driven by iPod,iPad, and acoustic violin are described.},
address = {Oslo, Norway},
author = {Smith, Benjamin D and Garnett, Guy E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {NIME,adaptive resonance theory,improvisation,interactive computer music,machine learning,machine listening},
pages = {108--111},
title = {{The Self-Supervising Machine}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}108.pdf},
year = {2011}
}
@inproceedings{Berdahl2008,
address = {Genoa, Italy},
author = {Berdahl, Edgar and Smith, Julius O},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {coupled,excitation,guitar,interface,modeling,models,nime08,physical,physically motivated,pitch detection,string,vibrating},
pages = {299--302},
title = {{A Tangible Virtual Vibrating String : A Physically Motivated Virtual Musical Instrument Interface}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}299.pdf},
year = {2008}
}
@inproceedings{Hochenbaum2011,
address = {Oslo, Norway},
author = {Hochenbaum, Jordan and Kapur, Ajay},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {attempt to provide the,best of both,bricktable,fiducial,gesture,hap-,multi-touch,proximity sensing,sensors,tangible interface,the smartfiducial is an,tics},
number = {June},
pages = {240--243},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Adding Z-Depth and Pressure Expressivity to Tangible Tabletop Surfaces}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}240.pdf},
year = {2011}
}
@inproceedings{nime2014:ncollins,
abstract = {The algorave movement has received reasonable international exposure in the last two years, including a series of concerts in Europe and beyond, and press coverage in a number of media. This paper seeks to illuminate some of the historical precedents to the scene, its primary aesthetic goals, and the divergent technological and musical approaches of representative participants. We keep in mind the novel possibilities in musical expression explored by algoravers. The scene is by no means homogeneous, and the very lack of uniformity of technique, from new live coding languages through code DJing to plug-in combination, with or without visual extension, is indicative of the flexibility of computers themselves as general information processors.},
address = {London, United Kingdom},
author = {Collins, Nick and McLean, Alex},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {355--358},
publisher = {Goldsmiths, University of London},
title = {{Algorave: A Survey of the History, Aesthetics and Technology of Live Performance of Algorithmic Electronic Dance Music}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}426.pdf},
year = {2014}
}
@inproceedings{Knapp2006,
abstract = {In this paper, we describe the networking of multiple Integral Music Controllers (IMCs) to enable an entirely new method for creating music by tapping into the composite gestures and emotions of not just one, but many performers. The concept and operation of an IMC is reviewed as well as its use in a network of IMC controllers. We then introduce a new technique of Integral Music Control by assessing the composite gesture(s) and emotion(s) of a group of performers through the use of a wireless mesh network. The Telemuse, an IMC designed precisely for this kind of performance, is described and its use in a new musical performance project under development by the ,
,
authors is discussed. },
address = {Paris, France},
author = {Knapp, Benjamin and Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Community-Institutional Relations,Health Services Accessibility,Medically Uninsured,Organizational Case Studies,Primary Health Care,Public-Private Sector Partnerships,San Francisco},
pages = {124--128},
title = {{Creating a Network of Integral Music Controllers}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}124.pdf},
year = {2006}
}
@inproceedings{Ishida2004,
address = {Hamamatsu, Japan},
author = {Ishida, Katsuhisa and Kitahara, Tetsuro and Takeda, Masayuki},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {improvisation support,jam session,melody correction,melody modeling,musical instrument,n-gram model},
pages = {177--180},
title = {{ism: Improvisation Supporting System based on Melody Correction}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}177.pdf},
year = {2004}
}
@inproceedings{nime2014:cdominguez,
abstract = {This paper presents a project that discusses a brief history of artistic systems that use photoresistors (light-dependent resistors) and results in the construction of an interface and performance controller. The controller combines an Arduino microcontroller with a grid of photoresistors set into a slab of wood covered with a thin acrylic sheet. A brief background on past uses of these components for music and film composition and instrument-building introduces a few different implementations and performance contexts for the controller. Topics such as implementation, construction, and performance possibilities (including electroacoustic and audio-visual performance) of the controller are also discussed.},
address = {London, United Kingdom},
author = {Dominguez, Carlos},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {78--79},
publisher = {Goldsmiths, University of London},
title = {{16-CdS: A Surface Controller for the Simultaneous Manipulation of Multiple Analog Components}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}552.pdf},
year = {2014}
}
@inproceedings{Fels:2004,
abstract = {The Tooka was created as an exploration of two personinstruments. We have worked with two Tooka performers toenhance the original experimental device to make a musicalinstrument played and enjoyed by them. The main additions tothe device include: an additional button that behaves as amusic capture button, a bend sensor, an additional thumbactuated pressure sensor for vibrato, additional musicalmapping strategies, and new interfacing hardware. Thesedevelopments a rose through exper iences andrecommendations from the musicians playing it. In addition tothe changes to the Tooka, this paper describes the learningprocess and experiences of the musicians performing with theTooka.},
address = {Hamamatsu, Japan},
author = {Fels, Sidney S and Kaastra, Linda and Takahashi, Sachiyo and Mccaig, Graeme},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Fels et al. - 2004 - Evolving Tooka from Experiment to Instrument.pdf:pdf},
keywords = {Musician-centred design,two-person musical instrument.},
pages = {1--6},
title = {{Evolving Tooka: from Experiment to Instrument}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}001.pdf},
year = {2004}
}
@inproceedings{Lew2004,
address = {Hamamatsu, Japan},
author = {Lew, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {dj,film editing,live cinema,video controller,visual music,vj},
pages = {144--149},
title = {{Live Cinema: Designing an Instrument for Cinema Editing as a Live Performance}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}144.pdf},
year = {2004}
}
@inproceedings{Levin2005,
address = {Vancouver, BC, Canada},
author = {Levin, Golan},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {2--3},
title = {{A Personal Chronology of Audiovisual Systems Research}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}002.pdf},
year = {2005}
}
@inproceedings{Quessy2007,
address = {New York City, NY, United States},
author = {Quessy, Alexandre},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {433},
title = {{Human Sequencer}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}433.pdf},
year = {2007}
}
@inproceedings{Paradiso2001,
address = {Seattle, WA, United States},
author = {Paradiso, Joseph A and Hsiao, Kai-yuh and Benbasat, Ari},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
file = {::},
keywords = {eas tags,musical controller,resonant tags,rfid},
pages = {30--33},
title = {{Tangible Music Interfaces Using Passive Magnetic Tags}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}030.pdf},
year = {2001}
}
@inproceedings{Feehan2009,
abstract = {AUDIO ORIENTEERING is a collaborative performance environment in which physical tokens are used to navigate an invisible sonic landscape. In this paper, I describe the hardware and software used to implement a prototype audio terrain with multiple interaction modes and sonic behaviors mapped onto three-dimensional space. },
address = {Pittsburgh, PA, United States},
author = {Feehan, Noah},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {3-d positioning,audio terrain,collaborative performance.,wii},
pages = {161--162},
title = {{Audio Orienteering -- Navigating an Invisible Terrain}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}161.pdf},
year = {2009}
}
@inproceedings{Chang2007,
abstract = {FigureWe present Zstretch, a textile music controller that supports expressive haptic interactions. The musical controller takes advantage of the fabric's topological constraints to enable proportional control of musical parameters. This novel interface explores ways in which one might treat music as a sheet of cloth. This paper proposes an approach to engage simple technologies for supporting ordinary hand interactions. We show that this combination of basic technology with general tactile movements can result in an expressive musical interface. a},
address = {New York City, NY, United States},
author = {Chang, Angela and Ishii, Hiroshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Tangible interfaces,musical expressivity,tactile design,textiles},
pages = {46--49},
title = {{Zstretch : A Stretchy Fabric Music Controller}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}046.pdf},
year = {2007}
}
@inproceedings{nime2014:ofried,
abstract = {The modern musician enjoys access to a staggering number of audio samples. Composition software can ship with many gigabytes of data, and there are many more to be found online. However, conventional methods for navigating these libraries are still quite rudimentary, and often involve scrolling through alphabetical lists. We present a system for sample exploration that allows audio clips to be sorted according to user taste, and arranged in any desired 2D formation such that similar samples are located near each other. Our method relies on two advances in machine learning. First, metric learning allows the user to shape the audio feature space to match their own preferences. Second, kernelized sorting finds an optimal arrangement for the samples in 2D. We demonstrate our system with two new interfaces for exploring audio samples, and evaluate the technology qualitatively and quantitatively via a pair of user studies.},
address = {London, United Kingdom},
author = {Fried, Ohad and Jin, Zeyu and Oda, Reid and Finkelstein, Adam},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {281--286},
publisher = {Goldsmiths, University of London},
title = {{AudioQuilt: 2D Arrangements of Audio Samples using Metric Learning and Kernelized Sorting}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}315.pdf},
year = {2014}
}
@inproceedings{Comajuncosas2011,
address = {Oslo, Norway},
author = {Comajuncosas, Josep M and Barrachina, Alex and O'Connell, John and Guaus, Enric},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {3d,audio mosaicing,concatenative synthesis,face,gestural controller,musical instrument,open-air inter-},
number = {June},
pages = {252--255},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Nuvolet: 3D Gesture-driven Collaborative Audio Mosaicing}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}252.pdf},
year = {2011}
}
@inproceedings{Leeuw:2012,
abstract = {This short paper follows an earlier NIME paper [1] describing the invention and construction of the Electrumpet. Revisions and playing experience are both part of the current paper.
The Electrumpet can be heard in the performance given by Hans Leeuw and Diemo Schwarz at this NIME conference.},
address = {Ann Arbor, Michigan},
author = {Leeuw, Hans},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Electrumpet,NIME,hybrid instruments.,live-electronics},
publisher = {University of Michigan},
title = {{The electrumpet, additions and revisions}},
year = {2012}
}
@inproceedings{Angel2011,
address = {Oslo, Norway},
author = {Angel, Claudia R},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {biofeedback,butoh,interactive,live electronics,performance},
number = {June},
pages = {421--424},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Creating Interactive Multimedia Works with Bio-data}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}421.pdf},
year = {2011}
}
@inproceedings{Flety2003,
abstract = {Ircam has been deeply involved into gesture analysis and sensingfor about four years now, as several artistic projects demonstrate.Ircam has often been solicited for sharing software and hardwaretools for gesture sensing, especially devices for the acquisition andconversion of sensor data, such as the AtoMIC Pro [1][2]. Thisdemo-paper describes the recent design of a new sensor to MIDIinterface called EoBody1},
address = {Montreal},
author = {Fl{\'{e}}ty, Emmanuel and Sirguy, Marc},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
keywords = {Computer Music.,Gestural controller,MIDI,Sensor},
pages = {225--226},
title = {{EoBody : a Follow-up to AtoMIC Pro{\{}$\backslash$textquoteright{\}}s Technology}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}225.pdf},
year = {2003}
}
@inproceedings{Bullock2011,
address = {Oslo, Norway},
author = {Bullock, Jamie and Beattie, Daniel and Turner, Jerome},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {live electronics,software,usability,user experience},
number = {June},
pages = {387--392},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Integra Live : a New Graphical User Interface for Live Electronic Music}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}387.pdf},
year = {2011}
}
@inproceedings{Roberts:2013,
abstract = {We present research that extends the scope of the mobile application Control, aprototyping environment for defining multimodal interfaces that controlreal-time artistic and musical performances. Control allows users to rapidlycreate interfaces employing a variety of modalities, including: speechrecognition, computer vision, musical feature extraction, touchscreen widgets,and inertial sensor data. Information from these modalities can be transmittedwirelessly to remote applications. Interfaces are declared using JSON and canbe extended with JavaScript to add complex behaviors, including the concurrentfusion of multimodal signals. By simplifying the creation of interfaces viathese simple markup files, Control allows musicians and artists to make novelapplications that use and combine both discrete and continuous data from thewide range of sensors available on commodity mobile devices.},
address = {Daejeon, Republic of Korea},
author = {Roberts, Charles and Forbes, Angus and H{\"{o}}llerer, Tobias},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Music,interaction,mobile,multimodal},
pages = {102--105},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Enabling Multimodal Mobile Interfaces for Musical Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Place:2013,
abstract = {A two-page demonstration paper.Abstract:The AlphaSphere is an electronic musical instrument featuring a series oftactile, pressure sensitive touch pads arranged in a spherical form. It isdesigned to offer a new playing style, while allowing for the expressivereal-time modulation of sound available in electronic-based music. It is alsodesigned to be programmable, enabling the flexibility to map a series ofdifferent notational arrangements to the pad-based interface.The AlphaSphere functions as an HID, MIDI and OSC device, which connects to acomputer and/or independent MIDI device, and its control messages can be mappedthrough the AlphaLive software. Our primary motivations for creating theAlphaSphere are to design an expressive music interface which can exploit thesound palate of synthesizers in a design which allows for the mapping ofnotational arrangements.},
address = {Daejeon, Republic of Korea},
author = {Place, Adam and Lacey, Liam and Mitchell, Thomas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {AlphaSphere,HID,MIDI,open source,polyphonic aftertouch},
pages = {491--492},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{AlphaSphere}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Schoonderwaldt2011,
address = {Oslo, Norway},
author = {Schoonderwaldt, Erwin and Jensenius, Alexander R},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {bowing,clogging,feet,fiddler,french-canadian,kinematics,mo-,motiongram,sonification,tion capture,video,violin},
number = {June},
pages = {256--259},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Effective and Expressive Movements in a French-Canadian fiddler's Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}256.pdf},
year = {2011}
}
@inproceedings{Macrae2008,
address = {Genoa, Italy},
author = {Macrae, Robert and Dixon, Simon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Computer Game,Graphical Interface,MIDI Display},
pages = {364--365},
title = {{From Toy to Tutor : Note-Scroller is a Game to Teach Music}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}364.pdf},
year = {2008}
}
@inproceedings{Nakamoto2007,
address = {New York City, NY, United States},
author = {Nakamoto, Misako and Kuhara, Yasuo},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {chorus,circle canon,ensemble,frog round,internet,max,msp,mysql database,nime07,song},
pages = {409--410},
title = {{Circle Canon Chorus System Used To Enjoy A Musical Ensemble Singing "Frog Round"}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}409.pdf},
year = {2007}
}
@inproceedings{Engum2011,
address = {Oslo, Norway},
author = {Engum, Trond},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
number = {June},
pages = {519--522},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Real-time Control and Creative Convolution}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}519.pdf},
year = {2011}
}
@inproceedings{Flety:2002,
abstract = {Research and musical creation with gestural-oriented interfaces have recently seen a renewal of interest and activity at Ircam [1][2]. In the course of several musical projects, undertaken by young composers attending the one-year Course in Composition and Computer Music or by guests artists, Ircam Education and Creation departments have proposed various solutions for gesture-controlled sound synthesis and processing. In this article, we describe the technical aspects of AtoMIC Pro, an Analog to MIDI converter proposed as a re-usable solution for digitizing several sensors in different contexts such as interactive sound installation or virtual instruments.The main direction of our researches, and of this one in particular, is to create tools that can be fully integrated into an artistic project as a real part of the composition and performance processes.},
address = {Dublin},
author = {Fl{\'{e}}ty, Emmanuel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Gestural controller,MIDI,Music. Solution for Multi-sensor Acquisition,Sensor},
pages = {59--64},
title = {{AtoMIC Pro: a Multiple Sensor Acquisition Device}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}059.pdf},
year = {2002}
}
@inproceedings{McLean:2013,
abstract = {The Human vocal tract is considered for its sonorous qualities incarrying prosodic information, which implicates vision in theperceptual processes of speech. These considerations are put in thecontext of previous work in NIME, forming background for theintroduction of two sound installations; ``Microphone'', which uses acamera and computer vision to translate mouth shapes to sounds, and``Microphone II'', a work-in-progress, which adds physical modellingsynthesis as a sound source, and visualisation of mouth movements.},
address = {Daejeon, Republic of Korea},
author = {McLean, Alex and Shin, EunJoo and Ng, Kia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {computer vision,face tracking,installation,microphone},
pages = {381--384},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Paralinguistic Microphone}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Nort2007,
address = {New York City, NY, United States},
author = {{Van Nort}, Doug and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {control,mapping,musical gestures,nime07,sound texture},
pages = {379--383},
title = {{Control Strategies for Navigation of Complex Sonic Spaces Transformation of Resonant Models}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}379.pdf},
year = {2007}
}
@inproceedings{Kapur2004,
address = {Hamamatsu, Japan},
author = {Kapur, Ajay and Lazier, Ariel J and Davidson, Philip L and Wilson, Scott and Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {atmel microcontroller,controller,electronic sitar,esitar,human computer interface,indian string controller,instrument graphical feedback,midi,veldt},
pages = {7--12},
title = {{The Electronic Sitar Controller}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}007.pdf},
volume = {2},
year = {2004}
}
@inproceedings{Chew2005,
address = {Vancouver, BC, Canada},
author = {Chew, Elaine and Jensenius, Alexander R and Liu, Jie and Yang, Aaron},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {224--227},
title = {{ESP: A Driving Interface for Expression Synthesis}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}224.pdf},
year = {2005}
}
@inproceedings{Murray-Browne2011,
address = {Oslo, Norway},
author = {Murray-Browne, Tim and Mainstone, Di and Bryan-Kinns, Nick and Plumbley, Mark D},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {ators or engage audiences,beyond those within the,composed instrument,constraint,field,however,many fail to enrol,performance,performers beyond their cre-,transparency},
number = {June},
pages = {56--59},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{The Medium is the Message: Composing Instruments and Performing Mappings}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}056.pdf},
year = {2011}
}
@inproceedings{Martin2010,
address = {Sydney, Australia},
author = {Martin, Aengus and Ferguson, Sam and Beilharz, Kirsty},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {concatenative synthesis,feedback,guitar,nime10},
number = {Nime},
pages = {364--367},
title = {{Mechanisms for Controlling Complex Sound Sources : Applications to Guitar Feedback Control}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}364.pdf},
year = {2010}
}
@inproceedings{Raisanen2008,
address = {Genoa, Italy},
author = {R{\"{a}}is{\"{a}}nen, Juhani},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {digital musical instrument,gestural controller,music,nime08,usability},
pages = {57--60},
title = {{Sormina  a New Virtual and Tangible Instrument}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}057.pdf},
year = {2008}
}
@inproceedings{Steiner2004,
address = {Hamamatsu, Japan},
author = {Steiner, Hans-Christoph},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {gestural control,haptic feedback,joystick,mouse,performance},
pages = {203--204},
title = {{StickMusic: Using Haptic Feedback with a Phase Vocoder}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}203.pdf},
year = {2004}
}
@inproceedings{Ahola2011,
address = {Oslo, Norway},
author = {Ahola, Tom and Tahiroglu, Koray and Ahmaniemi, Teemu and Belloni, Fabio and Ranki, Ville},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {accelerometer,dance,gyro,motion sensor,performance,positioning,pure data,qt,raja,sonification,visualization},
number = {June},
pages = {433--436},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Raja - A Multidisciplinary Artistic Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}433.pdf},
year = {2011}
}
@inproceedings{Buch2010,
address = {Sydney, Australia},
author = {Buch, Benjamin and Coussement, Pieter and Schmidt, L{\"{u}}der},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {dynamic mapping,embodiment,finite state au-,human-robot interaction,new media art,nime10,structured,tomata},
number = {June},
pages = {15--18},
title = {{"playing robot" : An Interactive Sound Installation in Human-Robot Interaction Design for New Media Art}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}411.pdf},
year = {2010}
}
@inproceedings{Lee:2004,
abstract = {This paper describes the first system designed to allow children to conduct an audio and video recording of an orchestra. No prior music experience is required to control theorchestra, and the system uses an advanced algorithm totime stretch the audio in real-time at high quality and without altering the pitch. We will discuss the requirements andchallenges of designing an interface to target our particularuser group (children), followed by some system implementation details. An overview of the algorithm used for audiotime stretching will also be presented. We are currently using this technology to study and compare professional andnon-professional conducting behavior, and its implicationswhen designing new interfaces for multimedia. You're theConductor is currently a successful exhibit at the Children'sMuseum in Boston, USA.},
address = {Hamamatsu, Japan},
author = {Lee, Eric and Nakra, Teresa M and Borchers, Jan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {conducting systems,design patterns,gesture recogni-,interactive exhibits,real-time audio stretching,tion},
pages = {68--73},
title = {{You're The Conductor: A Realistic Interactive Conducting System for Children}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}068.pdf},
year = {2004}
}
@inproceedings{Jensenius2006,
address = {Paris, France},
author = {Jensenius, Alexander R and Chew, Elaine},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {interactive systems,music soft-,software architecture},
pages = {150--155},
title = {{An Architectural Framework for Interactive Music Systems}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}150.pdf},
year = {2006}
}
@inproceedings{Hazlewood2008,
address = {Genoa, Italy},
author = {Hazlewood, William R and Knopke, Ian},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {al-,ambient musical information systems,gorithmic composition,human computer interaction,markov chain,musical instruments,nime08,probability},
pages = {281--284},
title = {{Designing Ambient Musical Information Systems}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}281.pdf},
year = {2008}
}
@inproceedings{Magnusson2006,
abstract = {The ixi software project started in 2000 with the intention to explore new interactive patterns and virtual interfaces in computer music software. The aim of this paper is not to describe these programs, as they have been described elsewhere [14][15], but rather explicate the theoretical background that underlies the design of these screen-based instruments. After an analysis of the similarities and differences in the design of acoustic and screen-based instruments, the paper describes how the creation of an interface is essentially the creation of a semiotic system that affects and influences the musician and the composer. Finally the terminology of this semiotics is explained as an interaction model. },
address = {Paris, France},
author = {Magnusson, Thor},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {HCI,Interfaces,OSC,actors,creative tools.,interaction design,interaction models,mapping,semiotics},
pages = {162--167},
title = {{Screen-Based Musical Interfaces as Semiotic Machines}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}162.pdf},
year = {2006}
}
@inproceedings{Pelletier2004,
address = {Hamamatsu, Japan},
author = {Pelletier, Jean-Marc},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {computer vision,image analysis,maxmsp,morphology,musical},
pages = {197--198},
title = {{A Shape-Based Approach to Computer Vision Musical Performance Systems}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}197.pdf},
year = {2004}
}
@inproceedings{Hochenbaum:2013,
abstract = {Music education is a rich subject with many approaches and methodologies thathave developed over hundreds of years. More than ever, technology playsimportant roles at many levels of a musician's practice. This paper begins toexplore some of the ways in which technology developed out of the NIMEcommunity (specifically hyperinstruments), can inform a musician's dailypractice, through short and long term metrics tracking and data visualization.},
address = {Daejeon, Republic of Korea},
author = {Hochenbaum, Jordan and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Ezither,Hyperinstruments,Metrics,Pedagogy,Practice Room},
pages = {307--312},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Toward The Future Practice Room: Empowering Musical Pedagogy through Hyperinstruments}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Gatzsche2008,
address = {Genoa, Italy},
author = {Gatzsche, Gabriel and Mehnert, Markus and St{\"{o}}cklmeier, Christian},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {3d,3d tonality,carol l,hardware controller,krumhansl,model,music,music theory,musical interface,nime08,pitch space,psychology,spiral of thirds,symmetry model,western tonal music},
pages = {325--330},
title = {{Interaction with Tonal Pitch Spaces}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}325.pdf},
year = {2008}
}
@inproceedings{Luhtala2011,
address = {Oslo, Norway},
author = {Luhtala, Matti and Kym{\"{a}}l{\"{a}}inen, Tiina and Plomp, Johan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Human-Technology Interaction (HTI),Music interfaces,User-Centred Design (UCD),design for all (DfA),design tools,modifiable interfaces,music therapy,performance.,prototyping},
pages = {429--432},
title = {{Designing a Music Performance Space for Persons with Intellectual Learning Disabilities}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}429.pdf},
year = {2011}
}
@inproceedings{Woldecke2010,
address = {Sydney, Australia},
author = {W{\"{o}}ldecke, Bj{\"{o}}rn and Geiger, Christian and Reckter, Holger and Schulz, Florian},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {generative music,mobile interfaces,multitouch interaction,nime10},
number = {Nime},
pages = {348--351},
title = {{ANTracks 2.0 - Generative Music on Multiple Multitouch Devices Categories and Subject Descriptors}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}348.pdf},
year = {2010}
}
@inproceedings{Leider2009,
address = {Pittsburgh, PA, United States},
author = {Leider, Colby},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {333},
title = {{Afflux/Reflux}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}333.pdf},
year = {2009}
}
@inproceedings{Bokesoy2011,
address = {Oslo, Norway},
author = {B{\"{o}}kesoy, Sinan and Adler, Patrick},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {interactive systems,robotic music,sound installation},
number = {June},
pages = {52--55},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{1city1001vibrations : Development of a Interactive Sound Installation with Robotic Instrument Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}052.pdf},
year = {2011}
}
@inproceedings{Fuhrmann:2013,
abstract = {This paper demonstrates how to use multiple Kinect(TM) sensors to map aperformers motion to music. We merge skeleton data streams from multiplesensors to compensate for occlusions of the performer. The skeleton jointpositions drive the performance via open sound control data. We discuss how toregister the different sensors to each other and how to smoothly merge theresulting data streams and how to map position data in a general framework tothe live electronics applied to a chamber music ensemble.},
address = {Daejeon, Republic of Korea},
author = {Fuhrmann, Anton and Kretz, Johannes and Burwik, Peter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {kinect,live electronics,motion tracking,multi sensor,open sound control,parameter mapping,sensor fusion},
month = {may},
pages = {358--362},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Multi Sensor Tracking for Live Sound Transformation}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Schiesser2006,
address = {Paris, France},
author = {Schiesser, S{\'{e}}bastien and Traube, Caroline},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {augmented instrument,gestural control,live electronics,mance,perfor-,saxophone},
pages = {308--313},
title = {{On Making and Playing an Electronically-augmented Saxophone}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}308.pdf},
year = {2006}
}
@inproceedings{Socolofsky2005,
abstract = {Contemplace is a spatial personality that redesigns itselfdynamically according to its conversations with its visitors.Sometimes welcoming, sometimes shy, and sometimeshostile, Contemplace's mood is apparent through a display ofprojected graphics, spatial sound, and physical motion.Contemplace is an environment in which inhabitationbecomes a two-way dialogue.},
address = {Vancouver, BC, Canada},
author = {Socolofsky, Eric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Flosc,Interactive space,Processing,graphic and aural display,motion tracking,spatial installation},
pages = {271},
title = {{Contemplace}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}271.pdf},
year = {2005}
}
@inproceedings{Lemouton2006,
abstract = {This paper deals with the first musical usage of anexperimental system dedicated to the optical detection ofthe position of a trombone's slide.},
address = {Paris, France},
author = {Lemouton, Serge and Stroppa, Marco and Sluchin, Benny},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {1,augmented instrument,chamber electronics,computer,interaction,musical motivation,performer,trombone},
pages = {304--307},
title = {{Using the Augmented Trombone in "I will not kiss your f.ing flag"}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}304.pdf},
year = {2006}
}
@inproceedings{Kuyken2008,
address = {Genoa, Italy},
author = {Kuyken, Bart and Verstichel, Wouter and Bossuyt, Frederick and Vanfleteren, Jan and Demey, Michiel and Leman, Marc},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {digital musical instrument,inertial sensing,nime08,wireless sensors},
pages = {229--232},
title = {{The HOP Sensor : Wireless Motion Sensor}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}229.pdf},
year = {2008}
}
@inproceedings{Liang2011,
abstract = {Computer music systems that coordinate or interact with human musicians exist in many forms. Often, coordination is at the level of gestures and phrases without synchronization at the beat level (or perhaps the notion of "beat" does not even exist). In music with beats, fine-grain synchronization can be achieved by having humans adapt to the computer (e.g. following a click track), or by computer accompaniment in which the computer follows a predetermined score. We consider an alternative scenario in which improvisation prevents traditional score following, but where synchronization is achieved at the level of beats, measures, and cues. To explore this new type of human-computer interaction, we have created new software abstractions for synchronization and coordination of music and interfaces in different modalities. We describe these new software structures, present examples, and introduce the idea of music notation as an interactive musical interface rather than a static document. },
address = {Oslo, Norway},
author = {Liang, Dawen and Xia, Guangyu and Dannenberg, Roger B},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {automatic accompaniment,interactive,music display,popular music,real-time,synchronization},
pages = {167--172},
title = {{A Framework for Coordination and Synchronization of Media}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}167.pdf},
year = {2011}
}
@inproceedings{Biggs2007,
address = {New York City, NY, United States},
author = {Biggs, Betsey},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {424},
title = {{The Tipping Point}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}424.pdf},
year = {2007}
}
@inproceedings{Schiemer2006,
abstract = {This paper describes two new live performance scenarios for performing music using bluetooth-enabled mobile phones. Interaction between mobile phones via wireless link is a key feature of the performance interface for each scenario. Both scenarios are discussed in the context of two publicly performed works for an ensemble of players in which mobile phone handsets are used both as sound sources and as hand-held controllers. In both works mobile phones are mounted in a specially devised pouch attached to a cord and physically swung to produce audio chorusing. During performance some players swing phones while others operate phones as hand-held controllers. Wireless connectivity enables interaction between flying and hand-held phones. Each work features different bluetooth implementations. In one a dedicated mobile phone acts as a server that interconnects multiple clients, while in the other point to point communication takes place between clients on an ad hoc basis. The paper summarises bluetooth tools designed for live performance realisation and concludes with a comparative evaluation of both scenarios for future implementation of performance by large ensembles of nonexpert players performing microtonal music using ubiquitous technology. },
address = {Paris, France},
author = {Schiemer, Greg and Havryliv, Mark},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Java 2 Micro Edition; j2me; Pure Data; PD; Real-Ti},
pages = {37--42},
title = {{Pocket Gamelan: Tuneable Trajectories for Flying Sources in Mandala 3 and Mandala 4}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}037.pdf},
year = {2006}
}
@inproceedings{Snyder2011,
address = {Oslo, Norway},
author = {Snyder, Jeff},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {capacitive,controller,decoupled led,grid,hexagon,hid,live music,live video,manta,portable,sensor,snyderphonics,touch,touch slider,usb,wood},
number = {June},
pages = {413--416},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Snyderphonics Manta Controller, a Novel USB Touch-Controller}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}413.pdf},
year = {2011}
}
@inproceedings{Gerhard:2012,
abstract = {The Instant Instrument Anywhere (IIA) is a small device which can be attached to any metal object to create an electronic instrument. The device uses capacitive sensing to detect proximity of the player's body to the metal object, and sound is generated through a surface transducer which can be attached to any flat surface. Because the capacitive sensor can be any shape or size, absolute capacitive thresholding is not possible since the baseline capacitance will change. Instead, we use a differential-based moving sum threshold which can rapidly adjust to changes in the environment or be re-calibrated to a new metal object. We show that this dynamic threshold is effective in rejecting environmental noise and rapidly adapting to new objects. We also present details for constructing Instant Instruments Anywhere, including using smartphone as the synthesis engine and power supply.},
address = {Ann Arbor, Michigan},
author = {Gerhard, David and Park, Brett},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Arduino,Capacitive Sensing},
publisher = {University of Michigan},
title = {{Instant Instrument Anywhere: A Self-Contained Capacitive Synthesizer}},
year = {2012}
}
@inproceedings{Lee2007,
address = {New York City, NY, United States},
author = {Lee, Eric and Enke, Urs and Borchers, Jan and de Jong, Leo},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
doi = {10.1145/1279740.1279765},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {dance movement analysis,nime07,onset analysis,rhythm analysis},
pages = {136},
title = {{Towards Rhythmic Analysis of Human Motion Using Acceleration-Onset Times}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}136.pdf},
year = {2007}
}
@inproceedings{Godbehere2008,
abstract = {We present examples of a wireless sensor network as applied to wearable digital music controllers. Recent advances in wireless Personal Area Networks (PANs) have precipitated the IEEE 802.15.4 standard for low-power, low-cost wireless sensor networks. We have applied this new technology to create a fully wireless, wearable network of accelerometers which are small enough to be hidden under clothing. Various motion analysis and machine learning techniques are applied to the raw accelerometer data in real-time to generate and control music on the fly. },
address = {Genoa, Italy},
author = {Godbehere, Andrew B and Ward, Nathan J},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {802.15.4,Wearable computing,accelerometers,digital musical controllers,gestural control,human-computer interaction,live performance,motion analysis,personal area networks},
pages = {237--240},
title = {{Wearable Interfaces for Cyberphysical Musical Expression}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}237.pdf},
year = {2008}
}
@inproceedings{Machover:2002,
address = {Dublin},
author = {Machover, Tod},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
pages = {115},
title = {{Instruments, Interactivity, and Inevitability}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}115.pdf},
year = {2002}
}
@inproceedings{Jorda2006,
address = {Paris, France},
author = {Jord{\`{a}}, Sergi and Alonso, Marcos},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {collaborative music,collaborative work,computer supported,music theory,musical instrument,tangible user interface},
pages = {208--211},
title = {{Mary Had a Little scoreTable* or the reacTable* Goes Melodic}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}208.pdf},
year = {2006}
}
@inproceedings{Dimitrov2011,
address = {Oslo, Norway},
author = {Dimitrov, Smilen and Serafin, Stefania},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {alsa,arduino,audio,driver,linux,sound card},
number = {June},
pages = {211--216},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Audio Arduino - an ALSA (Advanced Linux Sound Architecture) Audio Driver for FTDI-based Arduinos}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}211.pdf},
year = {2011}
}
@inproceedings{Smith:2012a,
address = {Ann Arbor, Michigan},
author = {Smith, Benjamin D and Garnett, Guy E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
publisher = {University of Michigan},
title = {{Unsupervised Play: Machine Learning Toolkit for Max}},
year = {2012}
}
@inproceedings{Maniatakos2008,
address = {Genoa, Italy},
author = {Maniatakos, Vassilios-Fivos A and Jacquemin, Christian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {HMM,affective computing,affective interface,gesture recognition,intelligent mapping,interactive performance},
pages = {122--127},
title = {{Towards an Affective Gesture Interface for Expressive Music Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}122.pdf},
year = {2008}
}
@inproceedings{Burt2010,
address = {Sydney, Australia},
author = {Burt, Warren},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {nime10},
number = {June},
pages = {15--18},
title = {{Packages for ArtWonk : New Mathematical Tools for Composers}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}493.pdf},
year = {2010}
}
@inproceedings{Diakopoulos:2013,
abstract = {Netpixl is a new micro-toolkit built to network devices within interactiveinstallations and environments. Using a familiar client-server model, Netpixlcentrally wraps an important aspect of ubiquitous computing: real-timemessaging. In the context of sound and music computing, the role of Netpixl isto fluidly integrate endpoints like OSC and MIDI within a larger multi-usersystem. This paper considers useful design principles that may be applied totoolkits like Netpixl while also emphasizing recent approaches to applicationdevelopment via HTML5 and Javascript, highlighting an evolution in networkedcreative computing.},
address = {Daejeon, Republic of Korea},
author = {Diakopoulos, Dimitri and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {html5,networking,toolkits,ubiquitious computing},
pages = {206--209},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Netpixl: Towards a New Paradigm for Networked Application Development}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Ciglar2008,
address = {Genoa, Italy},
author = {Ciglar, Miha},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {1,dancer,fig,from the system in,gesture recognition,haptic feedback,in,markers attached to the,motion tracking,nime08,s limbs,the dancer receives feedback,two ways},
pages = {203--206},
title = {{"3rd. Pole" -- A Composition Performed via Gestural Cues}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}203.pdf},
year = {2008}
}
@inproceedings{McMillen2008,
abstract = {The demonstration of a series of properly weighted and balanced Bluetooth sensor bows for violin, viola, cello and bass. },
address = {Genoa, Italy},
author = {McMillen, Keith A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Sensor bow,bluetooth,stringed instruments},
pages = {347--348},
title = {{Stage-Worthy Sensor Bows for Stringed Instruments}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}347.pdf},
year = {2008}
}
@inproceedings{StClair2009,
address = {Pittsburgh, PA, United States},
author = {{St. Clair}, Michael and Leitman, Sasha},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {expression,in doing so,installation,interactive,it also opens up,music,new territories for,nime09,play,playful physical motions into,playground,radical collaboration,real-time,the realm of artistic},
pages = {293--296},
title = {{PlaySoundGround : An Interactive Musical Playground}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}293.pdf},
year = {2009}
}
@inproceedings{Johnson2006,
address = {Paris, France},
author = {Johnson, Colin G and Gounaropoulos, Alex},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {natural language,neural networks,timbre},
pages = {101--102},
title = {{Timbre Interfaces using Adjectives and Adverbs}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}101.pdf},
year = {2006}
}
@inproceedings{Franco2005,
abstract = {This paper describes the development of AirStick, an interface for musical expression. AirStick is played "in the air", in a Theremin style. It is composed of an array of infrared proximity sensors, which allow the mapping of the position of any interfering obstacle inside a bi-dimensional zone. This controller sends both x and y control data to various real-time synthesis algorithms. },
address = {Vancouver, BC, Canada},
author = {Franco, Ivan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Computer Music.,Infrared Sensing,Music Controller},
pages = {248--249},
title = {{The Airstick: A Free-Gesture Controller Using Infrared Sensing}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}248.pdf},
year = {2005}
}
@inproceedings{Ben-Asher:2013,
abstract = {A system is presented for detecting common gestures, musical intentions andemotions of pianists in real-time using only kinesthetic data retrieved bywireless motion sensors. The algorithm can detect common Western musicalstructures such as chords, arpeggios, scales, and trills as well as musicallyintended emotions: cheerful, mournful, vigorous, dreamy, lyrical, and humorouscompletely and solely based on low-sample-rate motion sensor data. Thealgorithm can be trained per performer in real-time or can work based onprevious training sets. The system maps the emotions to a color set andpresents them as a flowing emotional spectrum on the background of a pianoroll. This acts as a feedback mechanism for emotional expression as well as aninteractive display of the music. The system was trained and tested on a numberof pianists and it classified structures and emotions with promising results ofup to 92{\%} accuracy.},
address = {Daejeon, Republic of Korea},
author = {Ben-Asher, Matan and Leider, Colby},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Computer Music,Expressive Piano Performance,IMUs,Machine Learning,Motion Sensors,Music and Emotion},
pages = {21--24},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Toward an Emotionally Intelligent Piano: Real-Time Emotion Detection and Performer Feedback via Kinesthetic Sensing in Piano Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Jorda2001,
address = {Seattle, WA},
author = {Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
pages = {46--50},
title = {{New Musical Interfaces and New Music-making Paradigms}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}046.pdf},
year = {2001}
}
@inproceedings{Gerhard2004,
address = {Hamamatsu, Japan},
author = {Gerhard, David and Hepting, Daryl and Mckague, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {alization,multimedia creation and interaction,parameter space,sonification,visu-},
pages = {96--99},
title = {{Exploration of the Correspondence between Visual and Acoustic Parameter Spaces}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}096.pdf},
year = {2004}
}
@inproceedings{nime2014:cgnegy1,
abstract = {CollideFx is a real-time audio effects processor that integrates the physics of real objects into the parameter space of the signal chain. Much like a traditional signal chain, the user can choose a series of effects and offer realtime control to their various parameters. In this work, we introduce a means of creating tree-like signal graphs that dynamically change their routing in response to changes in the location of the unit generators in a virtual space. Signals are rerouted using a crossfading scheme that avoids the harsh clicks and pops associated with amplitude discontinuities. The unit generators are easily controllable using a click and drag interface that responds using familiar physics. CollideFx brings the interactivity of a video game together with the purpose of creating interesting and complex audio effects. With little difficulty, users can craft custom effects, or alternatively, can fling a unit generator into a cluster of several others to obtain more surprising results, letting the physics engine do the decision making.},
address = {London, United Kingdom},
author = {Gnegy, Chet},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {427--430},
publisher = {Goldsmiths, University of London},
title = {{CollideFx: A Physics-Based Audio Effects Processor}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}308.pdf},
year = {2014}
}
@inproceedings{Schiemer2006,
address = {Paris, France},
author = {Schiemer, Greg and Havryliv, Mark},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
pages = {37--42},
title = {{Pocket Gamelan: Tuneable Trajectories for Flying Sources in Mandala 3 and Mandala 4}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}037.pdf},
year = {2006}
}
@inproceedings{Hochenbaum2010a,
address = {Sydney, Australia},
author = {Hochenbaum, Jordan and Vallis, Owen and Diakopoulos, Dimitri and Murphy, Jim and Kapur, Ajay},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {bricktable,computing,generative music,increasingly possible for musicians,interface,multi-touch interface,music information retrieval,nime10,surfaces such as bricktable,tangible,the realization of,this combination has made},
number = {Nime},
pages = {315--318},
title = {{Designing Expressive Musical Interfaces for Tabletop Surfaces}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}315.pdf},
year = {2010}
}
@inproceedings{Geiger2006,
address = {Paris, France},
author = {Geiger, G{\"{u}}nter},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
keywords = {controller,human computer interaction,instrument,mobile musical,pda,pure data,touch screen},
pages = {61--64},
title = {{Using the Touch Screen as a Controller for Portable Computer Music Instruments}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}061.pdf},
year = {2006}
}
@inproceedings{nime2014:kyerkes,
abstract = {Twkyr is a new interface for musical expression that emphasizes realtime manipulation, audification, and visualization of waveforms with a multitouch surface, offering different interactivity at different time scales, within the same waveform. The interactive audiovisual design of Tweakyr is motivated by the need for increased parsimony and transparency in electronic musical instruments and draws from the work of Curtis Roads on time scales as qualitative musical parameters, and Edward Tufte's ``data-ink'' principles for the improvement of data graphics.},
address = {London, United Kingdom},
author = {Yerkes, Karl and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {375--378},
publisher = {Goldsmiths, University of London},
title = {{Twkyr: a Multitouch Waveform Looper}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}508.pdf},
year = {2014}
}
@inproceedings{Sarkar2007,
address = {New York City, NY, United States},
author = {Sarkar, Mihir and Vercoe, Barry},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {indian percussions,network music performance,nime07,oration,real-time online musical collab-,strokes recognition,tabla bols},
pages = {317--320},
title = {{Recognition and Prediction in a Network Music Performance System for Indian Percussion}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}317.pdf},
volume = {2},
year = {2007}
}
@inproceedings{Collicutt2009,
address = {Pittsburgh, PA, United States},
author = {Collicutt, Mike and Casciato, Carmine and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {a case study in,buchla lightning ii,devices were selected for,evaluation of input devices,four instruments,in this paper,motion capture,nime09,on percussion tasks,radio baton,we decided to focus},
number = {Idmil},
pages = {1--6},
title = {{From Real to Virtual : A Comparison of Input Devices for Percussion Tasks}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}001.pdf},
year = {2009}
}
@inproceedings{lvarez-Fernndez2007,
address = {New York City, NY, United States},
author = {{\'{A}}lvarez-Fern{\'{a}}ndez, Miguel and Kersten, Stefan and Piascik, Asia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {432},
title = {{Soundanism}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}432.pdf},
year = {2007}
}
@inproceedings{Everman:2013,
abstract = {Few formal methods exist for evaluating digital musical instruments (DMIs) .This paper proposes a novel method of DMI evaluation using crowd-sourcedtagging. One of the challenges in devising such methods is that the evaluationof a musical instrument is an inherently qualitative task. While previouslyproposed methods have focused on quantitative methods and largely ignored thequalitative aspects of the task, tagging is well-suited to this and is alreadyused to classify things such as websites and musical genres. These, like DMIs,do not lend themselves to simple categorization or parameterization. Using the social tagging method, participating individuals assign descriptivelabels, or tags, to a DMI. A DMI can then be evaluated by analyzing the tagsassociated with it. Metrics can be generated from the tags assigned to theinstrument, and comparisons made to other instruments. This can give thedesigner valuable insight into the where the strengths of the design lie andwhere improvements may be needed. A prototype system for testing the method is proposed in the paper and iscurrently being implemented as part of an ongoing DMI evaluation project. It isexpected that results from the prototype will be available to report by thetime of the conference in May.},
address = {Daejeon, Republic of Korea},
author = {Everman, Michael and Leider, Colby},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Evaluation,digital musical instrument,evaluation,tagging},
mendeley-tags = {evaluation},
pages = {437--440},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Toward DMI Evaluation Using Crowd-Sourced Tagging Techniques}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Couturier2003,
abstract = {In this paper, we describe a new interface for musicalperformance, using the interaction with a graphical userinterface in a powerful manner: the user directly touches ascreen where graphical objects are displayed and can useseveral fingers simultaneously to interact with the objects. Theconcept of this interface is based on the superposition of thegesture spatial place and the visual feedback spatial place; i tgives the impression that the graphical objects are real. Thisconcept enables a huge freedom in designing interfaces. Thegesture device we have created gives the position of fourfingertips using 3D sensors and the data is performed in theMax/MSP environment. We have realized two practicalexamples of musical use of such a device, using PhotosonicSynthesis and Scanned Synthesis.},
address = {Montreal},
author = {Couturier, Jean-Michel and Arfib, Daniel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
keywords = {HCI,Max/MSP.,bimanual interaction,direct interaction,gesture devices,mapping,multimodality,touch screen,two-handed},
pages = {184--187},
title = {{Pointing Fingers: Using Multiple Direct Interactions with Visual Objects to Perform Music}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}184.pdf},
year = {2003}
}
@inproceedings{Todoroff2011,
address = {Oslo, Norway},
author = {Todoroff, Todor},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {wireless marg sensors},
number = {June},
pages = {515--518},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Wireless Digital/Analog Sensors for Music and Dance Performances}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}515.pdf},
year = {2011}
}
@inproceedings{Johnson:2013,
abstract = {This paper presents a new technique for interface-driven diffusion performance.Details outlining the development of a new tabletop surface-based performanceinterface, named tactile.space, are discussed. User interface and amplitudepanning processes employed in the creation of tactile.space are focused upon,and are followed by a user study-based evaluation of the interface. It is hopedthat the techniques described in this paper afford performers and composers anenhanced level of creative expression in the diffusion performance practice.This paper introduces and evaluates tactile.space, a multi-touch performanceinterface for diffusion built on the BrickTable. It describes how tactile.spaceimplements Vector Base Amplitude Panning to achieve real- time sourcepositioning. The final section of this paper presents the findings of a userstudy that was conducted by those who performed with the interface, evaluatingthe interface as a performance tool with a focus on the increased creativeexpression the interface affords, and directly comparing it to the traditionaldiffusion user interface.},
address = {Daejeon, Republic of Korea},
author = {Johnson, Bridget and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Multi touch,VBAP,diffusion,tabletop surface},
pages = {213--216},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{MULTI-TOUCH INTERFACES FOR PHANTOM SOURCE POSITIONING IN LIVE SOUND DIFFUSION}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Follmer2008,
address = {Genoa, Italy},
author = {Follmer, Sean and Warren, Chris and Marquez-Borbon, Adnan},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {nime08},
pages = {354--355},
title = {{The Pond : Interactive Multimedia Installation}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}354.pdf},
year = {2008}
}
@inproceedings{nime2014:jdeng,
abstract = {Music jamming is an extremely difficult task for musical novices. Trying to extend this meaningful activity, which can be highly enjoyable, to a larger recipient group, we present WIJAM, a mobile application for an ad-hoc group of musical novices to perform improvisation along with a music master. In this ``master-players" paradigm, the master offers a music backing, orchestrates the musical flow, and gives feedbacks to the players; the players improvise by tapping and sketching on their smartphones. We believe that this paradigm can be a significant contribution to the possibility of music playing by a group of novices with no instrumental training leading to decent musical results.},
address = {London, United Kingdom},
author = {Deng, Jun-qi and Lau, Francis Chi Moon and Ng, Ho-Cheung and Kwok, Yu-Kwong and Chen, Hung-Kwan and Liu, Yu-heng},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {407--410},
publisher = {Goldsmiths, University of London},
title = {{WIJAM: A Mobile Collaborative Improvisation Platform under Master-players Paradigm}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}284.pdf},
year = {2014}
}
@inproceedings{Francois2007,
address = {New York City, NY, United States},
author = {Fran{\c{c}}ois, Alexandre R and Chew, Elaine and Thurmond, Dennis},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {machine,nime07,performer-machine interaction,visualization design},
pages = {277--280},
title = {{Visual Feedback in Performer-Machine Interaction for Musical Improvisation}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}277.pdf},
year = {2007}
}
@inproceedings{KimBoyle2008,
abstract = {The rapid development of network communicationtechnologies has allowed composers to create new ways inwhich to directly engage participants in the exploration of newmusical environments. A number of distinctive aestheticapproaches to the musical application of networks will beoutlined in this paper each of which is mediated andconditioned by the technical and aesthetic foundations of thenetwork technologies themselves. Recent work in the field byartists such as Atau Tanaka and Metraform will be examined, aswill some of the earlier pioneering work in the genre by MaxNeuhaus. While recognizing the historical context ofcollaborative work, the ,
,
author will examine how the strategiesemployed in the work of these artists have helped redefine anew aesthetics of engagement in which play, spatial andtemporal dislocation are amongst the genre's definingcharacteristics.},
address = {Genoa, Italy},
author = {Kim-Boyle, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Networks,collaborative,interface.,open-form,play},
pages = {3--8},
title = {{Network Musics - Play , Engagement and the Democratization of Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}003.pdf},
year = {2008}
}
@inproceedings{Nilson2007,
address = {New York City, NY, United States},
author = {Nilson, Click},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
doi = {10.1145/1279740.1279760},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {live coding,nime07,practice,practising},
pages = {112--117},
title = {{Live Coding Practice}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}112.pdf},
year = {2007}
}
@inproceedings{Modler2003,
address = {Montreal, QC, Canada},
author = {Modler, Paul and Myatt, Tony and Saup, Michael},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {146--150},
title = {{An Experimental Set of Hand Gestures for Expressive Control of Musical Parameters in Realtime}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}146.pdf},
year = {2003}
}
@inproceedings{McKinney:2013,
abstract = {In this paper we present Shoggoth, a 3D graphics based program for performingnetwork music. In Shoggoth, users utilize video game style controls to navigateand manipulate a grid of malleable height maps. Sequences can be created bydefining paths through the maps which trigger and modulate audio playback. Withrespect to a context of computer music performance, and specific problems innetwork music, design goals and technical challenges are outlined. The systemis evaluated through established taxonomies for describing interfaces, followedby an enumeration of the merits of 3D graphics in networked performance. Indiscussing proposed improvements to Shoggoth, design suggestions for otherdevelopers and network musicians are drawn out.},
address = {Daejeon, Republic of Korea},
author = {McKinney, Chad and Collins, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {3D,Environment,Generative,Network},
pages = {400--405},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{An Interactive 3D Network Music Space}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Ryan2003,
abstract = {This report details work on the interdisciplinary mediaproject TGarden. The authors discuss the challengesencountered while developing a responsive musicalenvironment for the general public involving wearable,sensor-integrated clothing as the central interface and input device. The project's dramaturgical andtechnical/implementation background are detailed toprovide a framework for the creation of a responsive hardwareand software system that reinforces a tangible relationshipbetween the participant's improvised movement and musicalresponse. Finally, the authors take into consideration testingscenarios gathered from public prototypes in two Europeanlocales in 2001 to evaluate user experience of the system.},
address = {Montreal},
author = {Ryan, Joel and Salter, Christopher L},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
keywords = {Gesture,accelerometer,affordance,audience participation,augmented physicality.,dynamic compliance,dynamical system,effort,embodied action,enaction,energy,induced ballistics,interaction,interactive musical systems,interface,kinetics,phenomenology,physical model,responsive environment,time constant,wearable computing,wearable instrument},
pages = {87--90},
title = {{TGarden: Wearable Instruments and Augmented Physicality}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}087.pdf},
year = {2003}
}
@inproceedings{Molina2011,
address = {Oslo, Norway},
author = {Molina, Pablo and Haro, Mart{\'{i}}n and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {audio mosaicing,beat-mash,dj,interactive music interfaces,music information retrieval,percus-,reactable,real-,sion,tabletop interaction,time,turntable},
number = {June},
pages = {288--291},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{BeatJockey : A New Tool for Enhancing DJ Skills}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}288.pdf},
year = {2011}
}
@inproceedings{Altavilla2013,
abstract = {We present a study that explores the affordance evoked by sound andsound-gesture mappings. In order to do this, we make use of a sensor systemwith minimal form factor in a user study that minimizes cultural associationThe present study focuses on understanding how participants describe sounds andgestures produced while playing designed sonic interaction mappings. Thisapproach seeks to move from object-centric affordance towards investigatingembodied gestural sonic affordances.},
address = {Daejeon},
author = {Altavilla, Alessandro and Caramiaux, Baptiste and Tanaka, Atau},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Affordances,Gestural embodiment of sound,Mapping},
pages = {61--64},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Towards Gestural Sonic Affordances}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Endo2008,
abstract = {We developed a rhythmic instruments ensemble simulator generating animation using game controllers. The motion of a player is transformed into musical expression data of MIDI to generate sounds, and MIDI data are transformed into animation control parameters to generate movies. These animations and music are shown as the reflection of player performance. Multiple players can perform a musical ensemble to make more varied patterns of animation. Our system is so easy that everyone can enjoy performing a fusion of music and animation. },
address = {Genoa, Italy},
author = {Endo, Ayaka and Kuhara, Yasuo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Flash movie,Gesture music and animation.,MIDI,Max/MSP,Wii Remote,Wireless game controller},
pages = {345--346},
title = {{Rhythmic Instruments Ensemble Simulator Generating Animation Movies Using {\{}Bluetooth{\}} Game Controller}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}345.pdf},
year = {2008}
}
@inproceedings{Bardos2005,
address = {Vancouver, BC, Canada},
author = {Bardos, Laszlo and Korinek, Stefan and Lee, Eric and Borchers, Jan},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {180--183},
title = {{Bangarama: Creating Music With Headbanging}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}180.pdf},
year = {2005}
}
@inproceedings{Wessel2001,
address = {Seattle, WA, United States},
author = {Wessel, David and Wright, Matthew},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
file = {::},
keywords = {communications protocols,gestural controllers,latency,musical,reactive computing,signal processing},
pages = {11--14},
title = {{Problems and Prospects for Intimate Musical Control of Computers}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}011.pdf},
year = {2001}
}
@inproceedings{Freed:2013,
abstract = {We describe ``o.expr'' an expression language for dynamic, object- andagent-oriented computation of gesture signal processing workflows using OSCbundles. We illustrate the use of o.expr for a range of gesture processingtasks showing how stateless programming and homoiconicity simplify applicationsdevelopment and provide support for heterogeneous computational networks.},
address = {Daejeon, Republic of Korea},
author = {Freed, Adrian and MacCallum, John and Wessel, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Functional Programming,Gesture Signal Processing,Homoiconicity,Open Sound Control,Process Migration.},
pages = {347--351},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Agile Interface Development using OSC Expressions and Process Migration}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Freed:2013a,
abstract = {This paper positively addresses the problem that most NIME devices are ephemeralasting long enough to signal academic and technical prowess but rarely longerthan a few musical performances. We offer a case study that shows thatlongevity of use depends on stabilizing the interface and innovating theimplementation to maintain the required performance of the controller for theplayer.},
address = {Daejeon, Republic of Korea},
author = {Freed, Adrian and MacCallum, John and Mansfield, Sam},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Best practices,Fingerboard controller,Organology,Recrudescence,Unobtainium},
pages = {441--445},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{``Old'' is the New ``New'': a Fingerboard Case Study in Recrudescence as a NIME Development Strategy}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Young2004,
address = {Hamamatsu, Japan},
author = {Young, Diana and Fujinaga, Ichiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {bluetooth,drum stick,japanese drum,taiko,wireless},
pages = {23--26},
title = {{AoBachi: A New Interface for Japanese Drumming}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}023.pdf},
year = {2004}
}
@inproceedings{Bragg:2013,
abstract = {This paper presents a graph-theoretic model that supports the design andanalysis of data flow within digital musical instruments (DMIs). The state ofthe art in DMI design fails to provide any standards for the scheduling ofcomputations within a DMI's data flow. It does not provide a theoreticalframework within which we can analyze different scheduling protocols and theirimpact on the DMI's performance. Indeed, the mapping between the DMI's sensoryinputs and sonic outputs is classically treated as a black box. DMI designersand builders are forced to design and schedule the flow of data through thisblack box on their own. Improper design of the data flow can produceundesirable results, ranging from overflowing buffers that cause system crashesto misaligned sensory data that result in strange or disordered sonic events.In this paper, we attempt to remedy this problem by providing a framework forthe design and analysis of the DMI data flow. We also provide a schedulingalgorithm built upon that framework that guarantees desirable properties forthe resulting DMI.},
address = {Daejeon, Republic of Korea},
author = {Bragg, Danielle and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {DMI design,data flow,mapping function},
pages = {237--242},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Synchronous Data Flow Modeling for DMIs}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Bennett2006,
abstract = {The PETECUBE project consists of a series of musical interfaces designed to explore multi-modal feedback. This paper will briefly describe the definition of multimodal feedback, the aim of the project, the development of the first PETECUBE and proposed further work. },
address = {Paris, France},
author = {Bennett, Peter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Multi-modal Feedback. Haptics. Musical Instrument.},
pages = {81--84},
title = {{{\{}PET{\}}ECUBE: a Multimodal Feedback Interface}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}081.pdf},
year = {2006}
}
@inproceedings{Poepel2006,
abstract = {In this paper, some of the more recent developments in musical instruments related to the violin family are described, and analyzed according to several criteria adapted from other publications. While it is impossible to cover all such developments, we have tried to sample a variety of instruments from the last decade or so, with a greater focus on those published in the computer music literature. Experiences in the field of string players focusing on such developments are presented. Conclusions are drawn in which further research into violin-related digital instruments for string players may benefit from the presented criteria as well as the experiences. },
address = {Paris, France},
author = {Poepel, Cornelius and Overholt, Dan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Violin,bass,cello,controller.,digital,electronic,synthesis,viola},
pages = {390--395},
title = {{Recent Developments in Violin-related Digital Musical Instruments: Where Are We and Where Are We Going?}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}390.pdf},
year = {2006}
}
@inproceedings{Melo2005,
address = {Vancouver, BC, Canada},
author = {Melo, Mauricio and Fan, Doria},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {262--263},
title = {{Swayway - Midi Chimes}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}262.pdf},
year = {2005}
}
@inproceedings{Essl2009,
address = {Pittsburgh, PA, United States},
author = {Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {mobile phone instruments,nime,nime09,on-the-fly},
pages = {270--273},
title = {{SpeedDial : Rapid and On-The-Fly Mapping of Mobile Phone Instruments}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}270.pdf},
year = {2009}
}
@inproceedings{El-Shimy:2012,
abstract = {In this paper, we discuss the design and testing of a reactive envi- ronment for musical performance. Driven by the interpersonal in- teractions amongst musicians, our system gives users, i.e., several musicians playing together in a band, real-time control over certain aspects of their performance, enabling them to change volume lev- els dynamically simply by moving around. It differs most notably from the majority of ventures into the design of novel musical in- terfaces and installations in its multidisciplinary approach, draw- ing on techniques from Human-Computer Interaction, social sci- ences and ludology. Our User-Centered Design methodology was central to producing an interactive environment that enhances tra- ditional performance with novel functionalities. During a formal experiment, musicians reported finding our system exciting and enjoyable. We also introduce some additional interactions that can further enhance the interactivity of our reactive environment. In describing the particular challenges of working with such a unique and creative user as the musician, we hope that our approach can be of guidance to interface developers working on applications of a creative nature.},
address = {Ann Arbor, Michigan},
author = {El-Shimy, Dalia and Hermann, Thomas and Cooperstock, Jeremy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
publisher = {University of Michigan},
title = {{A Reactive Environment for Dynamic Volume Control}},
year = {2012}
}
@inproceedings{Kobori2006,
address = {Paris, France},
author = {Kobori, Daisuke and Kagawa, Kojiro and Iida, Makoto and Arakawa, Chuichi},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
pages = {110--113},
title = {{LINE: Interactive Sound and Light Installation}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}110.pdf},
volume = {87},
year = {2006}
}
@inproceedings{Dubrau2008,
address = {Genoa, Italy},
author = {Dubrau, Josh and Havryliv, Mark},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Freud,Poetry,language sonification,linguistics,psychoanalysis,realtime poetry.},
pages = {164--167},
title = {{P[a]ra[pra]xis : Poetry in Motion}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}164.pdf},
year = {2008}
}
@inproceedings{Muth2003,
address = {Montreal, QC, Canada},
author = {Muth, David and Burton, Ed},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
pages = {222--224},
title = {{Sodaconductor}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}222.pdf},
year = {2003}
}
@inproceedings{Fried:2013,
abstract = {We present a method for automatic feature extraction and cross-modal mappingusing deep learning. Our system uses stacked autoencoders to learn a layeredfeature representation of the data. Feature vectors from two (or more)different domains are mapped to each other, effectively creating a cross-modalmapping. Our system can either run fully unsupervised, or it can use high-levellabeling to fine-tune the mapping according a user's needs. We show severalapplications for our method, mapping sound to or from images or gestures. Weevaluate system performance both in standalone inference tasks and incross-modal mappings.},
address = {Daejeon, Republic of Korea},
author = {Fried, Ohad and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Deep learning,feature learning,gestural control,mapping},
month = {may},
pages = {531--534},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Cross-modal Sound Mapping Using Deep Learning}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Everman:2013,
abstract = {Few formal methods exist for evaluating digital musical instruments (DMIs) .This paper proposes a novel method of DMI evaluation using crowd-sourcedtagging. One of the challenges in devising such methods is that the evaluationof a musical instrument is an inherently qualitative task. While previouslyproposed methods have focused on quantitative methods and largely ignored thequalitative aspects of the task, tagging is well-suited to this and is alreadyused to classify things such as websites and musical genres. These, like DMIs,do not lend themselves to simple categorization or parameterization. Using the social tagging method, participating individuals assign descriptivelabels, or tags, to a DMI. A DMI can then be evaluated by analyzing the tagsassociated with it. Metrics can be generated from the tags assigned to theinstrument, and comparisons made to other instruments. This can give thedesigner valuable insight into the where the strengths of the design lie andwhere improvements may be needed. A prototype system for testing the method is proposed in the paper and iscurrently being implemented as part of an ongoing DMI evaluation project. It isexpected that results from the prototype will be available to report by thetime of the conference in May.},
address = {Daejeon, Republic of Korea},
author = {Everman, Michael and Leider, Colby},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Evaluation,digital musical instrument,evaluation,tagging},
mendeley-tags = {evaluation},
pages = {437--440},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Toward DMI Evaluation Using Crowd-Sourced Tagging Techniques}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Jorda2002,
address = {Dublin, Ireland},
author = {Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {multimedia interaction,musical robots,real-time musi-},
pages = {102--107},
title = {{Afasia: the Ultimate Homeric One-man-multimedia-band}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}102.pdf},
year = {2002}
}
@inproceedings{Maki-patola2005,
address = {Vancouver, BC, Canada},
author = {M{\"{a}}ki-patola, Teemu},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
number = {1},
pages = {144--147},
title = {{User Interface Comparison for Virtual Drums}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}144.pdf},
year = {2005}
}
@inproceedings{Wessel2009,
address = {Pittsburgh, PA, United States},
author = {Wessel, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {335},
title = {{Hands On - A New Work from SLABS Controller and Generative Algorithms}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}335.pdf},
year = {2009}
}
@inproceedings{Jaimovich:2012,
abstract = {In order to further understand our emotional reaction to music, a museum-based installation was designed to collect physiological and self-report data from people listening to music. This demo will describe the technical implementation of this installation as a tool for collecting large samples of data in public spaces. The Emotion in Motion terminal is built upon a standard desktop computer running Max/MSP and using sensors that measure physiological indicators of emotion that are connected to an Arduino. The terminal has been installed in museums and galleries in Europe and the USA, helping create the largest database of physiology and self-report data while listening to music.},
address = {Ann Arbor, Michigan},
author = {Jaimovich, Javier and Ortiz, Miguel and Coghlan, Niall and Knapp, R Benjamin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Biosignals,Database,EDA,FTM,GEMS,GSR,HR,Max/MSP,POX,Physiological Signals,SAM,SC,Self-Report},
publisher = {University of Michigan},
title = {{The Emotion in Motion Experiment: Using an Interactive Installation as a Means for Understanding Emotional Response to Music}},
year = {2012}
}
@inproceedings{Young2002,
address = {Dublin, Ireland},
author = {Young, Diana},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {3,4,5,6,7,bossa,cello and subsequent hyperinstruments,i,later,media lab,princeton,projects such as the,t,these endeavors used mea-,were developed,work at the m},
pages = {201--206},
title = {{The Hyperbow Controller: Real-Time Dynamics Measurement of Violin Performance}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}201.pdf},
year = {2002}
}
@inproceedings{Jacobs2008,
address = {Genoa, Italy},
author = {Jacobs, Robert and Feldmeier, Mark and Paradiso, Joseph A},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {nime08},
pages = {193--196},
title = {{A Mobile Music Environment Using a PD Compiler and Wireless Sensors}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}193.pdf},
year = {2008}
}
@inproceedings{Gillian:2012,
abstract = {This paper presents Digito, a gesturally controlled virtual musical instrument. Digito is controlled through a number of intricate hand gestures, providing both discrete and continuous control of Digito's sound engine; with the fine-grain hand gestures captured by a 3D depth sensor and recognized using computer vision and machine learning algorithms. We describe the design and initial iterative development of Digito, the hand and finger tracking algorithms and gesture recognition algorithms that drive the system, and report the insights gained during the initial development cycles and user testing of this gesturally controlled virtual musical instrument.},
address = {Ann Arbor, Michigan},
author = {Gillian, Nicholas and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Gesture Recognition,Virtual Musical Instrument},
publisher = {University of Michigan},
title = {{Digito: A Fine-Grain Gesturally Controlled Virtual Musical Instrument}},
year = {2012}
}
@inproceedings{Nash2008,
address = {Genoa, Italy},
author = {Nash, Chris and Blackwell, Alan},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {composition,gesture,nime08,performance,polytempi,realtime,tempo},
pages = {28--33},
title = {{Realtime Representation and Gestural Control of Musical Polytempi}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}028.pdf},
year = {2008}
}
@inproceedings{nime2014:lfeugere,
abstract = {In this demonstration, the mapping and the gestural control strategy developed in the Digitartic are presented. Digitartic is a musical instrument able to control sung syllables. Performative rule-based synthesis allows for controlling semi-consonants, plosive, fricative and nasal consonants with a same gesture, despite the structural differences in natural production of such vocal segments. A graphic pen tablet is used for capturing the gesture with a high sampling rate and resolution. This system alows for both performing various manners of articulation and having a continuous control over the articulation.},
address = {London, United Kingdom},
author = {Feug{\`{e}}re, Lionel and D'Alessandro, Christophe},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {86--87},
publisher = {Goldsmiths, University of London},
title = {{Rule-Based Performative Synthesis of Sung Syllables}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}522.pdf},
year = {2014}
}
@inproceedings{Bottcher2009,
address = {Pittsburgh, PA, United States},
author = {B{\"{o}}ttcher, Niels and Dimitrov, Smilen},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {augmented saxophone,hyper instruments,mapping,nime09,physical computing},
pages = {151--152},
title = {{An Early Prototype of the Augmented PsychoPhone}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}151.pdf},
year = {2009}
}
@inproceedings{Klugel2011,
address = {Oslo, Norway},
author = {Kl{\"{u}}gel, Niklas and Frie{\ss}, Marc R and Groh, Georg and Echtler, Florian},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {collaborative music composition,creativity,tabletop interface},
number = {June},
pages = {32--35},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{An Approach to Collaborative Music Composition}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}032.pdf},
year = {2011}
}
@inproceedings{Crevoisier2006,
address = {Paris, France},
author = {Crevoisier, Alain and Bornand, C{\'{e}}dric and Guichard, Arnaud and Matsumura, Seiichiro and Arakawa, Chuichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
pages = {212--215},
title = {{Sound Rose: Creating Music and Images with a Touch Table}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}212.pdf},
year = {2006}
}
@inproceedings{Muller2010,
address = {Sydney, Australia},
author = {M{\"{u}}ller, Alexander and Hemmert, Fabian and Wintergerst, G{\"{o}}tz and Jagodzinski, Ron},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {force feedback,haptic feedback,interactive,nime10,pen controller},
number = {June},
pages = {477--478},
title = {{Reflective Haptics : Resistive Force Feedback for Musical Performances with Stylus-Controlled Instruments}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}477.pdf},
year = {2010}
}
@inproceedings{Comajuncosas2011,
abstract = {This research presents a 3D gestural interface for collaborative concatenative sound synthesis and audio mosaicing.Our goal is to improve the communication between the audience and performers by means of an enhanced correlationbetween gestures and musical outcome. Nuvolet consists ofa 3D motion controller coupled to a concatenative synthesis engine. The interface detects and tracks the performers hands in four dimensions (x,y,z,t) and allows them toconcurrently explore two or three-dimensional sound cloudrepresentations of the units from the sound corpus, as wellas to perform collaborative target-based audio mosaicing.Nuvolet is included in the Esmuc Laptop Orchestra catalogfor forthcoming performances.},
address = {Oslo, Norway},
author = {Comajuncosas, Josep M and Barrachina, Alex and O'Connell, John and Guaus, Enric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {3D,audio mosaicing,concatenative synthesis,gestural controller,musical instrument,open-air interface},
pages = {252--255},
title = {{Nuvolet: {\{}3D{\}} Gesture-driven Collaborative Audio Mosaicing}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}252.pdf},
year = {2011}
}
@inproceedings{Wanderley2006,
address = {Paris, France},
author = {Wanderley, Marcelo M and Birnbaum, David and Malloch, Joseph and Sinyor, Elliot and Boissinot, Julien},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Wiki,collaborative website,open content,sensors},
pages = {180--183},
title = {{SensorWiki.org: A Collaborative Resource for Researchers and Interface Designers}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}180.pdf},
year = {2006}
}
@inproceedings{Teles2008,
address = {Genoa, Italy},
author = {Teles, Paulo C and Boyle, Aidan},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {nime08},
pages = {269--272},
title = {{Developing an "Antigenous" Art Installation Based on a Touchless Endosystem Interface}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}269.pdf},
year = {2008}
}
@inproceedings{Solis2009,
address = {Pittsburgh, PA, United States},
author = {Solis, Jorge and Ninomiya, Takeshi and Petersen, Klaus and Takeuchi, Masaki and Takanishi, Atsuo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {64--69},
title = {{Anthropomorphic Musical Performance Robots at Waseda University : Increasing Understanding of the Nature of Human Musical Interaction Abstract}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}064.pdf},
year = {2009}
}
@inproceedings{Settel2003,
address = {Montreal, QC, Canada},
author = {Settel, Zack and Lippe, Cort},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {197--200},
title = {{Convolution Brother's Instrument Design}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}197.pdf},
year = {2003}
}
@inproceedings{Steiner2007,
address = {New York City, NY, United States},
author = {Steiner, Hans-Christoph},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {375--378},
title = {{A Unified Toolkit for Accessing Human Interface Devices in Pure Data and Max / MSP}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}375.pdf},
year = {2007}
}
@inproceedings{Collins2007,
address = {New York City, NY, United States},
author = {Collins, Nick},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {accompaniment,concatenative sound syn-,feature matching,inner parts,interactive mu-,melodic similarity,nime07,thesis},
pages = {220--223},
title = {{Matching Parts : Inner Voice Led Control for Symbolic and Audio Accompaniment}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}220.pdf},
year = {2007}
}
@inproceedings{Surges:2012,
abstract = {This paper describes three hardware devices for integrating modular synthesizers with computers, each with a different approach to the relationship between hardware and software. The devices discussed are the USB-Octomod, an 8-channel OSC-compatible computer-controlled control-voltage generator, the tabulaRasa, a hardware table-lookup oscillator synthesis module with corresponding waveform design software, and the pucktronix.snake.corral, a dual 8x8 computer-controlled analog signal routing matrix. The devices make use of open-source hardware and software, and are designed around affordable micro-controllers and integrated circuits. },
address = {Ann Arbor, Michigan},
author = {Surges, Greg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
publisher = {University of Michigan},
title = {{DIY Hybrid Analog/Digital Modular Synthesis}},
year = {2012}
}
@inproceedings{Lahdeoja2008,
address = {Genoa, Italy},
author = {L{\"{a}}hdeoja, Otso},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {augmented instrument,electric guitar,gesture-sound,nime08},
pages = {53--56},
title = {{An Approach to Instrument Augmentation : the Electric Guitar}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}053.pdf},
year = {2008}
}
@inproceedings{Kuyken2008,
abstract = {This paper describes the HOP system. It consists of a wireless module built up by multiple nodes and a base station. The nodes detect acceleration of e.g. human movement. At a rate of 100 Hertz the base station collects the acceleration samples. The data can be acquired in real-time software like Pure Data and Max/MSP. The data can be used to analyze and/or sonify movement. },
address = {Genoa, Italy},
author = {Kuyken, Bart and Verstichel, Wouter and Bossuyt, Frederick and Vanfleteren, Jan and Demey, Michiel and Leman, Marc},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Digital Musical Instrument,Hop Sensor,Inertial Sensing,Wireless Sensors},
pages = {229--232},
title = {{The HOP Sensor : Wireless Motion Sensor}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}229.pdf},
year = {2008}
}
@inproceedings{Poepel:2004,
abstract = {A system is introduced that allows a string player to control asynthesis engine with the gestural skills he is used to. Theimplemented system is based on an electric viola and asynthesis engine that is directly controlled by the unanalysedaudio signal of the instrument and indirectly by controlparameters mapped to the synthesis engine. This method offersa highly string-specific playability, as it is sensitive to thekinds of musical articulation produced by traditional playingtechniques. Nuances of sound variation applied by the playerwill be present in the output signal even if those nuances arebeyond traditionally measurable parameters like pitch,amplitude or brightness. The relatively minimal hardwarerequirements make the instrument accessible with littleexpenditure.},
address = {Hamamatsu, Japan},
author = {Poepel, Cornelius},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {Electronic bowed string instrument,human computer interface,musical instrument design,oscillation controlled sound synthesis,playability},
pages = {150--153},
title = {{Synthesized Strings for String Players}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}150.pdf},
year = {2004}
}
@inproceedings{Kanebako2007,
address = {New York City, NY, United States},
author = {Kanebako, Junichi and Gibson, James and Mignonneau, Laurent},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {D.J.,Phenakistoscope,Turntable,multimedia performance},
pages = {396--398},
title = {{Mountain Guitar : a Musical Instrument for Everyone}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}396.pdf},
year = {2007}
}
@inproceedings{Wechsler2009,
address = {Pittsburgh, PA, United States},
author = {Wechsler, Robert},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {320},
title = {{The Oklo Phenomenon}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}320.pdf},
year = {2009}
}
@inproceedings{Mulder2010,
address = {Sydney, Australia},
author = {Mulder, Jos},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {amplification,modal perception,multi,musical instruments,nime10,performance practice,sound technology},
number = {Nime},
pages = {13--18},
title = {{The Loudspeaker as Musical Instrument}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}013.pdf},
year = {2010}
}
@inproceedings{Gong:2012,
abstract = {This paper describes a novel music control sensate surface, which enables integration between any musical instruments with a v ersatile, customizable, and essentially cost-effective user interface. This sensate surface is based on c onductive inkjet printing technology which allows capacitive sensor electrodes and connections between electronics components to be printed onto a large roll of flexible substrate that is unrestricted in length. The high dynamic range capacitive sensing electrodes can not only infer touch, but near-range, non-contact gestural nuance in a music performance. With this sensate surface, users can ``cut'' out their desired shapes, ``paste'' the number of inputs, and customize their controller interface, which can then send signals wirelessly to effects or software synthesizers. We seek to find a solution for integrating the form factor of traditional music controllers seamlessly on top of one's music instrument and meanwhile adding expressiveness to the music performance by sensing and incorporating movements and gestures to manipulate the musical output. We present an example of implementation on an electric ukulele and provide several design examples to demonstrate the versatile capabilities of this system.},
address = {Ann Arbor, Michigan},
author = {Gong, Nan-Wei and Zhao, Nan and Paradiso, Joseph},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Sensate surface,customizable controller surface,flexible electronics,music controller skin},
publisher = {University of Michigan},
title = {{A Customizable Sensate Surface for Music Control}},
year = {2012}
}
@inproceedings{Collins2007,
address = {New York City, NY, United States},
author = {Collins, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {accompaniment,concatenative sound syn-,feature matching,inner parts,interactive mu-,melodic similarity,nime07,thesis},
pages = {220--223},
title = {{Matching Parts : Inner Voice Led Control for Symbolic and Audio Accompaniment}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}220.pdf},
year = {2007}
}
@inproceedings{Ciglar2010,
address = {Sydney, Australia},
author = {Ciglar, Miha},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {feedback,haptics,interface,nime10,nonlinear acoustics,parametric array,ultrasound,vibro-tactility},
number = {Nime},
pages = {19--22},
title = {{An Ultrasound Based Instrument Generating Audible and Tactile Sound}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}019.pdf},
year = {2010}
}
@inproceedings{Endo:2012,
address = {Ann Arbor, Michigan},
author = {Endo, Ayaka and Moriyama, Takuma and Kuhara, Yasuo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
publisher = {University of Michigan},
title = {{Tweet Harp}},
year = {2012}
}
@inproceedings{LeGroux2010,
address = {Sydney, Australia},
author = {{Le Groux}, Sylvain and Manzolli, Jonatas and Verschure, Paul F},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {biosignals,brain-computer interface,collaborative musical performance,interactive music sys-,nime10,tem},
number = {NIME},
pages = {309--314},
title = {{Disembodied and Collaborative Musical Interaction in the Multimodal Brain Orchestra}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}309.pdf},
year = {2010}
}
@inproceedings{Johnson2006,
abstract = {How can we provide interfaces to synthesis algorithms thatwill allow us to manipulate timbre directly, using the sametimbre-words that are used by human musicians to communicate about timbre? This paper describes ongoingwork that uses machine learning methods (principally genetic algorithms and neural networks) to learn (1) to recognise timbral characteristics of sound and (2) to adjust timbral characteristics of existing synthesized sounds.},
address = {Paris, France},
author = {Johnson, Colin G and Gounaropoulos, Alex},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {timbre; natural language; neural networks},
pages = {101--102},
title = {{Timbre Interfaces using Adjectives and Adverbs}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}101.pdf},
year = {2006}
}
@inproceedings{Funk2005,
address = {Vancouver, BC, Canada},
author = {Funk, Mathias and Kuwabara, Kazuhiro and Lyons, Michael J},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {127--131},
title = {{Sonification of Facial Actions for Musical Expression}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}127.pdf},
year = {2005}
}
@inproceedings{Torresen2010,
address = {Sydney, Australia},
author = {Torresen, Jim and Renton, Eirik and Jensenius, Alexander R},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {microcontroller,nime10,wireless communication,zigbee},
number = {Nime},
pages = {368--371},
title = {{Wireless Sensor Data Collection based on ZigBee Communication}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}368.pdf},
year = {2010}
}
@inproceedings{Mccaig:2002,
abstract = {Here we present 2Hearts, a music system controlled bythe heartbeats of two people. As the players speak and touch, 2Hearts extracts meaningful variables from their heartbeat signals. These variables are mapped to musical parameters, conveying the changing patterns of tension and relaxation in the players' relationship. We describe the motivation for creating 2Hearts, observations from the prototypes that have been built, and principles learnt in the ongoing development process.},
address = {Dublin},
author = {Mccaig, Graeme and Fels, Sidney S},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Affective Computing,Ambient Display,Biosensor,Heart Rate,Interactive Music,Non-Verbal Communication},
pages = {120--125},
title = {{Playing on Heart-Strings: Experiences with the 2{\{}H{\}}earts System}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}120.pdf},
year = {2002}
}
@inproceedings{Valle2008,
abstract = {This paper is about GeoGraphy, a graph-based system forthe control of both musical composition and interactive performance and its implementation in a real-time, interactiveapplication. The implementation includes a flexible userinterface system.},
address = {Genoa, Italy},
author = {Valle, Andrea},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {a graph,composition,figure 1,interfaces,left,live coding,musical algorithmic composition,nime08,performance,vertex durations and coor-},
pages = {257--260},
title = {{GeoGraphy : a Real-Time, Graph-Based Composition Environment}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}257.pdf},
year = {2008}
}
@inproceedings{Hunt2003,
address = {Montreal, QC, Canada},
author = {Hunt, Andy D and Kirk, Ross},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {135--139},
title = {{MidiGrid: Past, Present and Future}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}135.pdf},
year = {2003}
}
@inproceedings{Eaton2005,
address = {Vancouver, BC, Canada},
author = {Eaton, John and Moog, Robert},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {258--259},
title = {{Multiple-Touch-Sensitive Keyboard}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}258.pdf},
year = {2005}
}
@inproceedings{Kerllenevich2011,
address = {Oslo, Norway},
author = {Kerlle{\~{n}}evich, Hern{\'{a}}n and Egu{\'{i}}a, Manuel C and Riera, Pablo E},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {biological neural networks,complex pat-,musical interface,network performance,rhythm generation,terns},
number = {June},
pages = {331--336},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{An Open Source Interface based on Biological Neural Networks for Interactive Music Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}331.pdf},
year = {2011}
}
@inproceedings{Klugel:2013,
abstract = {Controlling the timbre generated by an audio synthesizerin a goal-oriented way requires a profound understandingof the synthesizer's manifold structural parameters. Especially shapingtimbre expressively to communicate emotional affect requires expertise.Therefore, novices in particular may not be able to adequately control timbrein viewof articulating the wealth of affects musically. In this context, the focus ofthis paper is the development of a model that can represent a relationshipbetween timbre and an expected emotional affect . The results of the evaluationof the presented model are encouraging which supports its use in steering oraugmenting the control of the audio synthesis. We explicitly envision thispaper as a contribution to the field of Synthesis by Analysis in the broadersense, albeit being potentially suitable to other related domains.},
address = {Daejeon, Republic of Korea},
author = {Kl{\"{u}}gel, Niklas and Groh, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Analysis by Synthesis,Deep Belief Networks,Emotional affect,Machine Learning,Timbre},
pages = {525--530},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Towards Mapping Timbre to Emotional Affect}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Lebel2006,
address = {Paris, France},
author = {Lebel, Denis and Malloch, Joseph},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {digital musical instrument,kinesthetic feedback},
pages = {85--88},
title = {{The G-Spring Controller}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}085.pdf},
year = {2006}
}
@inproceedings{Miyama2010,
abstract = {Peacock is a newly designed interface for improvisational performances. The interface is equipped with thirty-five proximity sensors arranged in five rows and seven columns. The sensors detect the movements of a performer's hands and arms in a three-dimensional space above them. The interface digitizes the output of the sensors into sets of high precision digital packets, and sends them to a patch running in Pdextended with a sufficiently high bandwidth for performances with almost no computational resource consumption in Pd. The precision, speed, and efficiency of the system enable the sonification of hand gestures in realtime without the need to attach any physical devices to the performer's body. This paper traces the interface's evolution, discussing relevant technologies, hardware construction, system design, and input monitoring. },
address = {Sydney, Australia},
author = {Miyama, Chikashi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Computer music,Hardware and software design,Musical interface,Sensor technologies},
pages = {380--382},
title = {{Peacock : A Non-Haptic {\{}3D{\}} Performance Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}380.pdf},
year = {2010}
}
@inproceedings{nime2014:dschwarz1,
abstract = {We propose ways of enriching the timbral potential of gestural sonic material captured via piezo or contact microphones, through latency-free convolution of the microphone signal with grains from a sound corpus. This creates a new way to combine the sonic richness of large sound corpora, easily accessible via navigation through a timbral descriptor space, with the intuitive gestural interaction with a surface, captured by any contact microphone. We use convolution to excite the grains from the corpus via the microphone input, capturing the contact interaction sounds, which allows articulation of the corpus by hitting, scratching, or strumming a surface with various parts of the hands or objects. We also show how changes of grains have to be carefully handled, how one can smoothly interpolate between neighbouring grains, and finally evaluate the system against previous attempts.},
address = {London, United Kingdom},
author = {Schwarz, Diemo and Tremblay, Pierre Alexandre and Harker, Alex},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {247--250},
publisher = {Goldsmiths, University of London},
title = {{Rich Contacts: Corpus-Based Convolution of Contact Interaction Sound for Enhanced Musical Expression}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}451.pdf},
year = {2014}
}
@inproceedings{VanNort2004,
address = {Hamamatsu, Japan},
author = {{Van Nort}, Doug and Wanderley, Marcelo M and Depalle, Philippe},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {computational,interface design,interpolation,mapping},
pages = {87--91},
title = {{On the Choice of Mappings Based on Geometric Properties}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}087.pdf},
year = {2004}
}
@inproceedings{McPherson:2013,
abstract = {This paper presents a portable optical measurement system for capturingcontinuous key motion on any piano. Very few concert venues have MIDI-enabledpianos, and many performers depend on the versatile but discontinued MoogPianoBar to provide MIDI from a conventional acoustic instrument. The scannerhardware presented in this paper addresses the growing need for alternativesolutions while surpassing existing systems in the level of detail measured.Continuous key position on both black and white keys is gathered at 1kHz samplerate. Software extracts traditional and novel features of keyboard touch fromeach note, which can be flexibly mapped to sound using MIDI or Open SoundControl. RGB LEDs provide rich visual feedback to assist the performer ininteracting with more complex sound mapping arrangements. An application ispresented to the magnetic resonator piano, an electromagnetically-augmentedacoustic grand piano which is performed using continuous key positionmeasurements.},
address = {Daejeon, Republic of Korea},
author = {McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Piano,gesture sensing,keyboard,magnetic resonator piano,mapping,optical sensing,visual feedback},
pages = {152--157},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Portable Measurement and Mapping of Continuous Piano Gesture}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Shear2011,
address = {Oslo, Norway},
author = {Shear, Greg and Wright, Matthew},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {aftertouch,augmented in-,electromagnetic,feedback,keyboard,rhodes,strument,sustain},
number = {June},
pages = {14--17},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{The Electromagnetically Sustained Rhodes Piano}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}014.pdf},
year = {2011}
}
@inproceedings{Magnusson2010,
abstract = {The analysis of digital music systems has traditionally been characterized by an approach that can be defined as phenomenological. The focus has been on the body and its relationship to the machine, often neglecting the system's conceptual design. This paper brings into focus the epistemic features of digital systems, which implies emphasizing the cognitive, conceptual and music theoretical side of our musical instruments. An epistemic dimension space for the analysis of musical devices is proposed. },
address = {Sydney, Australia},
author = {Magnusson, Thor},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Epistemic tools,analysis.,dimension space,music theory},
pages = {43--46},
title = {{An Epistemic Dimension Space for Musical Devices}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}043.pdf},
year = {2010}
}
@inproceedings{DeSilva2004,
address = {Hamamatsu, Japan},
author = {de Silva, Gamhewage C and Smyth, Tamara and Lyons, Michael J},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {bioacoustics,face tracking,mouth controller},
pages = {169--172},
title = {{A Novel Face-tracking Mouth Controller and its Application to Interacting with Bioacoustic Models}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}169.pdf},
year = {2004}
}
@inproceedings{Polotti2011,
address = {Oslo, Norway},
author = {Polotti, Pietro and Goina, Maurizio},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {gesture sonification,interactive performance,public art},
number = {June},
pages = {64--67},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{EGGS in Action}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}064.pdf},
year = {2011}
}
@inproceedings{Yang:2013,
abstract = {What is the function of visuals in the design of an augmented keyboardperformance device with projection? We address this question by thinkingthrough the impact of choices made in three examples on notions of locus ofattention, visual anticipation and causal gestalt to articulate a space ofdesign choices. Visuals can emphasize and deemphasize aspects of performanceand help clarify the role input has to the performance. We suggest that thisprocess might help thinking through visual feedback design in NIMEs withrespect to the performer or the audience.},
address = {Daejeon, Republic of Korea},
author = {Yang, Qi and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Kinect,NIME,Visual feedback,augmented keyboard,gesture,interaction,musical instrument},
pages = {252--255},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Visual Associations in Augmented Keyboard Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Donnarumma:2012,
abstract = {Performing music with a computer and loudspeakers represents always a challenge. The lack of a traditional instrument requires the performer to study idiomatic strategies by which musicianship becomes apparent. On the other hand, the audience needs to decode those strategies, so to achieve an understanding and appreciation of the music being played. The issue is particularly relevant to the performance of music that results from the mediation between biological signals of the human body and physical performance.
The present article tackles this concern by demonstrating a new model of musical performance; what I define biophysical music. This is music generated and played in real time by amplifying and processing the acoustic sound of a performer's muscle contractions. The model relies on an original and open source technology made of custom biosensors and a related software framework. The succesfull application of these tools is discussed in the practical context of a solo piece for sensors, laptop and loudspeakers. Eventually, the compositional strategies that characterize the piece are discussed along with a systematic description of the relevant mapping techniques and their sonic outcome.},
address = {Ann Arbor, Michigan},
author = {Donnarumma, Marco},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Muscle sounds,augmented body,biophysical music,embodiment.,human-computer interaction,realtime performance},
publisher = {University of Michigan},
title = {{Music for Flesh II: informing interactive music performance with the viscerality of the body system}},
year = {2012}
}
@inproceedings{Hatanaka2003,
abstract = {A handheld electronic musical instrument, named the BentoBox, was developed. The motivation was to develop aninstrument which one can easily carry around and play inmoments of free time, for example when riding public transportation or during short breaks at work. The device wasdesigned to enable quick learning by having various scalesprogrammed for different styles of music, and also beexpressive by having hand controlled timbral effects whichcan be manipulated while playing. Design analysis anditeration lead to a compact and ergonomic device. This paperfocuses on the ergonomic design process of the hardware.},
address = {Montreal},
author = {Hatanaka, Motohide},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
keywords = {MIDI controller,electronic musical instrument,ergonomics,human computer interface.,musical instrument design,playability},
pages = {77--82},
title = {{Ergonomic Design of A Portable Musical Instrument}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}077.pdf},
year = {2003}
}
@inproceedings{Makelberge:2012,
abstract = {"Perfect Take" is a public installation out of networked acoustic instruments that let composers from all over the world exhibit their MIDI-works by means of the Internet. The primary aim of this system is to offer composers a way to have works exhibited and recorded in venues and with technologies not accessible to him/her under normal circumstances. The Secondary aim of this research is to highlight experience design as a complement to interaction design, and a shift of focus from functionality of a specific gestural controller, towards the environments, events and processes that they are part of.},
address = {Ann Arbor, Michigan},
author = {Makelberge, Nicolas and Barbosa, {\'{A}}lvaro and Perrotta, Andr{\'{e}} and Ferreira, Lu{\'{i}}s Sarmento},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Disklavier,MIDI,NIME,Networked Music,creativity,music collaboration},
month = {may},
publisher = {University of Michigan},
title = {{Perfect Take: Experience design and new interfaces for musical expression}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}208.pdf},
year = {2012}
}
@inproceedings{Milne2011,
address = {Oslo, Norway},
author = {Milne, Andrew J and Xamb{\'{o}}, Anna and Laney, Robin and Sharp, David B and Prechtl, Anthony and Holland, Simon},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {face,generalized keyboard,ipad,isomorphic layout,microtonality,multi-touch sur-,musical interface design,tablet},
number = {June},
pages = {244--247},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Hex Player  A Virtual Musical Controller}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}244.pdf},
year = {2011}
}
@inproceedings{Rodet2005,
address = {Vancouver, BC, Canada},
author = {Rodet, Xavier and Lambert, Jean-Philippe and Cahen, Roland and Gaudy, Thomas and Guedy, Fabrice and Gosselin, Florian and Mobuchon, Pascal},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Haptic,control,installation.,interaction,music,sound},
pages = {109--114},
title = {{Study of haptic and visual interaction for sound and music control in the Phase project}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}109.pdf},
year = {2005}
}
@inproceedings{Skogstad:2012,
abstract = {In this paper we present the Dance Jockey System, a system developed for using a full body inertial motion capture suit (Xsens MVN) in music/dance performances. We present different strategies for extracting relevant postures and actions from the continuous data, and how these postures and actions can be used to control sonic and musical features. The system has been used in several public performances, and we believe it has great potential for further exploration. However, to overcome the current practical and technical challenges when working with the system, it is important to further refine tools and software in order to facilitate making of new performance pieces.},
address = {Ann Arbor, Michigan},
author = {Skogstad, St{\aa}le A and Nymoen, Kristian and de Quay, Yago and Jensenius, Alexander Refsum},
booktitle = {Proceedings of the International Conference on New Interfaces For Musical Expression},
pages = {226--229},
title = {{Developing the {\{}D{\}}ance {\{}J{\}}ockey System for Musical Interaction with the {\{}X{\}}sens {\{}MVN{\}} Suit}},
year = {2012}
}
@inproceedings{Guedes2007,
address = {New York City, NY, United States},
author = {Guedes, Carlos},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {dance,in dance,interaction between music and,interactive,interactive dance,interactive performance,musical rhythm and rhythm,nime07,performance systems},
pages = {417--419},
title = {{Establishing a Musical Channel of Communication between Dancers and Musicians in Computer-Mediated Collaborations in Dance Performance}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}417.pdf},
year = {2007}
}
@inproceedings{Zappi2014,
abstract = {This paper investigates the process of appropriation in digital musical instrument performance, examining the effect of instrument complexity on the emergence of personal playing styles. Ten musicians of varying background were given a deliberately constrained musical instrument, a wooden cube containing a touch/force sensor, speaker and embedded computer. Each cube was identical in construction, but half the instruments were configured for two degrees of freedom while the other half allowed only a single degree. Each musician practiced at home and presented two performances, in which their techniques and reactions were assessed through video, sensor data logs, questionnaires and interviews. Results show that the addition of a second degree of freedom had the counterintuitive effect of reducing the exploration of the instrument's affordances; this suggested the presence of a dominant constraint in one of the two configurations which strongly differentiated the process of appropriation across the two groups of participants.},
address = {London, United Kingdom},
author = {Zappi, Victor and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
keywords = {DMI,evaluation},
mendeley-tags = {DMI,evaluation},
pages = {455--460},
publisher = {Goldsmiths, University of London},
title = {{Dimensionality and Appropriation in Digital Musical Instrument Design}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}409.pdf},
year = {2014}
}
@inproceedings{Dannenberg2007,
address = {New York City, NY, United States},
author = {Dannenberg, Roger B},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {accompaniment,beat,conducting,intelligent,music synchronization,nime07,synthetic performer,tracking,virtual orchestra},
pages = {130--135},
title = {{New Interfaces for Popular Music Performance}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}130.pdf},
year = {2007}
}
@inproceedings{t-Klooster:2012,
abstract = {This paper describes the development of the Emotion Light, an interactive biofeedback artwork where the user listens to a piece of electronic music whilst holding a semi-transparent sculpture that tracks his/her bodily responses and translates these into changing light patterns that emerge from the sculpture. The context of this work is briefly described and the questions it poses are derived from interviews held with audience members.},
address = {Ann Arbor, Michigan},
author = {{van 't Klooster}, Adinda Rosa},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Interactive biofeedback artwork,aesthetic interaction,affective computing,biology inspired system,biosignals,bodily response,heart rate,mediating body,music and emotion,novel interfaces,practice based research},
publisher = {University of Michigan},
title = {{The body as mediator of music in the Emotion Light}},
year = {2012}
}
@inproceedings{Sarwate:2013,
abstract = {The Variator is a compositional assistance tool that allows users to quicklyproduce and experiment with variations on musical objects, such as chords,melodies, and chord progressions. The transformations performed by the Variatorcan range from standard counterpoint transformations (inversion, retrograde,transposition) to more complicated custom transformations, and the system isbuilt to encourage the writing of custom transformations.This paper explores the design decisions involved in creating a compositionalassistance tool, describes the Variator interface and a preliminary set ofimplemented transformation functions, analyzes the results of the evaluationsof a prototype system, and lays out future plans for expanding upon thatsystem, both as a stand-alone application and as the basis for an opensource/collaborative community where users can implement and share their owntransformation functions.},
address = {Daejeon, Republic of Korea},
author = {Sarwate, Avneesh and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Composition assistance tool,computer-aided composition,social composition},
pages = {279--282},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Variator: A Creativity Support Tool for Music Composition}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Schlei:2012,
abstract = {This paper describes the design and realization of TC-11, a software instrument based on programmable multi-point controllers. TC-11 is a modular synthesizer for the iPad that uses multi-touch and device motion sensors for control. It has a robust patch programming interface that centers around multi-point controllers, providing powerful flexibility. This paper details the origin, design principles, programming implementation, and performance result of TC- 11.},
address = {Ann Arbor, Michigan},
author = {Schlei, Kevin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {TC-11,controller mapping,iPad,multi-point,multi-touch,synthesis programming},
publisher = {University of Michigan},
title = {{TC-11: A Programmable Multi-Touch Synthesizer for the iPad}},
year = {2012}
}
@inproceedings{Allison:2013,
abstract = {Distributed performance systems present many challenges to the artist inmanaging performance information, distribution and coordination of interface tomany users, and cross platform support to provide a reasonable level ofinteraction to the widest possible user base.Now that many features of HTML 5 are implemented, powerful browser basedinterfaces can be utilized for distribution across a variety of static andmobile devices. The author proposes leveraging the power of a web applicationto handle distribution of user interfaces and passing interactions via OSC toand from realtime audio/video processing software. Interfaces developed in thisfashion can reach potential performers by distributing a unique user interfaceto any device with a browser anywhere in the world.},
address = {Daejeon, Republic of Korea},
author = {Allison, Jesse and Oh, Yemin and Taylor, Benjamin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {HTML5,NIME,Ruby on Rails,browser based interface,collaborative performance,distributed instruments,distributed interface,distributed performance systems},
pages = {1--6},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{NEXUS: Collaborative Performance for the Masses, Handling Instrument Interface Distribution through the Web}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Allison:2013,
abstract = {Distributed performance systems present many challenges to the artist inmanaging performance information, distribution and coordination of interface tomany users, and cross platform support to provide a reasonable level ofinteraction to the widest possible user base.Now that many features of HTML 5 are implemented, powerful browser basedinterfaces can be utilized for distribution across a variety of static andmobile devices. The author proposes leveraging the power of a web applicationto handle distribution of user interfaces and passing interactions via OSC toand from realtime audio/video processing software. Interfaces developed in thisfashion can reach potential performers by distributing a unique user interfaceto any device with a browser anywhere in the world.},
address = {Daejeon, Republic of Korea},
author = {Allison, Jesse and Oh, Yemin and Taylor, Benjamin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {HTML5,NIME,Ruby on Rails,browser based interface,collaborative performance,distributed instruments,distributed interface,distributed performance systems},
pages = {1--6},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{NEXUS: Collaborative Performance for the Masses, Handling Instrument Interface Distribution through the Web}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Overholt2005,
address = {Vancouver, BC, Canada},
author = {Overholt, Dan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {34--37},
title = {{The Overtone Violin}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}034.pdf},
year = {2005}
}
@inproceedings{Baldan2013,
abstract = {This paper presents an audio-based tennis simulation game for mobile devices, which uses motion input and non-verbal audio feedback as exclusive means of interaction. Players have to listen carefully to the provided auditory clues, like racquet hits and ball bounces, rhythmically synchronizing their movements in order to keep the ball into play. The device can be swung freely and act as a full-fledged motionbased controller, as the game does not rely at all on visual feedback and the device display can thus be ignored. The game aims to be entertaining but also effective for educational purposes, such as ear training or improvement of the sense of timing, and enjoyable both by visually-impaired and sighted users.},
address = {Daejeon, Republic of Korea},
author = {Baldan, Stefano and G{\"{o}}tzen, Amalia De and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Audio game,mobile devices,motion-based,rhythmic interaction,sonic interaction design},
pages = {200--201},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Sonic Tennis: a rhythmic interaction game for mobile devices}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Wang2013,
abstract = {By building a wired passive stylus we have added pressure sensitivity toexisting capacitive touch screen devices for less than},
address = {Daejeon, Republic of Korea},
author = {Wang, Johnty and D'Alessandro, Nicolas and Pon, Aura and Fels, Sidney},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {10 in materials,about1/10th the cost of existing solutions. The s,and increased latency equalto the period of at le,theoccupation of one audio input and output chann},
pages = {input interfaces, touch screens, tablets, pressure},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{PENny: An Extremely Low-Cost Pressure-Sensitive Stylus for Existing Capacitive Touchscreens}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Reboursiere:2012,
abstract = {In this paper we present a series of algorithms developed to detect the following guitar playing techniques : bend, hammer-on, pull-off, slide, palm muting and harmonic. Detection of playing techniques can be used to control exter- nal content (i.e audio loops and effects, videos, light events, etc.), as well as to write real-time score or to assist guitar novices in their learning process. The guitar used is a Godin Multiac with an under-saddle RMC hexaphonic piezo pickup (one pickup per string, i.e six mono signals).},
address = {Ann Arbor, Michigan},
author = {Reboursi{\`{e}}re, Lo{\"{i}}c and L{\"{a}}hdeoja, Otso and Drugman, Thomas and Dupont, St{\'{e}}phane and Picard-Limpens, C{\'{e}}cile and Riche, Nicolas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {Guitar audio analysis,augmented guitar,controller,hexaphonic pickup,playing techniques},
publisher = {University of Michigan},
title = {{Left and right-hand guitar playing techniques detection}},
year = {2012}
}
@inproceedings{Kallblad2008,
abstract = {It started with an idea to create an empty space in which you activated music and light as you moved around. In responding to the music and lighting you would activate more or different sounds and thereby communicate with the space through your body. This led to an artistic research project in which children's spontaneous movement was observed, a choreography made based on the children's movements and music written and recorded for the choreography. This music was then decomposed and choreographed into an empty space at Botkyrka konsthall creating an interactive dance installation. It was realized using an interactive sound and light system in which 5 video cameras were detecting the motion in the room connected to a 4-channel sound system and a set of 14 light modules. During five weeks people of all ages came to dance and move around in the installation. The installation attracted a wide range of people of all ages and the tentative evaluation indicates that it was very positively received and that it encouraged free movement in the intended way. Besides observing the activity in the installation interviews were made with schoolchildren age 7 who had participated in the installation. },
address = {Genoa, Italy},
author = {K{\"{a}}llblad, Anna and Friberg, Anders and Svensson, Karl and Edelholm, Elisabet S},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Installation,children's movement,dance,interactive multimedia,video recognition},
pages = {128--133},
title = {{Hoppsa Universum -- An Interactive Dance Installation for Children}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}128.pdf},
year = {2008}
}
@inproceedings{Aaron2011,
address = {Oslo},
author = {Aaron, Samuel and Blackwell, Alan and Hoadley, Richard and Regan, Tim},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {abstractions,collabora-,concurrency,controllers,eu-,figure 1,improvisation,live coding,monome,performing live at the,the,tion,tones,$\lambda$},
number = {June},
pages = {381--386},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{A Principled Approach to Developing New Languages for Live Coding}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}381.pdf},
year = {2011}
}
@inproceedings{Bennett2007,
address = {New York City, NY, United States},
author = {Bennett, Peter and Ward, Nicholas and O'Modhrain, Sile and Rebelo, Pedro},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {effortful interaction,haptics,hci,laban analysis,nime07,physicality},
pages = {273--276},
title = {{DAMPER : A Platform for Effortful Interface Development}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}273.pdf},
year = {2007}
}
@inproceedings{Pakarinen2008,
address = {Genoa, Italy},
author = {Pakarinen, Jyri and V{\"{a}}lim{\"{a}}ki, Vesa and Puputti, Tapio},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {gesture control,nime08,physical mod-,slide guitar,sound synthesis},
pages = {49--52},
title = {{Slide Guitar Synthesizer with Gestural Control}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}049.pdf},
year = {2008}
}
@inproceedings{Bowen2005,
address = {Vancouver, BC, Canada},
author = {Bowen, Adam},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {268--269},
title = {{Soundstone: A 3-D Wireless Music Controller}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}268.pdf},
year = {2005}
}
@inproceedings{Schmeder2009,
address = {Pittsburgh, PA, United States},
author = {Schmeder, Andrew and Freed, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {bedded web services,diy design,em-,nime09,rapid prototyping,real-time musical interface,reconfigurable},
pages = {121--124},
title = {{A Low-level Embedded Service Architecture for Rapid DIY Design of Real-time Musical Instruments}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}121.pdf},
year = {2009}
}
@inproceedings{Fasciani:2013,
abstract = {ABSTRACT: Mapping gestures to digital musical instrument parameters is nottrivial when the dimensionality of the sensor-captured data is high and themodel relating the gesture to sensor data is unknown. In these cases, afront-end processing system for extracting gestural information embedded in thesensor data is essential. In this paper we propose an unsupervised offlinemethod that learns how to reduce and map the gestural data to a genericinstrument parameter control space. We make an unconventional use of theSelf-Organizing Maps to obtain only a geometrical transformation of thegestural data, while dimensionality reduction is handled separately. Weintroduce a novel training procedure to overcome two main Self- Organizing Mapslimitations which otherwise corrupt the interface usability. As evaluation, weapply this method to our existing Voice-Controlled Interface for musicalinstruments, obtaining sensible usability improvements.},
address = {Daejeon, Republic of Korea},
author = {Fasciani, Stefano and Wyse, Lonce},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Gestural Controller,Multi Dimensional Control,Self-Organizing Maps,Unsupervised Gesture Mapping,Voice Control},
pages = {507--512},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Self-Organizing Gesture Map for a Voice-Controlled Instrument Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Robertson2007,
address = {New York City, NY, United States},
author = {Robertson, Andrew and Plumbley, Mark D},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {automatic accompaniment,human-computer interaction,nime07},
pages = {234--237},
title = {{B-Keeper : A Beat-Tracker for Live Performance}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}234.pdf},
year = {2007}
}
@article{Berg2009,
author = {Berg, Paul},
doi = {10.1080/07494460802664049},
file = {::},
isbn = {0749446080},
issn = {0749-4467},
journal = {Contemporary Music Review},
month = {feb},
number = {1},
pages = {75--87},
title = {{Composing Sound Structures with Rules}},
url = {http://www.tandfonline.com/doi/abs/10.1080/07494460802664049},
volume = {28},
year = {2009}
}
@inproceedings{Gruenbaum2007,
abstract = {Almost all traditional musical instruments have a one-to-one correspondence between a given fingering and the pitch that sounds for that fingering. The Samchillian Tip Tip Tip Cheeepeeeee does not - it is a keyboard MIDI controller that is based on intervals rather than fixed pitches. That is, a given keypress will sound a pitch a number of steps away from the last note sounded (within the key signature and scale selected) according to the 'delta' value assigned to that key. The advantages of such a system are convenience, speed, and the ability to play difficult, unusual and/or unintended passages extemporaneously. },
address = {New York City, NY, United States},
author = {Gruenbaum, Leon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {MIDI controller,computer keyboard,interval,keyboard,microtonal,musical instrument,pitch,relative,samchillian},
pages = {256--259},
title = {{The Samchillian Tip Tip Tip Cheeepeeeee : A Relativistic Keyboard Instrument}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}256.pdf},
year = {2007}
}
@inproceedings{Pineyro:2012,
abstract = {The Electric Slide Organistrum (Figure 1) is an acoustic stringed instrument played through a video capture system. The vibration of the instrument string is generated electro- magnetically and the pitch variation is achieved by movements carried out by the player in front of a video camera. This instrument results from integrating an ancient technique for the production of sounds as it is the vibration of a string on a soundbox and actual human-computer interaction technology such as motion detection.},
address = {Ann Arbor, Michigan},
author = {Pi{\~{n}}eyro, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Bowed string,Electromagnetic actuation,Gestural Interface,Pickup,eBow},
publisher = {University of Michigan},
title = {{Electric Slide Organistrum}},
year = {2012}
}
@inproceedings{Franinovic2007,
abstract = {The distinctive features of interactive sound installations inpublic space are considered, with special attention to therich, if undoubtedly difficult, environments in which theyexist. It is argued that such environments, and the socialcontexts that they imply, are among the most valuable features of these works for the approach that we have adoptedto creation as research practice. The discussion is articulated through case studies drawn from two of our installations, Recycled Soundscapes (2004) and Skyhooks (2006).Implications for the broader design of new musical instruments are presented.},
address = {New York City, NY, United States},
author = {Franinovic, Karmen and Visell, Yon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {architecture,interaction,music,nime07,sound in-,urban design},
pages = {191--196},
title = {{New Musical Interfaces in Context : Sonic Interaction Design in the Urban Setting}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}191.pdf},
year = {2007}
}
@inproceedings{Borchers2006,
abstract = {The MICON is an electronic music stand extending Maestro!, the latest in a series of interactive conducting exhibits that use real orchestral audio and video recordings. The MICON uses OpenGL-based rendering to display and animate score pages with a high degree of realism. It offers three different score display formats to match the user's level of expertise. A realtime animated visual cueing system helps users with their conducting. The MICON has been evaluated with music students. },
address = {Paris, France},
author = {Borchers, Jan and Hadjakos, Aristotelis and M{\"{u}}hlh{\"{a}}user, Max},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Music stand,conducting.,exhibit,score display},
pages = {254--259},
title = {{MICON A Music Stand for Interactive Conducting}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}254.pdf},
year = {2006}
}
@inproceedings{Crevoisier2011,
address = {Oslo, Norway},
author = {Crevoisier, Alain and Picard-Limpens, C{\'{e}}cile},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {graspable interfaces,interaction,mapping,nime,tangibles,user-defined interfaces},
number = {June},
pages = {236--239},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Mapping Objects with the Surface Editor}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}236.pdf},
year = {2011}
}
@inproceedings{Laibowitz2003,
address = {Montreal, QC, Canada},
author = {Laibowitz, Mat},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {216--217},
title = {{BASIS: A Genesis in Musical Interfaces}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}216.pdf},
year = {2003}
}
@inproceedings{Lehrman2006,
address = {Paris, France},
author = {Lehrman, Paul D and Singer, Eric},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {computer control,dadaism,mechanical,midi,music,percussion,player pianos,robotics,sound effects},
number = {June},
pages = {300--303},
title = {{A "Ballet M{\'{e}}canique" for the 21st Century: Performing George Antheil's Dadaist Masterpiece with Robots}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}300.pdf},
year = {2006}
}
@inproceedings{Overholt2011,
abstract = {The Overtone Fiddle is a new violin-family instrument that incorporates electronic sensors, integrated DSP, and physical actuation of the acoustic body. An embedded tactile sound transducer creates extra vibrations in the body of the Overtone Fiddle, allowing performer control and sensation via both traditional violin techniques, as well as extended playing techniques that incorporate shared man/machine control of the resulting sound. A magnetic pickup system is mounted to the end of the fiddle's fingerboard in order to detect the signals from the vibrating strings, deliberately not capturing vibrations from the full body of the instrument. This focused sensing approach allows less restrained use of DSP-generated feedback signals, as there is very little direct leakage from the actuator embedded in the body of the instrument back to the pickup. },
address = {Oslo, Norway},
author = {Overholt, Dan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Overholt - 2011 - The Overtone Fiddle an Actuated Acoustic Instrument(2).pdf:pdf},
keywords = {Active Acoustics,Actuated Musical Instruments,Electronic Violin,Hybrid Instruments},
pages = {30--33},
title = {{The Overtone Fiddle: an Actuated Acoustic Instrument}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}004.pdf},
year = {2011}
}
@inproceedings{Weinberg2002,
address = {Dublin, Ireland},
author = {Weinberg, Gil and Aimi, Roberto and Jennings, Kevin},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {cussive controllers,group playing,interdependent musical networks,per-},
pages = {186--191},
title = {{The Beatbug Network  A Rhythmic System for Interdependent Group Collaboration}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}186.pdf},
year = {2002}
}
@inproceedings{Young2006,
abstract = {In this paper we present progress of an ongoingcollaboration between researchers at the MIT MediaLaboratory and the Royal Academy of Music (RAM). The aimof this project is to further explore the expressive musicalpotential of the Hyperbow, a custom music controller firstdesigned for use in violin performance. Through the creationof new repertoire, we hope to stimulate the evolution of thisinterface, advancing its usability and refining itscapabilities. In preparation for this work, the Hyperbowsystem has been adapted for cello (acoustic and electric)performance. The structure of our collaboration is described,and two of the pieces currently in progress are presented.Feedback from the performers is also discussed, as well asfuture plans.},
address = {Paris, France},
author = {Young, Diana and Nunn, Patrick and Vassiliev, Artem},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Cello,bow,composition.,controller,electroacoustic music},
pages = {396--401},
title = {{Composing for Hyperbow: A Collaboration Between {\{}MIT{\}} and the Royal Academy of Music}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}396.pdf},
year = {2006}
}
@inproceedings{Luhtala:2012,
abstract = {This paper introduces `The Aesthetic Experience Prism', a framework for studying how components of aesthetic experience materialize in the model's of interaction of novel musical interfaces as well as how the role of aesthetics could be made more explicit in the processes of designing interaction for musical technologies. The Aesthetic Experience Prism makes use of Arthur Danto's framework of aesthetic experience that consists of three conceptual entities: (1) metaphor; (2) expression; and (3) style. In this paper we present key questions driving the research, theoretical background, artistic research approach and user research activities.
In the DIYSE project a proof-of-concept music creation system prototype was developed in a collaborative design setting. The prototype provides means to the performer to create music with minimum effort while allowing for versatile interaction. We argue that by using an artistic research approach specifically targeting designing for aesthetic experience we were able to transform the knowledge from early design ideas to resulting technology products in which model's of interaction metaphors, expression and style are in an apparent role.},
address = {Ann Arbor, Michigan},
author = {Luhtala, Matti and Niemel{\"{a}}inen, Ilkka and Plomp, Johan and Turunen, Markku and Tuomisto, Julius},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Aesthetics,Artistic Research,Exploration,Interaction Design},
publisher = {University of Michigan},
title = {{Studying Aesthetics in a Musical Interface Design Process Through `Aesthetic Experience Prism'}},
year = {2012}
}
@inproceedings{Marshall2011,
address = {Oslo, Norway},
author = {Marshall, Mark T and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Marshall, Wanderley - 2011 - Examining the Effects of Embedded Vibrotactile Feedback on the Feel of a Digital Musical Instrument.pdf:pdf},
keywords = {digital musical instruments,feel,vibrotactile feedback},
number = {June},
pages = {399--404},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Examining the Effects of Embedded Vibrotactile Feedback on the Feel of a Digital Musical Instrument}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}399.pdf},
year = {2011}
}
@inproceedings{Schiesser:2012,
abstract = {An augmented bass clarinet is developed in order to extend the performance and composition potential of the instru-ment. Four groups of sensors are added: key positions, inertial movement, mouth pressure and trigger switches. The instrument communicates wirelessly with a receiver setup which produces an OSC data stream, usable by any appli-cation on a host computer.
The SABRe projects intention is to be neither tied to its inventors nor to one single player but to offer a reference design for a larger community of bass clarinet players and composers. For this purpose, several instruments are made available and a number of composer residencies, workshops, presentations and concerts are organized. These serve for evaluation and improvement purposes in order to build a robust and user friendly extended musical instrument, that opens new playing modalities.},
address = {Ann Arbor, Michigan},
author = {Schiesser, S{\'{e}}bastien and Schacher, Jan C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Schiesser, Schacher - 2012 - SABRe The Augmented Bass Clarinet.pdf:pdf},
keywords = {OSC,air pressure,augmented instrument,bass clarinet,gesture,sensors},
month = {may},
publisher = {University of Michigan},
title = {{SABRe: The Augmented Bass Clarinet}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}193.pdf},
year = {2012}
}
@inproceedings{Wang2009a,
address = {Pittsburgh, PA, United States},
author = {Wang, Ge and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {334},
title = {{PLOrk Beat Science 2.0}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}334.pdf},
year = {2009}
}
@inproceedings{BryanKinns2004,
address = {Hamamatsu, Japan},
author = {Bryan-Kinns, Nick and Healey, Patrick G},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {collaboration,composition,improvisation,music},
pages = {27--30},
title = {{Daisyphone: Support for Remote Music Collaboration}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}027.pdf},
year = {2004}
}
@inproceedings{Gaye2006,
address = {Paris, France},
author = {Gaye, Lalya and Holmquist, Lars E and Behrendt, Frauke and Tanaka, Atau},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
pages = {22--25},
title = {{Mobile Music Technology: Report on an Emerging Community}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}022.pdf},
year = {2006}
}
@inproceedings{Bennett2006,
address = {Paris, France},
author = {Bennett, Peter},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {haptics,multi-modal feedback,musical instrument},
pages = {81--84},
title = {{PETECUBE: a Multimodal Feedback Interface}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}081.pdf},
year = {2006}
}
@inproceedings{Bottoni2007,
address = {New York City, NY, United States},
author = {Bottoni, Paolo and Caporali, Riccardo and Capuano, Daniele and Faralli, Stefano and Labella, Anna and Pierro, Mario},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {controller,dsp,dual-core,nime07,synthesizer,touch-screen},
pages = {394--395},
title = {{Use of a Dual-Core DSP in a Low-Cost, Touch-Screen Based Musical Instrument}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}394.pdf},
year = {2007}
}
@inproceedings{Kiefer2010,
address = {Sydney, Australia},
author = {Kiefer, Chris},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {evaluation,human computer,interaction,musical controller,nime10,reservoir computing,tangible user interface},
number = {Nime},
pages = {291--296},
title = {{A Malleable Interface for Sonic Exploration}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}291.pdf},
year = {2010}
}
@inproceedings{Poepel2005,
abstract = {While many new interfaces for musical expression have been presented in the past, methods to evaluate these interfaces are rare.This paper presents a method and a study comparing the potentialfor musical expression of different string-instrument based musicalinterfaces. Cues for musical expression are defined based on results of research in musical expression and on methods for musicaleducation in instrumental pedagogy. Interfaces are evaluated according to how well they are estimated to allow players making useof their existing technique for the creation of expressive music.},
address = {Vancouver, BC, Canada},
author = {Poepel, Cornelius},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Poepel - 2005 - On interface expressivity a player-based study.pdf:pdf},
keywords = {Musical Expression,audio signal driven sound synthesis,electronic bowed string instrument,evaluation of musical input devices},
pages = {228--231},
title = {{On Interface Expressivity: A Player-Based Study}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}228.pdf},
year = {2005}
}
@inproceedings{nime2014:dvannort,
abstract = {This paper discusses an approach to instrument conception that is based on a careful consideration of the coupling of tactile and sonic gestural action both into and out of the performance system. To this end we propose a design approach that not only considers the materiality of the instrument, but that leverages it as a central part of the conception of the sonic quality, the control structuring and what generally falls under the umbrella of "mapping" design. As we will discuss, this extended computational matter-centric view is of benefit towards holistically understanding an ``instrument" gestural engagement, as it is realized through physical material, sonic gestural matter and felt human engagement. We present instrumental systems that have arisen as a result of this approach to instrument design.},
address = {London, United Kingdom},
author = {Navab, Navid and Nort, Doug Van and Wei, Sha Xin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {387--390},
publisher = {Goldsmiths, University of London},
title = {{A Material Computation Perspective on Audio Mosaicing and Gestural Conditioning}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}544.pdf},
year = {2014}
}
@inproceedings{Todoroff2009,
address = {Pittsburgh, PA, United States},
author = {Todoroff, Todor and Bettens, Fr{\'{e}}d{\'{e}}ric and Reboursi{\`{e}}re, Lo{\"{i}}c and Chu, Wen-Yang},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {dancing and playing the,dominica eyckmans,extension du corps sonore,figure 1,gesture recognition,interpolation,mapping,nime09,sensor data pre-processing,viola},
pages = {141--146},
title = {{"Extension du Corps Sonore" - Dancing Viola}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}141.pdf},
year = {2009}
}
@inproceedings{Nakra2009,
address = {Pittsburgh, PA, United States},
author = {Nakra, Teresa M and Ivanov, Yuri and Smaragdis, Paris and Ault, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {conducting,gesture,interactive installations,nime09},
pages = {250--255},
title = {{The UBS Virtual Maestro : an Interactive Conducting System}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}250.pdf},
year = {2009}
}
@inproceedings{Marinelli2005,
abstract = {Mocean is an immersive environment that creates sensoryrelationships between natural media, particularly exploringthe potential of water as an emotive interface.},
address = {Vancouver, BC, Canada},
author = {Marinelli, Maia and Lamenzo, Jared and Borissov, Liubo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {New interface,PIC microcontroller,human computer interface.,natural media,pipe organ,water,wind instrument},
pages = {272},
title = {{Mocean}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}272.pdf},
year = {2005}
}
@inproceedings{Robertson2008,
address = {Genoa, Italy},
author = {Robertson, Andrew and Plumbley, Mark D and Bryan-Kinns, Nick},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {automatic accompaniment,beat tracking,human-computer,interaction,musical interface evaluation,nime08},
pages = {319--324},
title = {{A Turing Test for B-Keeper : Evaluating an Interactive}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}319.pdf},
year = {2008}
}
@inproceedings{Cannon2010,
address = {Sydney, Australia},
author = {Cannon, Joanne and Favilla, Stuart},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {augmented instruments,expressive spatial,nime10,playable instruments},
number = {Nime},
pages = {120--124},
title = {{Expression and Spatial Motion : Playable Ambisonics}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}120.pdf},
year = {2010}
}
@inproceedings{Magnus:2012,
abstract = {The purpose of the Musician Assistance and Score Distribution (MASD) system is to assist novice musicians with playing in an orchestra, concert band, choir or other musical ensemble. MASD helps novice musicians in three ways. It removes the confusion that results from page turns, aides a musician's return to the proper location in the music score after the looking at the conductor and notifies musicians of conductor instructions. MASD is currently verified by evaluating the time between sending beats or conductor information and this information being rendered for the musician. Future work includes user testing of this system.
There are three major components to the MASD system. These components are Score Distribution, Score Rendering and Information Distribution. Score Distribution passes score information to clients and is facilitated by the Internet Communication Engine (ICE). Score Rendering uses the GUIDO Library to display the musical score. Information Distribution uses ICE and the IceStorm service to pass beat and instruction information to musicians.},
address = {Ann Arbor, Michigan},
author = {Magnus, Nathan and Gerhard, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {musician assistance,score distribution,score rendering,score-following},
publisher = {University of Michigan},
title = {{Musician Assistance and Score Distribution (MASD)}},
year = {2012}
}
@inproceedings{Gurevich2009,
abstract = {In this paper we discuss the concept of style, focusing in particular on methods of designing new instruments that facilitate the cultivation and recognition of style. We distinguishbetween style and structure of an interaction and discuss thesignificance of this formulation within the context of NIME.Two workshops that were conducted to explore style in interaction design are described, from which we identify elements of style that can inform and influence the design process. From these, we suggest steps toward designing forstyle in new musical interactions.},
address = {Pittsburgh, PA, United States},
author = {Gurevich, Michael and Stapleton, Paul and Bennett, Peter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {expression,skill,structure,style,virtuosity},
pages = {213--217},
title = {{Designing for Style in New Musical Interactions}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}213.pdf},
year = {2009}
}
@inproceedings{Fyans2009,
abstract = {The development of new interfaces for musical expressionhas created a need to study how spectators comprehend newperformance technologies and practices. As part of a largerproject examining how interactions with technology can becommunicated with the spectator, we relate our model ofspectator understanding of error to the NIME discourse surrounding transparency, mapping, skill and success.},
address = {Pittsburgh, PA, United States},
author = {Fyans, A Cavan and Gurevich, Michael and Stapleton, Paul},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {HCI,design,performance,skill,transparency},
pages = {171--172},
title = {{Where Did It All Go Wrong ? A Model of Error From the Spectator{\{}$\backslash$textquoteright{\}}s Perspective}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}171.pdf},
year = {2009}
}
@inproceedings{Kocaballi2010,
address = {Sydney, Australia},
author = {Kocaballi, A Baki and Gemeinboeck, Petra and Saunders, Rob},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {distal perception,enactive interfaces,human agency,nime10,sensory supplementation,sonic feedback,tactile feedback},
number = {Nime},
pages = {47--50},
title = {{Investigating the Potential for Shared Agency using Enactive Interfaces}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}047.pdf},
year = {2010}
}
@inproceedings{Goto2009,
address = {Pittsburgh, PA, United States},
author = {Goto, Suguru},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {328},
title = {{BodyJack}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}328.pdf},
year = {2009}
}
@inproceedings{Julia2011,
address = {Oslo, Norway},
author = {Juli{\`{a}}, Carles F and Gallardo, Daniel and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {framework,pure data,tabletop,tangible},
number = {June},
pages = {457--460},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{MTCF : A Framework for Designing and Coding Musical Tabletop Applications Directly in Pure Data}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}457.pdf},
year = {2011}
}
@inproceedings{Fraietta2005,
abstract = {This paper will demonstrate the use of the Smart Controller workbench in the Interactive Bell Garden. },
address = {Vancouver, BC, Canada},
author = {Fraietta, Angelo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Algorithmic Composition,Control Voltage,MIDI,Open Sound Control,Programmable Logic Control,Sound Installations,Synthesizers.},
pages = {260--261},
title = {{Smart Controller / Bell Garden Demo}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}260.pdf},
year = {2005}
}
@inproceedings{Fasciani:2013,
abstract = {ABSTRACT: Mapping gestures to digital musical instrument parameters is nottrivial when the dimensionality of the sensor-captured data is high and themodel relating the gesture to sensor data is unknown. In these cases, afront-end processing system for extracting gestural information embedded in thesensor data is essential. In this paper we propose an unsupervised offlinemethod that learns how to reduce and map the gestural data to a genericinstrument parameter control space. We make an unconventional use of theSelf-Organizing Maps to obtain only a geometrical transformation of thegestural data, while dimensionality reduction is handled separately. Weintroduce a novel training procedure to overcome two main Self- Organizing Mapslimitations which otherwise corrupt the interface usability. As evaluation, weapply this method to our existing Voice-Controlled Interface for musicalinstruments, obtaining sensible usability improvements.},
address = {Daejeon, Republic of Korea},
author = {Fasciani, Stefano and Wyse, Lonce},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Gestural Controller,Multi Dimensional Control,Self-Organizing Maps,Unsupervised Gesture Mapping,Voice Control},
month = {may},
pages = {507--512},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Self-Organizing Gesture Map for a Voice-Controlled Instrument Interface}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Jessop2011,
address = {Oslo, Norway},
author = {Jessop, Elena and Torpey, Peter A and Bloomberg, Benjamin},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {ambisonics,death and the powers,disembodied performance,gestural in-,opera,significant amount of technology,terfaces,to,tod machover,we wanted the story},
number = {June},
pages = {349--354},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Music and Technology in Death and the Powers}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}349.pdf},
year = {2011}
}
@inproceedings{nime2014:jsnyder,
abstract = {In this paper, we present the ``Mobile Marching Band" (MMB) as a new mode of musical performance with mobile computing devices. We define an MMB to be, at its most general, any ensemble utilizing mobile computation that can travel as it performs, with the performance being independent of its location. We will discuss the affordances and limitations of mobile-based instrument design and performance, specifically within the context of a ``moving" ensemble. We will also discuss the use of a Mobile Marching Band as an educational tool. Finally, we will explore our implementation of a Mobile Parade, a digital Brazilian samba ensemble.},
address = {London, United Kingdom},
author = {Snyder, Jeff and Sarwate, Avneesh},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {147--150},
publisher = {Goldsmiths, University of London},
title = {{Mobile Device Percussion Parade}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}542.pdf},
year = {2014}
}
@inproceedings{Flety2005,
abstract = {The Wise Box is a new wireless digitizing interface for sensors and controllers. An increasing demand for this kind of hardware, especially in the field of dance and computer performance lead us to design a wireless digitizer that allows for multiple users, with high bandwidth and accuracy. The interface design was initiated in early 2004 and shortly described in reference [1]. Our recent effort was directed to make this device available for the community on the form of a manufactured product, similarly to our previous interfaces such as AtoMIC Pro, Eobody or Ethersense [1][2][3]. We describe here the principles we used for the design of the device as well as its technical specifications. The demo will show several devices running at once and used in real-time with a various set of sensors. },
address = {Vancouver, BC, Canada},
author = {Fl{\'{e}}ty, Emmanuel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Fl{\'{e}}ty - 2005 - The WiSe Box a Multi-performer Wireless Sensor Interface using {\{}WiFi{\}} and OSC.pdf:pdf},
keywords = {802.11,Gesture,OpenSoundControl.,Sensors,WiFi},
pages = {266--267},
title = {{The WiSe Box: a Multi-performer Wireless Sensor Interface using {\{}WiFi{\}} and OSC}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}266.pdf},
year = {2005}
}
@inproceedings{Montag2011,
address = {Oslo, Norway},
author = {Montag, Matthew and Sullivan, Stefan and Dickey, Scott and Leider, Colby},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {diy,frustrated total internal reflection,haptics,latency,mu-,multi-touch,music composition,sic performance},
number = {June},
pages = {8--13},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{A Low-Cost, Low-Latency Multi-Touch Table with Haptic Feedback for Musical Applications}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}008.pdf},
year = {2011}
}
@inproceedings{Bowers2005,
address = {Vancouver, BC, Canada},
author = {Bowers, John and Archer, Phil},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {5--10},
title = {{Not Hyper, Not Meta, Not Cyber but Infra-Instruments}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}005.pdf},
year = {2005}
}
@inproceedings{Snyder:2012,
abstract = {This paper presents the JD-1, a digital controller for analog modular synthesizers. The JD-1 features a capacitive touch- sensing keyboard that responds to continuous variations in finger contact, high-accuracy polyphonic control-voltage outputs, a built-in sequencer, and digital interfaces for connection to MIDI and OSC devices. Design goals include interoperability with a wide range of synthesizers, very high- resolution pitch control, and intuitive control of the sequencer from the keyboard.},
address = {Ann Arbor, Michigan},
author = {Snyder, Jeff and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {analog synthesizer,capacitive touch sensing,keyboard,sequencer},
publisher = {University of Michigan},
title = {{The JD-1: an Implementation of a Hybrid Keyboard/Sequencer Controller for Analog Synthesizers}},
year = {2012}
}
@inproceedings{Wyse2010,
address = {Sydney, Australia},
author = {Wyse, Lonce and Duy, Nguyen D},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {expressive control,musical interface,nime10,parameter mapping},
number = {June},
pages = {15--18},
title = {{Instrumentalizing Synthesis Models}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}140.pdf},
year = {2010}
}
@inproceedings{nime2014:olahdeoja,
abstract = {This paper provides a report of a research effort to transform architectural and scenographic surfaces into sound sources and use them in artistic creation. Structure-borne sound drivers are employed to induce sound into the solid surfaces, making them vibrate and emit sound. The sound waves can be perceived both via the aural (airborne diffusion) as well as the tactile (structure-borne diffusion) senses. The paper describes the main challenges encountered in the use of structure-borne sound technology, as well as the current results in overcoming them. Two completed artistic projects are presented in order to illustrate the creative possibilities enabled by the research.},
address = {London, United Kingdom},
author = {L{\"{a}}hdeoja, Otso},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {319--322},
publisher = {Goldsmiths, University of London},
title = {{Structure-Borne Sound and Aurally Active Spaces}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}310.pdf},
year = {2014}
}
@inproceedings{Freed2010,
address = {Sydney, Australia},
author = {Freed, Adrian},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {dimension reduction,interpolation,nime10,radial basis functions},
number = {Nime},
pages = {343--347},
title = {{Visualizations and Interaction Strategies for Hybridization Interfaces}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}343.pdf},
year = {2010}
}
@inproceedings{Oda:2013,
abstract = {The Internet allows musicians and other artists to collaborate remotely.However, network latency presents a fundamental challenge for remotecollaborators who need to coordinate and respond to each other's performancein real time. In this paper, we investigate the viability of predictingpercussion hits before they have occurred, so that information about thepredicted drum hit can be sent over a network, and the sound can be synthesizedat a receiver's location at approximately the same moment the hit occurs atthe sender's location. Such a system would allow two percussionists to playin perfect synchrony despite the delays caused by computer networks. Toinvestigate the feasibility of such an approach, we record vibraphone malletstrikes with a high-speed camera and track the mallet head position. We showthat 30 ms before the strike occurs, it is possible to predict strike time andvelocity with acceptable accuracy. Our method fits a second-order polynomial tothe data to produce a strike time prediction that is within the bounds ofperceptual synchrony, and a velocity estimate that will enable the soundpressure level of the synthesized strike to be accurate within 3 dB.},
address = {Daejeon, Republic of Korea},
author = {Oda, Reid and Finkelstein, Adam and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Networked performance,computer vision,prediction},
pages = {94--97},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Towards Note-Level Prediction for Networked Music Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Sanganeria:2013,
abstract = {GrainProc is a touchscreen interface for real-time granular synthesis designedfor live performance. The user provides a real-time audio input (electricguitar, for example) as a granularization source and controls various synthesisparameters with their fingers or toes. The control parameters are designed togive the user access to intuitive and expressive live granular manipulations.},
address = {Daejeon, Republic of Korea},
author = {Sanganeria, Mayank and Werner, Kurt},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {CCRMA,Granular synthesis,real-time,toe control,touch screen interface},
pages = {223--226},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{GrainProc: a real-time granular synthesis interface for live performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{nime2014:ibergstrom,
abstract = {We introduce two complementary OSC schemata for two contexts of use. The first is for the complete description of an OSC namespace: detailing the full set of messages each OSC-enabled system can receive or send, alongside choice metadata we deem necessary to make full use of each system's description. The second context of use is a snapshot (partial or full) of the system's state. We also relate our proposed schemata to the current state of the art, and how using these resolves issues that were left pending with previous research.},
address = {London, United Kingdom},
author = {Bergstrom, Ilias and Llobera, Joan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {311--314},
publisher = {Goldsmiths, University of London},
title = {{OSC-Namespace and OSC-State: Schemata for Describing the Namespace and State of OSC-Enabled Systems}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}300.pdf},
year = {2014}
}
@inproceedings{Mitchell:2012,
abstract = {This paper presents a toolbox of gestural control mechanisms which are available when the input sensing apparatus is a pair of data gloves fitted with orientation sensors. The toolbox was developed in advance of a live music performance in which the mapping from gestural input to audio output was to be developed rapidly in collaboration with the performer. The paper begins with an introduction to the associated literature before introducing a range of continuous, discrete and combined control mechanisms, enabling a flexible range of mappings to be explored and modified easily. An application of the toolbox within a live music performance is then described with an evaluation of the system with ideas for future developments.},
address = {Ann Arbor, Michigan},
author = {Mitchell, Thomas and Madgwick, Sebastian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {Computer Music,Data Gloves,Gestural Control},
publisher = {University of Michigan},
title = {{Musical Interaction with Hand Posture and Orientation: A Toolbox of Gestural Control Mechanisms}},
year = {2012}
}
@inproceedings{Andersen2003,
address = {Montreal, QC, Canada},
author = {Andersen, Tue H},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {30--35},
title = {{Mixxx : Towards Novel DJ Interfaces}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}030.pdf},
year = {2003}
}
@inproceedings{nime2014:ebertelli,
abstract = {Through examining the decisions and sequences of presenting a multi-media instrument fabrication program to students, this paper seeks to uncover practical elements of best practice and possible improvements in science and music education. The Conductive Music program incorporates public engagement principles, open-source hardware, DIY ethos, contemporary composition techniques, and educational activities for creative and analytical thinking. These activities impart positive skills through multi-media content delivery for all learning types. The program is designed to test practices for engaging at-risk young people from urban areas in the construction and performance of new electronic instruments. The goal is to open up the world of electronic music performance to a new generation of young digital artists and to replace negative social behaviours with creative outlets for expression through technology and performance. This paper highlights the key elements designed to deliver the program's agenda and examines the ways in which these aims were realised or tested in the classroom.},
address = {London, United Kingdom},
author = {Robertson, Emily and Bertelli, Enrico},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {517--520},
publisher = {Goldsmiths, University of London},
title = {{Conductive Music: Teaching Innovative Interface Design and Composition Techniques with Open-Source Hardware}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}301.pdf},
year = {2014}
}
@inproceedings{Marier2010,
address = {Sydney, Australia},
author = {Marier, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {electroacoustic music,expressivity,interface,nime10,performance},
number = {Nime},
pages = {356--359},
title = {{The Sponge A Flexible Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}356.pdf},
year = {2010}
}
@inproceedings{Sjuve2008,
abstract = {This paper describes the development of a wireless wearablecontroller, GO, for both sound processing and interactionwith wearable lights. Pure Data is used for sound processing.The GO prototype is built using a PIC microcontroller usingvarious sensors for receiving information from physicalmovements.},
address = {Genoa, Italy},
author = {Sjuve, Eva},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Gestural interface,Interactive Lights.,Pure Data,Wireless controller},
pages = {362--363},
title = {{Prototype GO : Wireless Controller for Pure Data}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}362.pdf},
year = {2008}
}
@inproceedings{Fried:2013,
abstract = {We present a method for automatic feature extraction and cross-modal mappingusing deep learning. Our system uses stacked autoencoders to learn a layeredfeature representation of the data. Feature vectors from two (or more)different domains are mapped to each other, effectively creating a cross-modalmapping. Our system can either run fully unsupervised, or it can use high-levellabeling to fine-tune the mapping according a user's needs. We show severalapplications for our method, mapping sound to or from images or gestures. Weevaluate system performance both in standalone inference tasks and incross-modal mappings.},
address = {Daejeon, Republic of Korea},
author = {Fried, Ohad and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Deep learning,feature learning,gestural control,mapping},
pages = {531--534},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Cross-modal Sound Mapping Using Deep Learning}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{nime2014:etomas1,
abstract = {Tangible Scores are a new paradigm for musical instrument design with a physical configuration inspired by graphic scores. In this paper we will focus on the design aspects of this new interface as well as on some of the related technical details. Creating an intuitive, modular and expressive instrument for textural music was the primary driving force. Following these criteria, we literally incorporated a musical score onto the surface of the instrument as a way of continuously controlling several parameters of the sound synthesis. Tangible Scores are played with both hands and they can adopt multiple physical forms. Complex and expressive sound textures can be easily played over a variety of timbres, enabling precise control in a natural manner.},
address = {London, United Kingdom},
author = {Tom{\'{a}}s, Enrique and Kaltenbrunner, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {609--614},
publisher = {Goldsmiths, University of London},
title = {{Tangible Scores: Shaping the Inherent Instrument Score}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}352.pdf},
year = {2014}
}
@inproceedings{Arfib2002,
address = {Dublin, Ireland},
author = {Arfib, Daniel and Dudon, Jacques},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {digital emulation,max-msp,photosonic synthesis},
pages = {1--3},
title = {{A Digital Emulator of the Photosonic Instrument}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}001.pdf},
year = {2002}
}
@inproceedings{Nishino2009,
address = {Pittsburgh, PA, United States},
author = {Nishino, Hiroki},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {adjacency and angles between,computer vision,fiducial tracking,interaction techniques,nime09,on region adjacency tree,regions together,tangible,touch and reactivision depends,user interface,while d-},
pages = {62--63},
title = {{A 2D Fiducial Tracking Method based on Topological Region Adjacency and Angle Information}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}062.pdf},
year = {2009}
}
@inproceedings{Freed:2013a,
abstract = {This paper positively addresses the problem that most NIME devices are ephemeralasting long enough to signal academic and technical prowess but rarely longerthan a few musical performances. We offer a case study that shows thatlongevity of use depends on stabilizing the interface and innovating theimplementation to maintain the required performance of the controller for theplayer.},
address = {Daejeon, Republic of Korea},
author = {Freed, Adrian and MacCallum, John and Mansfield, Sam},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Best practices,Fingerboard controller,Organology,Recrudescence,Unobtainium},
pages = {441--445},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{``Old'' is the New ``New'': a Fingerboard Case Study in Recrudescence as a NIME Development Strategy}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Pellarin2005,
address = {Vancouver, BC, Canada},
author = {Pellarin, Lars and B{\"{o}}ttcher, Niels and Olsen, Jakob M and Gregersen, Ole and Serafin, Stefania and Guglielmi, Michel},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {152--155},
title = {{Connecting Strangers at a Train Station}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}152.pdf},
year = {2005}
}
@inproceedings{Mayton:2012,
abstract = {We present Patchwerk, a networked synthesizer module with tightly coupled web browser and tangible interfaces. Patchwerk connects to a pre-existing modular synthesizer using the emerging cross-platform HTML5 WebSocket standard to enable low-latency, high-bandwidth, concurrent control of analog signals by multiple users. Online users control physical outputs on a custom-designed cabinet that reflects their activity through a combination of motorized knobs and LEDs, and streams the resultant audio. In a typical installation, a composer creates a complex physical patch on the modular synth that exposes a set of analog and digital parameters (knobs, buttons, toggles, and triggers) to the web-enabled cabinet. Both physically present and online audiences can control those parameters, simultane- ously seeing and hearing the results of each other's actions. By enabling collaborative interaction with a massive analog synthesizer, Patchwerk brings a broad audience closer to a rare and historically important instrument. Patchwerk is available online at http://synth.media.mit.edu.},
address = {Ann Arbor, Michigan},
author = {Mayton, Brian and Dublon, Gershon and Joliat, Nicholas and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {HTML5,Modular synthesizer,collaborative musical instrument,tangible interface},
publisher = {University of Michigan},
title = {{Patchwork: Multi-User Network Control of a Massive Modular Synthesizer}},
year = {2012}
}
@inproceedings{nime2014:lpereira,
abstract = {The Well--Sequenced Synthesizer is a series of sequencers that create music in dialog with the user. Through the sequencers' physical interfaces, users can control music theory-based generative algorithms. This series --a work-in-progress-- is composed by three sequencers at this time. The first one, called The Counterpointer, takes a melody input from the user and responds by generating voices based on the rules of eighteenth--century counterpoint. The second one is based on a recent treatise on harmony and counterpoint by music theorist Dmitri Tymoczco: El Ordenador lets users explore a set of features of tonality by constraining randomly generated music according to one or more of them. El Ordenador gives the user less control than The Counterpointer, but more than La Mec{\{}{\'{a}}{\}}nica, the third sequencer in the series. La Mec{\{}{\'{a}}{\}}nica plays back the sequences generated by El Ordenador using a punch-card reading music box mechanism. It makes the digital patterns visible and tactile, and links them back to the physical world.},
address = {London, United Kingdom},
author = {Hors, Luisa Pereira},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {88--89},
publisher = {Goldsmiths, University of London},
title = {{The Well-Sequenced Synthesizer}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}2.pdf},
year = {2014}
}
@inproceedings{IXISoftware2002,
address = {Dublin, Ireland},
author = {{,IXI Software}},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
pages = {101},
title = {{IXI software}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}101.pdf},
year = {2002}
}
@inproceedings{Clay:2012,
abstract = {The augmented ballet project aims at gathering research from several fields and directing them towards a same application case: adding virtual elements (visual and acoustic) to a dance live performance, and allowing the dancer to interact with them. In this paper, we describe a novel interaction that we used in the frame of this project: using the dancer's movements to recognize the emotions he expresses, and use these emotions to generate musical audio flows evolving in real-time. The originality of this interaction is threefold. First, it covers the whole interaction cycle from the input (the dancer's movements) to the output (the generated music). Second, this interaction isn't direct but goes through a high level of abstraction: dancer's emotional expression is recognized and is the source of music generation. Third, this interaction has been designed and validated through constant collaboration with a choreographer, culminating in an augmented ballet performance in front of a live audience.},
address = {Ann Arbor, Michigan},
author = {Clay, Alexis and Couture, Nadine and Desainte-Catherine, Myriam and Vulliard, Pierre-Henri and Larralde, Joseph and Decarsin, Elodie},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {Interactive sonification,gesture and music,interaction,live performance,motion,musical human-computer interaction},
publisher = {University of Michigan},
title = {{Movement to emotions to music: using whole body emotional expression as an interaction for electronic music generation}},
year = {2012}
}
@inproceedings{Nilson2007,
abstract = {Live coding is almost the antithesis of immediate physicalmusicianship, and yet, has attracted the attentions of a numberof computer-literate musicians, as well as the music-savvyprogrammers that might be more expected. It is within thecontext of live coding that I seek to explore the question ofpractising a contemporary digital musical instrument, which i soften raised as an aside but more rarely carried out in research(though see [12]). At what stage of expertise are the membersof the live coding movement, and what practice regimes mighthelp them to find their true potential? },
address = {New York City, NY, United States},
author = {Nilson, Click},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Practice,live coding,practising},
pages = {112--117},
title = {{Live Coding Practice}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}112.pdf},
year = {2007}
}
@inproceedings{Havryliv2009,
address = {Pittsburgh, PA, United States},
author = {Havryliv, Mark and Naghdy, Fazel and Schiemer, Greg and Hurd, Timothy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {force-feedback,haptics,mechanical analysis,nime09},
pages = {187--192},
title = {{Haptic Carillon  Analysis {\&} Design of the Carillon Mechanism}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}187.pdf},
year = {2009}
}
@inproceedings{Torre2008,
abstract = {In this paper, we describe an algorithm for the numericalevaluation of the orientation of an object to which a clusterof accelerometers, gyroscopes and magnetometers has beenattached. The algorithm is implemented through a set ofMax/Msp and pd new externals. Through the successfulimplementation of the algorithm, we introduce Pointingat, a new gesture device for the control of sound in a 3Denvironment. This work has been at the core of the Celeritas Project, an interdisciplinary research project on motiontracking technology and multimedia live performances between the Tyndall Institute of Cork and the InteractionDesign Centre of Limerick.},
address = {Genoa, Italy},
author = {Torre, Giuseppe and Torres, Javier and Fernstr{\"{o}}m, Mikael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {eu-,ler,max,micro-electro-mechanical,msp,nime08,orientation matrix,pd,pitch yaw and roll,quaternion,sensors,surement unit,tracking orientation,wimu,wireless inertial mea-},
pages = {103--106},
title = {{The Development of Motion Tracking Algorithms for Low Cost Inertial Measurement Units}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}103.pdf},
year = {2008}
}
@inproceedings{Geiger2008,
address = {Genoa, Italy},
author = {Geiger, Christian and Reckter, Holger and Paschke, David and Schulz, Florian and Poepel, Cornelius},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {3d interaction techniques,an important concept for,both hands,evaluation,few wimp interface concepts,in contrast the use,make efficient use of,nime08,of both hands is,theremin-based interfaces},
pages = {303--306},
title = {{Towards Participatory Design and Evaluation of Theremin-based Musical Interfaces}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}303.pdf},
year = {2008}
}
@inproceedings{Muller:2012,
abstract = {This paper discusses the utilization of human skin as a tangible interface for musical expression and collaborative performance. We present an overview of existing different instrument designs that include the skin as the main input. As a further development of a previous exploration [16] we outline the setup and interaction methods of `Skintimacy', an instrument that appropriates the skin for low voltage power transmission in multi-player interaction. Observations deriving from proof-of-concept exploration and performances using the instrument are brought into the reflection and discussion concerning the capabilities and limitations of skin as an input surface.},
address = {Ann Arbor, Michigan},
author = {M{\"{u}}ller-Rakow, Alexander and Fuchs, Jochen},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Skin-based instruments,collaborative interfaces,embodiment,intimacy,multi-player performance,skin conductivity},
month = {may},
publisher = {University of Michigan},
title = {{The Human Skin as an Interface for Musical Expression}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}177.pdf},
year = {2012}
}
@inproceedings{Levy:2012,
abstract = {OMax is an improvisation software based on a graph repre- sentation encoding the pattern repetitions and structures of a sequence, built incrementally and in real-time from a live Midi or Audio source. We present in this paper a totally rewritten version of the software. The new design leads to refine the spectral listening of OMax and to consider different methods to build the symbolic alphabet labeling our symbolic units. The very modular and versatile archi- tecture makes possible new musical configurations and we tried the software with different styles and musical situa- tions. A novel visualization is proposed, which displays the current state of the learnt knowledge and allows to notice, both on the fly and a posteriori, points of musical interest and higher level structures.},
address = {Ann Arbor, Michigan},
author = {Levy, Benjamin and Bloch, Georges and Assayag, Gerard},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Improvisation,Machine Learning,Machine Listen- ing,OMax,Sequence Model,Software Architecture,Visualization},
publisher = {University of Michigan},
title = {{OMaxist Dialectics: Capturing, Visualizing and Expanding Improvisations}},
year = {2012}
}
@inproceedings{nime2014:ssalazar,
abstract = {Effective software interaction design must consider all of the capabilities and limitations of the platform for which it is developed. To this end, we propose a new model for computer music system design on touchscreen devices, combining both pen/stylus input and multitouch gestures. Such a model surpasses the barrier of touchscreen-based keyboard input, preserving the primary interaction of touch and direct manipulation throughout the development of a complex musical program. We have implemented an iPad software application utilizing these principles, called ``Auraglyph." Auraglyph offers a number of fundamental audio processing and control operators, as well as facilities for structured input and output. All of these software objects are created, parameterized, and interconnected via stylus and touch in- put. Underlying this application is an advanced handwriting recognition framework, LipiTk, which can be trained to recognize both alphanumeric characters and arbitrary figures, shapes, and patterns.},
address = {London, United Kingdom},
author = {Salazar, Spencer and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {106--109},
publisher = {Goldsmiths, University of London},
title = {{Auraglyph: Handwritten Computer Music Composition and Design}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}560.pdf},
year = {2014}
}
@inproceedings{Fried:2013,
abstract = {We present a method for automatic feature extraction and cross-modal mappingusing deep learning. Our system uses stacked autoencoders to learn a layeredfeature representation of the data. Feature vectors from two (or more)different domains are mapped to each other, effectively creating a cross-modalmapping. Our system can either run fully unsupervised, or it can use high-levellabeling to fine-tune the mapping according a user's needs. We show severalapplications for our method, mapping sound to or from images or gestures. Weevaluate system performance both in standalone inference tasks and incross-modal mappings.},
address = {Daejeon, Republic of Korea},
author = {Fried, Ohad and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Deep learning,feature learning,gestural control,mapping},
pages = {531--534},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Cross-modal Sound Mapping Using Deep Learning}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Hoskinson2003,
address = {Montreal, QC, Canada},
author = {Hoskinson, Reynald and van den Doel, Kees and Fels, Sidney S},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {99--103},
title = {{Real-time Adaptive Control of Modal Synthesis}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}099.pdf},
year = {2003}
}
@inproceedings{Aylward2006,
abstract = {We describe the design of a system of compact, wireless sensor modules meant to capture expressive motion whenworn at the wrists and ankles of a dancer. The sensors form ahigh-speed RF network geared toward real-time dataacquisition from multiple devices simultaneously, enabling asmall dance ensemble to become a collective interface formusic control. Each sensor node includes a 6-axis inertialmeasurement unit (IMU) comprised of three orthogonalgyroscopes and accelerometers in order to capture localdynamics, as well as a capacitive sensor to measure closerange node-to-node proximity. The nodes may also beaugmented with other digital or analog sensors. This paperdescribes application goals, presents the prototype hardwaredesign, introduces concepts for feature extraction andinterpretation, and discusses early test results.},
address = {Paris, France},
author = {Aylward, Ryan and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Aylward, Paradiso - 2006 - Sensemble A Wireless, Compact, Multi-User Sensor System for Interactive Dance.pdf:pdf},
keywords = {Interactive dance,collective motion analysis,inertial gesture tracking,multi-user interface,wearable sensor networks},
pages = {134--139},
title = {{Sensemble: A Wireless, Compact, Multi-User Sensor System for Interactive Dance}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}134.pdf},
year = {2006}
}
@inproceedings{McDonald2009,
abstract = {The Vibrobyte is a wireless haptic interface specialized forco-located musical performance. The hardware is designedaround the open source Arduino platform, with haptic control data encapsulated in OSC messages, and OSC/hardwarecommunications handled by Processing. The Vibrobyte wasfeatured at the International Computer Music Conference2008 (ICMC) in a telematic performance between ensembles in Belfast, Palo Alto (California, USA), and Troy (NewYork, USA).},
address = {Pittsburgh, PA, United States},
author = {McDonald, Kyle and Kouttron, Dane and Bahn, Curtis and Braasch, Jonas and Oliveros, Pauline},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {haptics,interface,nime09,performance,telematic},
pages = {41--42},
title = {{The Vibrobyte : A Haptic Interface for Co-Located Performance}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}041.pdf},
year = {2009}
}
@inproceedings{Oh2010,
address = {Sydney, Australia},
author = {Oh, Jieun and Herrera, Jorge and Bryan, Nicholas J and Dahl, Luke and Wang, Ge},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {iphone,live performance,mobile,mobile phone orchestra,nime10},
number = {Nime},
pages = {82--87},
title = {{Evolving The Mobile Phone Orchestra}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}082.pdf},
year = {2010}
}
@inproceedings{Feugere:2013,
abstract = {Digitartic, a system for bi-manual gestural control of Vowel-Consonant-Vowelperformative singing synthesis is presented. This system is an extension of areal-time gesture-controlled vowel singing instrument developed in the Max/MSPlanguage. In addition to pitch, vowels and voice strength control, Digitarticis designed for gestural control of articulation parameters for a wide set onconsonant, including various places and manners of articulation. The phases ofarticulation between two phonemes are continuously controlled and can bedriven in real time without noticeable delay, at any stage of the syntheticphoneme production. Thus, as in natural singing, very accurate rhythmicpatterns are produced and adapted while playing with other musicians. Theinstrument features two (augmented) pen tablets for controlling voiceproduction: one is dealing with the glottal source and vowels, the second oneis dealing with consonant/vowel articulation. The results show very naturalconsonant and vowel synthesis. Virtual choral practice confirms theeffectiveness of Digitartic as an expressive musical instrument.},
address = {Daejeon, Republic of Korea},
author = {Feug{\`{e}}re, Lionel and D'Alessandro, Christophe},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {articulation,formants synthesis,gestural control,singing voice synthesis,syllabic synthesis},
pages = {331--336},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Digitartic: bi-manual gestural control of articulation in performative singing synthesis}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Miyashita2004,
address = {Hamamatsu, Japan},
author = {Miyashita, Homei and Nishimoto, Kazushi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {chroma profile,concert for piano and,figure 1,graphic score of,improvisation,musical score,orches -,peltier device},
pages = {104--107},
title = {{Thermoscore: A New-type Musical Score with Temperature Sensation}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}104.pdf},
year = {2004}
}
@inproceedings{Dubois2009,
address = {Pittsburgh, PA, United States},
author = {Dubois, R Luke and Flanigan, Lesley},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {336},
title = {{Bioluminescence}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}336.pdf},
year = {2009}
}
@inproceedings{Rohs2006,
address = {Paris, France},
author = {Rohs, Michael and Essl, Georg and Roth, Martin},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
pages = {31--36},
title = {{CaMus: Live Music Performance using Camera Phones and Visual Grid Tracking}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}031.pdf},
year = {2006}
}
@inproceedings{Ferguson:2013,
abstract = {Feedback created by guitars and amplifiers is difficult to use in musicalsettings -- parameters such as pitch and loudness are hard to specify preciselyby fretting a string or by holding the guitar near an amplifier. This researchinvestigates methods for controlling the level and pitch of the feedbackproduced by a guitar and amplifier, which are based on incorporatingcorpus-based control into the system. Two parameters are used to define thecontrol parameter space -- a simple automatic gain control system to controlthe output level, and a band-pass filter frequency for controlling the pitch ofthe feedback. This control parameter space is mapped to a corpus of soundscreated by these parameters and recorded, and these sounds are analysed usingsoftware created for concatenative synthesis. Following this process, thedescriptors taken from the analysis can be used to select control parametersfrom the feedback system.},
address = {Daejeon, Republic of Korea},
author = {Ferguson, Sam and Martin, Aengus and Johnston, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
pages = {541--546},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A corpus-based method for controlling guitar feedback}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Caramiaux2011a,
abstract = {In this paper we present an experimental study concerninggestural embodiment of environmental sounds in a listeningcontext. The presented work is part of a project aiming atmodeling movement-sound relationships, with the end goalof proposing novel approaches for designing musical instruments and sounding objects. The experiment is based onsound stimuli corresponding to "causal" and "non-causal"sounds. It is divided into a performance phase and an interview. The experiment is designed to investigate possiblecorrelation between the perception of the "causality" of environmental sounds and different gesture strategies for thesound embodiment. In analogy with the perception of thesounds' causality, we propose to distinguish gestures that"mimic" a sound's cause and gestures that "trace" a sound'smorphology following temporal sound characteristics. Results from the interviews show that, first, our causal soundsdatabase lead to consistent descriptions of the action at theorigin of the sound and participants mimic this action. Second, non-causal sounds lead to inconsistent metaphoric descriptions of the sound and participants make gestures following sound "contours". Quantitatively, the results showthat gesture variability is higher for causal sounds that noncausal sounds.},
address = {Oslo, Norway},
author = {Caramiaux, Baptiste and Susini, Patrick and Bianco, Tommaso and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Houix, Olivier and Schnell, Norbert and Misdariis, Nicolas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Embodiment,Environmental Sound Perception,Gesture Sound Interaction,Listening},
pages = {144--148},
title = {{Gestural Embodiment of Environmental Sounds : an Experimental Study}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}144.pdf},
year = {2011}
}
@inproceedings{Kiefer2008,
address = {Genoa, Italy},
author = {Kiefer, Chris and Collins, Nick and Fitzpatrick, Geraldine},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {evaluating musical interac-,evaluation,hci methodology,nime08,wiimote},
mendeley-tags = {evaluation},
pages = {87--90},
title = {{HCI Methodology For Evaluating Musical Controllers : A Case Study}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}087.pdf},
year = {2008}
}
@inproceedings{Ferguson2010,
address = {Sydney, Australia},
author = {Ferguson, Sam and Schubert, Emery and Stevens, Catherine},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {continuous response,dance,emotion,motion capture,nime10},
number = {Nime},
pages = {481--484},
title = {{Movement in a Contemporary Dance Work and its Relation to Continuous Emotional Response}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}481.pdf},
year = {2010}
}
@inproceedings{Hochenbaum:2013,
abstract = {Music education is a rich subject with many approaches and methodologies thathave developed over hundreds of years. More than ever, technology playsimportant roles at many levels of a musician's practice. This paper begins toexplore some of the ways in which technology developed out of the NIMEcommunity (specifically hyperinstruments), can inform a musician's dailypractice, through short and long term metrics tracking and data visualization.},
address = {Daejeon, Republic of Korea},
author = {Hochenbaum, Jordan and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Ezither,Hyperinstruments,Metrics,Pedagogy,Practice Room},
pages = {307--312},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Toward The Future Practice Room: Empowering Musical Pedagogy through Hyperinstruments}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Kirk2009,
address = {Pittsburgh, PA, United States},
author = {Kirk, Jonathon and Weisert, Lee},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {290--292},
title = {{The Argus Project : Underwater Soundscape Composition with Laser- Controlled Modulation}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}290.pdf},
year = {2009}
}
@inproceedings{Beilharz2006,
address = {Paris, France},
author = {Beilharz, Kirsty and Jakovich, Joanne and Ferguson, Sam},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
keywords = {gesture-controllers,hyper-instrument,sonification},
pages = {352--357},
title = {{Hyper-shaku (Border-crossing): Towards the Multi-modal Gesture-controlled Hyper-Instrument}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}352.pdf},
year = {2006}
}
@inproceedings{nime2014:mmarier,
abstract = {The development of the cushion-like musical interface called the sponge started about seven years ago. Since then, it was extensively used to perform in various settings. The sponge itself is described, but the main focus is on the evolution of the mapping strategies that are used. The author reviews the guidelines proposed by other researchers and explains how they were concretely applied with the sponge. He concludes that no single strategy constitutes a solution to the issue of mapping and that musical compositions are complex entities that require the use of a multitude of mapping strategies in parallel. It is hoped that the mappings described combined with new strategies will eventually lead to the emergence of a musical language that is idiomatic to the sponge.},
address = {London, United Kingdom},
author = {Marier, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {525--528},
publisher = {Goldsmiths, University of London},
title = {{Designing Mappings for the Sponge: Towards Spongistic Music}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}292.pdf},
year = {2014}
}
@inproceedings{Newton2011,
address = {Oslo, Norway},
author = {Newton, Dan and Marshall, Mark T},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {augmented instruments,cal instruments,digital musi-,instrument design,performance},
number = {June},
pages = {155--160},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Examining How Musicians Create Augmented Musical Instruments}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}155.pdf},
year = {2011}
}
@inproceedings{Grosshauser2008,
abstract = {The described project is a new approach to use highly sensitive low force pressure sensor matrices for malposition, cramping and tension of hands and fingers, gesture and keystroke analysis and for new musical expression. In the latter, sensors are used as additional touch sensitive switches and keys. In pedagogical issues, new ways of technology enhanced teaching, self teaching and exercising are described. The used sensors are custom made in collaboration with the ReactiveS Sensorlab. },
address = {Genoa, Italy},
author = {Grosshauser, Tobias},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Cramping,Finger,Force,Gesture and Posture Analysis.,Left Hand,Piano,Pressure Measurement,Right Hand,Sensor,Strings,Time Line,Violin},
pages = {97--102},
title = {{Low Force Pressure Measurement : Pressure Sensor Matrices for Gesture Analysis , Stiffness Recognition and Augmented Instruments}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}097.pdf},
year = {2008}
}
@inproceedings{Brown2010,
address = {Sydney, Australia},
author = {Brown, Andrew R},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {collaborative,ensemble,generative,interaction,network,nime10},
number = {Nime},
title = {{Network Jamming : Distributed Performance using Generative Music}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}283.pdf},
year = {2010}
}
@inproceedings{Mills2010a,
abstract = {The evolution of networked audio technologies has created unprecedented opportunities for musicians to improvise with instrumentalists from a diverse range of cultures and disciplines. As network speeds increase and latency is consigned to history, tele-musical collaboration, and in particular improvisation will be shaped by new methodologies that respond to this potential. While networked technologies eliminate distance in physical space, for the remote improviser, this creates a liminality of experience through which their performance is mediated. As a first step in understanding the conditions arising from collaboration in networked audio platforms, this paper will examine selected case studies of improvisation in a variety of networked interfaces. The ,
,
author will examine how platform characteristics and network conditions influence the process of collective improvisation and the methodologies musicians are employing to negotiate their networked experiences. },
address = {Sydney, Australia},
author = {Mills, Roger},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {improvisation,internet audio,networked collaboration,nime10,sound},
pages = {186--191},
title = {{Dislocated Sound : A Survey of Improvisation in Networked Audio Platforms}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}186.pdf},
year = {2010}
}
@inproceedings{Goncalves2011,
address = {Oslo, Norway},
author = {Gon{\c{c}}alves, Andr{\'{e}}},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {computer driven control voltage,controlled computer,embedded systems,gen-,voltage-,voltage-controlled synthesizer},
number = {June},
pages = {92--95},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Towards a Voltage-Controlled Computer Control and Interaction Beyond an Embedded System}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}092.pdf},
year = {2011}
}
@inproceedings{nime2014:sknotts,
abstract = {This paper reports the results of an online survey of 160 laptop ensembles and the relative democracy of their organisational and social structures. For the purposes of this research a laptop ensemble is defined as a performing group of three or more musicians for whom the laptop is the main sound generating source and who typically perform together in the same room. The concept of democracy (i.e. governance by members of the group) has been used as a starting point to assess firstly what types of organisational structures are currently used in laptop ensembles and secondarily to what extent laptop ensembles consider the implications of organisational and social structure on their musical output. To assess this I recorded a number of data points including ensemble size, whether the group has a director or conductor, use of homogenous vs. heterogenous hardware and software, whether they perform composed pieces or mainly improvise, the level of network interaction and whether or not the ensemble has an academic affiliation. The survey allowed me to define a scale of democracy in laptop ensembles and typical features of the most and least democratic groups. Some examples are given of democratic and autocratic activity in existing laptop ensembles. This work is part of a larger scale project investigating the effect of social structures on the musical output of laptop ensembles.},
address = {London, United Kingdom},
author = {Knotts, Shelly and Collins, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Knotts, Collins - 2014 - The Politics of Laptop Ensembles A Survey of 160 Laptop Ensembles and their Organisational Structures.pdf:pdf},
pages = {191--194},
publisher = {Goldsmiths, University of London},
title = {{The Politics of Laptop Ensembles: A Survey of 160 Laptop Ensembles and their Organisational Structures}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}521.pdf},
year = {2014}
}
@inproceedings{nime2014:ahazzard,
abstract = {Music plays a vital role in accompanying all manner of our experiences. Soundtracks within films, video games and ceremonies possess a unique ability to enhance a narrative, suggest emotional content and mark key transitions. Moreover, soundtracks often achieve all of this without being the primary focus, on the contrary they typically assume a supporting role. The proliferation of mobile devices increasingly leads us to listen to music while on the move and musicians are seizing on locative technologies as a tool for creating new kinds of music that directly respond to people's movements through space. In light of these trends, we consider the interesting question of how composers might set about creating musical soundtracks to accompany mobile experiences. What we have in mind are experiences such as guided walks, tours and even pervasive games. The novelty of our research here is in the music serving as an accompaniment to enhance a location specific activity, much as a soundtrack does for a film. This calls for composers to take into account the key features of the experience, and its setting, to gently complement them through the music. We examine this process from a composer's perspective by presenting `from the field' an account of how they address the multifaceted challenges of designing a soundtrack for public sculpture park. We chart a composer's rationale as they developed a soundtrack for this site over multiple iterations of design, testing and refinement. We expose key relationships between the raw materials of music (melody, harmony, timbre, rhythm and dynamics) and those of the physical setting, that enable the composer to gracefully mesh the music into the fabric of the space. The result is to propose a set of recommendations to inform the composition of mobile soundtracks that we intend to guide future practice and research.},
address = {London, United Kingdom},
author = {Hazzard, Adrian and Benford, Steve and Burnett, Gary},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {411--414},
publisher = {Goldsmiths, University of London},
title = {{You'll Never Walk Alone: Composing Location-Based Soundtracks}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}506.pdf},
year = {2014}
}
@inproceedings{Gurevich2007,
address = {New York City, NY, United States},
author = {Gurevich, Michael and Trevi{\~{n}}o, Jeffrey},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {aesthetic goal,communication,construct,discipline,discourse,emotion,evaluation,experience,expression,expressivity,model,nime07,non-expressive,transparency},
pages = {106--111},
title = {{Expression and Its Discontents : Toward an Ecology of Musical Creation}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}106.pdf},
year = {2007}
}
@inproceedings{Stanza2007,
address = {New York City, NY, United States},
author = {Stanza, ,},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {427},
title = {{Sensity}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}427.pdf},
year = {2007}
}
@inproceedings{Dimitrov2008,
abstract = {This paper reports on a Short-Term Scientific Mission (STSM)sponsored by the Sonic Interaction Design (SID) EuropeanCOST Action IC601.Prototypes of objects for the novel instrument Reactablewere developed, with the goal of studying sonification ofmovements on this platform using physical models. A physical model of frictional interactions between rubbed dry surfaces was used as an audio generation engine, which alloweddevelopment in two directions - a set of objects that affordsmotions similar to sliding, and a single object aiming tosonify contact friction sound. Informal evaluation was obtained from a Reactable expert user, regarding these sets ofobjects. Experiments with the objects were also performed- related to both audio filtering, and interfacing with otherobjects for the Reactable.},
address = {Genoa, Italy},
author = {Dimitrov, Smilen and Alonso, Marcos and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Reactable,contact fric- tion,motion sonification,physical model},
pages = {211--214},
title = {{Developing Block-Movement, Physical-Model Based Objects for the Reactable}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}211.pdf},
year = {2008}
}
@inproceedings{Jorda2005,
abstract = {In this paper we study the potential and the challenges posed by multi-user instruments, as tools that can facilitate interaction and responsiveness not only between performers and their instrument but also between performers as well. Several previous studies and taxonomies are mentioned, after what different paradigms exposed with examples based on traditional mechanical acoustic instruments. In the final part, several existing systems and implementations, now in the digital domain, are described and identified according to the models and paradigms previously introduced. },
address = {Vancouver, BC, Canada},
author = {Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Multi-user instruments,collaborative music,new instruments design guidelines.},
pages = {23--26},
title = {{Multi-user Instruments: Models, Examples and Promises}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}023.pdf},
year = {2005}
}
@inproceedings{Smallwood2009,
address = {Pittsburgh, PA, United States},
author = {Smallwood, Scott},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {340},
title = {{Sound Lanterns}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}340.pdf},
year = {2009}
}
@inproceedings{Pelletier2008,
abstract = {This paper describes a generalized motion-based framework forthe generation of large musical control fields from imaging data.The framework is general in the sense that it does not depend ona particular source of sensing data. Real-time images of stageperformers, pre-recorded and live video, as well as more exoticdata from imaging systems such as thermography, pressuresensor arrays, etc. can be used as a source of control. Featurepoints are extracted from the candidate images, from whichmotion vector fields are calculated. After some processing, thesemotion vectors are mapped individually to sound synthesisparameters. Suitable synthesis techniques include granular andmicrosonic algorithms, additive synthesis and micro-polyphonicorchestration. Implementation details of this framework isdiscussed, as well as suitable creative and artistic uses andapproaches.},
address = {Genoa, Italy},
author = {Pelletier, Jean-Marc},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Computer vision,control field,image analysis,imaging,mapping,microsound,motion flow,sonification,synthesis},
pages = {158--163},
title = {{Sonified Motion Flow Fields as a Means of Musical Expression}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}158.pdf},
year = {2008}
}
@inproceedings{Erkut2011,
abstract = {We present a generic, structured model for design and evaluation of musical interfaces. This model is developmentoriented, and it is based on the fundamental function of themusical interfaces, i.e., to coordinate the human action andperception for musical expression, subject to human capabilities and skills. To illustrate the particulars of this modeland present it in operation, we consider the previous designand evaluation phase of iPalmas, our testbed for exploringrhythmic interaction. Our findings inform the current design phase of iPalmas visual and auditory displays, wherewe build on what has resonated with the test users, and explore further possibilities based on the evaluation results.},
address = {Oslo, Norway},
author = {Erkut, Cumhur and Jylh{\"{a}}, Antti and Discioglu, Reha},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {multimodal displays,rhythmic interaction,sonification,uml},
pages = {477--480},
title = {{A Structured Design and Evaluation Model with Application to Rhythmic Interaction Displays}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}477.pdf},
year = {2011}
}
@inproceedings{Julia2011,
abstract = {In the past decade we have seen a growing presence of tabletop systems applied to music, lately with even some products becoming commercially available and being used byprofessional musicians in concerts. The development of thistype of applications requires several demanding technicalexpertises such as input processing, graphical design, realtime sound generation or interaction design, and because ofthis complexity they are usually developed by a multidisciplinary group.In this paper we present the Musical Tabletop CodingFramework (MTCF) a framework for designing and codingmusical tabletop applications by using the graphical programming language for digital sound processing Pure Data(Pd). With this framework we try to simplify the creationprocess of such type of interfaces, by removing the need ofany programming skills other than those of Pd.},
address = {Oslo, Norway},
author = {Juli{\`{a}}, Carles F and Gallardo, Daniel and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Pure Data,framework,tabletop,tangible},
pages = {457--460},
title = {{MTCF : A Framework for Designing and Coding Musical Tabletop Applications Directly in Pure Data}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}457.pdf},
year = {2011}
}
@inproceedings{Hsu2006,
abstract = {This paper describes recent enhancements in an interactive system designed to improvise with saxophonist John Butcher [1]. In addition to musical parameters such as pitch and loudness, our system is able to analyze timbral characteristics of the saxophone tone in real-time, and use timbral information to guide the generation of response material. We capture each saxophone gesture on the fly, extract a set of gestural and timbral contours, and store them in a repository. Improvising agents can consult the repository when generating responses. The gestural or timbral progression of a saxophone phrase can be remapped or transformed; this enables a variety of response material that also references audible contours of the original saxophone gestures. A single simple framework is used to manage gestural and timbral information extracted from analysis, and for expressive control of virtual instruments in a free improvisation context. },
address = {Paris, France},
author = {Hsu, William},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Interactive music systems,instrument control.,timbre analysis},
pages = {376--379},
title = {{Managing Gesture and Timbre for Analysis and Instrument Control in an Interactive Environment}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}376.pdf},
year = {2006}
}
@inproceedings{Choi2010,
address = {Sydney, Australia},
author = {Choi, Hongchan and Wang, Ge},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {algorithmic composition,audiovisual,automata,behavior simulation,music,music sequencer,musical interface,nime10,visualization},
number = {June},
pages = {15--18},
title = {{LUSH : An Organic Eco + Music System}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}112.pdf},
year = {2010}
}
@inproceedings{Goina2008,
address = {Genoa, Italy},
author = {Goina, Maurizio and Polotti, Pietro},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {bauhaus,gesture analysis,klee,nime08,sonification},
pages = {150--153},
title = {{Elementary Gestalts for Gesture Sonification}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}150.pdf},
year = {2008}
}
@inproceedings{Lee:2013a,
abstract = {We introduce a form of networked music performance where a performer plays amobile music instrument while it is being implemented on the fly by a livecoder. This setup poses a set of challenges in performing a music instrumentwhich changes over time and we suggest design guidelines such as making asmooth transition, varying adoption of change, and sharing information betweenthe pair of two performers. A proof-of-concept instrument is implemented on amobile device using UrMus, applying the suggested guidelines. We wish that thismodel would expand the scope of live coding to the distributed interactivesystem, drawing existing performance ideas of NIMEs.},
address = {Daejeon, Republic of Korea},
author = {Lee, Sang Won and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {live coding,mobile music,network music,on-the-fly instrument},
pages = {493--498},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Live Coding The Mobile Music Instrument}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Schacher2010,
abstract = {Mapping in interactive dance performance poses a number of questions related to the perception and expression of gestures in contrast to pure motion-detection and analysis. A specific interactive dance project is discussed, in which two complementary sensing modes are integrated to obtain higherlevel expressive gestures. These are applied to a modular nonlinear composition, in which the exploratory dance performance assumes the role of instrumentalist and conductor. The development strategies and methods for each of the involved artists are discussed and the software tools and wearable devices that were developed for this project are presented. },
address = {Sydney, Australia},
author = {Schacher, Jan C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Mapping,artistic strategies,computer vision,delaunay tessellation.,mapping tools,motion sensing,splines,wearable sensors},
pages = {250--254},
title = {{Motion To Gesture To Sound : Mapping For Interactive Dance}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}250.pdf},
year = {2010}
}
@inproceedings{Bottoni2006,
address = {Paris, France},
author = {Bottoni, Paolo and Faralli, Stefano and Labella, Anna and Pierro, Mario},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {agent,figure 1,mapping,max,msp,performer requests a goal,planning,s plans to reach,the goal are trans-,to the gomax},
pages = {322--325},
title = {{Mapping with Planning Agents in the Max/MSP Environment: the GO/Max Language}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}322.pdf},
year = {2006}
}
@inproceedings{Dannenberg2007,
abstract = {Augmenting performances of live popular music with computer systems poses many new challenges. Here, "popular music" is taken to mean music with a mostly steady tempo, some improvisational elements, and largely predetermined melodies, harmonies, and other parts. The overall problem is studied by developing a framework consisting of constraints and subproblems that any solution should address. These problems include beat acquisition, beat phase, score location, sound synthesis, data preparation, and adaptation. A prototype system is described that offers a set of solutions to the problems posed by the framework, and future work is suggested. },
address = {New York City, NY, United States},
author = {Dannenberg, Roger B},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {accompaniment,beat,conducting,intelligent,music synchronization,nime07,synthetic performer,tracking,virtual orchestra},
pages = {130--135},
title = {{New Interfaces for Popular Music Performance}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}130.pdf},
year = {2007}
}
@inproceedings{vanTroyer2012,
abstract = {We introduce a prototype of a new tangible step sequencerthat transforms everyday objects into percussive musicalinstruments. DrumTop adapts our everyday task-orientedhand gestures with everyday objects as the basis of musicalinteraction, resulting in an easily graspable musical interfacefor musical novices. The sound, tactile, and visual feedbackcomes directly from everyday objects as the players programdrum patterns and rearrange the objects on the tabletopinterface. DrumTop encourages the players to explore themusical potentiality of their surroundings and be musicallycreative through rhythmic interactions with everyday objects. The interface consists of transducers that trigger ahit, causing the objects themselves to produce sound whenthey are in close contact with the transducers. We discusshow we designed and implemented our current DrumTopprototype and describe how players interact with the interface. We then highlight the players' experience with Drumtop and our plans for future work in the fields of musiceducation and performance.},
address = {Ann Arbor, Michigan},
author = {van Troyer, Akito},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Everyday Objects,Percussion,Playful Experience,Step Sequencer,Tangible User Interfaces,Transducers},
publisher = {University of Michigan},
title = {{DrumTop: Playing with Everyday Objects}},
year = {2012}
}
@inproceedings{:2002,
abstract = {We are interested in exhibiting our programs at your demo section at the conference. We believe that the subject of your conference is precisely what we are experimenting with in our musical software. },
address = {Dublin},
author = {Software, ,IXI},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Further info on our website http//www.ixi-software},
pages = {101},
title = {{IXI software}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}101.pdf},
year = {2002}
}
@inproceedings{Rotondo:2012,
address = {Ann Arbor, Michigan},
author = {Rotondo, Michael and Kruge, Nick and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
publisher = {University of Michigan},
title = {{Many-Person Instruments for Computer Music Performance}},
year = {2012}
}
@inproceedings{McGee:2012,
abstract = {SenSynth is an open-source mobile application that allows for arbitrary, dynamic mapping between several sensors and sound synthesis parameters. In addition to synthesis techniques commonly found on mobile devices, SenSynth includes a scanned synthesis source for the audification of sensor data. Using SenSynth, we present a novel instrument based on the audification of accelerometer data and introduce a new means of mobile synthesis control via a wearable magnetic ring. SenSynth also employs a global pitch quantizer so one may adjust the level of virtuosity required to play any instruments created via mapping.},
address = {Ann Arbor, Michigan},
author = {McGee, Ryan and Ashbrook, Daniel and White, Sean},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {audification,mobile music,mobile sensors,sonification},
publisher = {University of Michigan},
title = {{SenSynth: a Mobile Application for Dynamic Sensor to Sound Mapping}},
year = {2012}
}
@inproceedings{Wang2011a,
address = {Oslo, Norway},
author = {Wang, Johnty and D'Alessandro, Nicolas and Fels, Sidney S and Pritchard, Bob},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
number = {June},
pages = {531--532},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{SQUEEZY : Extending a Multi-touch Screen with Force Sensing Objects for Controlling Articulatory Synthesis}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}531.pdf},
year = {2011}
}
@inproceedings{Bukvic2010,
address = {Sydney, Australia},
author = {Bukvic, Ivika and Martin, Thomas and Standley, Eric and Matthews, Michael},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {l2ork,laptop orchestra,linux,nime10},
number = {Nime},
pages = {170--173},
title = {{Introducing L2Ork : Linux Laptop Orchestra}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}170.pdf},
year = {2010}
}
@inproceedings{Lee:2013,
abstract = {This work aims at a music piece for large-scale audience participation usingmobile phones as musical instruments at a music performance. Utilizing theubiquity of smart phones, we attempted to accomplish audience engagement bycrafting an accessible musical instrument with which audience can be a part ofthe performance. Drawing lessons learnt from the creative works of mobilemusic, audience participation, and the networked instrument a mobile musicalinstrument application is developed so that audience can download the app atthe concert, play the instrument instantly, interact with other audiencemembers, and contribute to the music by sound generated from their mobilephones. The post-survey results indicate that the instrument was easy to use,and the audience felt connected to the music and other musicians.},
address = {Daejeon, Republic of Korea},
author = {Lee, Sang Won and Freeman, Jason},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {audience participation,mobile music,networked instrument},
pages = {450--455},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{echobo : Audience Participation Using The Mobile Music Instrument}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Hashida2008,
abstract = {One of the advantages of case-based systems is that theycan generate expressions even if the user doesn't know howthe system applies expression rules. However, the systemscannot avoid the problem of data sparseness and do notpermit a user to improve the expression of a certain part ofa melody directly. After discussing the functions requiredfor user-oriented interface for performance rendering systems, this paper proposes a directable case-based performance rendering system, called Itopul. Itopul is characterized by 1) a combination of the phrasing model and thepulse model, 2) the use of a hierarchical music structure foravoiding from the data sparseness problem, 3) visualizationof the processing progress, and 4) music structures directlymodifiable by the user.},
address = {Genoa, Italy},
author = {Hashida, Mitsuyo and Ito, Yosuke and Katayose, Haruhiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Case-based Approach,Performance Rendering,User Interface},
pages = {277--280},
title = {{A Directable Performance Rendering System : Itopul}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}277.pdf},
year = {2008}
}
@inproceedings{Gadd:2002,
abstract = {We explore the role that metaphor plays in developing expressive devices by examining the MetaMuse system. MetaMuse is a prop-based system that uses the metaphor of rainfall to make the process of granular synthesis understandable. We discuss MetaMuse within a framework we call"transparency" that can be used as a predictor of the expressivity of musical devices. Metaphor depends on a literature,or cultural basis, which forms the basis for making transparent device mappings. In this context we evaluate the effect of metaphor in the MetaMuse system.},
address = {Dublin},
author = {Gadd, Ashley and Fels, Sidney S},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Expressive interface,granular synthesis.,metaphor,prop-based controller,transparency},
pages = {65--70},
title = {{MetaMuse: Metaphors for Expressive Instruments}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}065.pdf},
year = {2002}
}
@inproceedings{nime2014:mfunk,
abstract = {When looking at modern music club settings, especially in the area of electronic music, music is consumed in a unidirectional way -- from DJ or producer to the audience -- with little direct means to influence and participate. In this paper we challenge this phenomenon and aim for a new bond between the audience and the DJ through the creation of an interactive dance concept: Experio. Experio allows for multiple audience participants influencing the musical performance through dance, facilitated by a musical moderator using a tailored interface. This co-creation of electronic music on both novice and expert levels is a new participatory live performance approach, which is evaluated on the basis of thousands of visitors who interacted with Experio during several international exhibitions.},
address = {London, United Kingdom},
author = {van Hout, Bastiaan and Giacolini, Luca and Hengeveld, Bart and Funk, Mathias and Frens, Joep},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {46--49},
publisher = {Goldsmiths, University of London},
title = {{Experio: a Design for Novel Audience Participation in Club Settings}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}481.pdf},
year = {2014}
}
@inproceedings{Fels2002,
address = {Dublin, Ireland},
author = {Fels, Sidney S and Vogt, Florian},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {communication,cooperative music,human-human,intimacy,passive haptic interface,two person musical instruments},
pages = {50--55},
title = {{Tooka: Explorations of Two Person Instruments}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}050.pdf},
year = {2002}
}
@inproceedings{Kahrs2009,
address = {Pittsburgh, PA, United States},
author = {Kahrs, Mark and Skulina, David and Bilbao, Stefan and Campbell, Murray},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {electroacoustics,flat panel,nime09},
pages = {106--109},
title = {{An Electroacoustically Controlled Vibrating Plate}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}106.pdf},
year = {2009}
}
@inproceedings{Burlet:2013,
abstract = {Sensor-based gesture recognition is investigated as a possible solution to theproblem of managing an overwhelming number of audio effects in live guitarperformances. A realtime gesture recognition system, which automaticallytoggles digital audio effects according to gestural information captured by anaccelerometer attached to the body of a guitar, is presented. To supplement theseveral predefined gestures provided by the recognition system, personalizedgestures may be trained by the user. Upon successful recognition of a gesture,the corresponding audio effects are applied to the guitar signal and visualfeedback is provided to the user. An evaluation of the system yielded 86{\%}accuracy for user-independent recognition and 99{\%} accuracy for user-dependentrecognition, on average.},
address = {Daejeon, Republic of Korea},
author = {Burlet, Gregory and Fujinaga, Ichiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Augmented instrument,accelerometer,gesture recognition,pattern recognition,performance practice},
pages = {41--44},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Stompboxes: Kicking the Habit}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Stanza2007,
address = {New York City, NY, United States},
author = {Stanza, ,},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {427},
title = {{Sensity}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}427.pdf},
year = {2007}
}
@inproceedings{Carrillo2010,
address = {Sydney, Australia},
author = {Carrillo, Alfonso P and Bonada, Jordi},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {control,nime10,spectral,synthesis,violin,virtual},
number = {Nime},
pages = {229--232},
title = {{The Bowed Tube : a Virtual Violin}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}229.pdf},
year = {2010}
}
@inproceedings{Popp2011,
address = {Oslo, Norway},
author = {Popp, Phillip and Wright, Matthew},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {analysis tech-,control,gesture recognition,machine learning,machine learning and statistical,recently,spectral model synthesis,synthesis,wacom tablet},
number = {June},
pages = {284--287},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Intuitive Real-Time Control of Spectral Model Synthesis}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}284.pdf},
year = {2011}
}
@inproceedings{Pardue:2013,
abstract = {The Hand Controller is a new interface designed to enable a performer toachieve detailed control of audio and visual parameters through a tangibleinterface combined with motion tracking of the hands to capture large scalephysical movement. Such movement empowers an expressive dynamic for bothperformer and audience. However tracking movements in free space isnotoriously difficult for virtuosic performance. The lack of tactile feedbackleads to difficulty learning the repeated muscle movements required for precisecontrol. In comparison, the hands have shown an impressive ability to mastercomplex motor tasks through feel. The hand controller uses both modes ofinteraction. Electro-magnetic field tracking enables 6D hand motion trackingwhile two options provide tactile interaction- a set of tracks that providelinear positioning and applied finger pressure, or a set of trumpet like sliderkeys that provide continuous data describing key depth. Thumbs actuateadditional pressure sensitive buttons. The two haptic interfaces are mountedto a comfortable hand grip that allows a significant range of reach, andpressure to be applied without restricting hand movement highly desirable inexpressive motion.},
address = {Daejeon, Republic of Korea},
author = {Pardue, Laurel and Sebastian, William},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {force sensing resistor,free gesture,hand,interface,new musical instrument,position tracking,tactile feedback},
pages = {90--93},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Hand-Controller for Combined Tactile Control and Motion Tracking}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Milmoe2007,
address = {New York City, NY, United States},
author = {Milmoe, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {423},
title = {{NIME Performance {\&} Installation : Sonic Pong V3.0}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}423.pdf},
year = {2007}
}
@inproceedings{Kaltenbrunner2004,
address = {Hamamatsu, Japan},
author = {Kaltenbrunner, Martin and Geiger, G{\"{u}}nter and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {dynamic patching,musical instrument,sound synthesis,tangible interfaces,visual programming},
pages = {19--22},
title = {{Dynamic Patches for Live Musical Performance}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}019.pdf},
year = {2004}
}
@inproceedings{Grosshauser:2013,
abstract = {Several new technologies to capture motion, gesture and forces for musical instrument players' analyses have been developed in the last years. In research and for augmented instruments one parameter is underrepresented so far. It is finger position and pressure measurement, applied by the musician while playing the musical instrument. In this paper we show a flexible linear-potentiometer and forcesensitive-resistor (FSR) based solution for position, pressure and force sensing between the contact point of the fingers and the musical instrument. A flexible matrix printed circuit board (PCB) is fixed on a piano key. We further introduce linear potentiometer based left hand finger position sensing for string instruments, integrated into a violin and a guitar finger board. Several calibration and measurement scenarios are shown. The violin sensor was evaluated with 13 music students regarding playability and robustness of the system. Main focus was a the integration of the sensors into these two traditional musical instruments as unobtrusively as possible to keep natural haptic playing sensation. The musicians playing the violin in different performance situations stated good playability and no differences in the haptic sensation while playing. The piano sensor is rated, due to interviews after testing it in a conventional keyboard quite unobtrusive, too, but still evokes a different haptic sensation.},
address = {Daejeon, Republic of Korea},
author = {Grosshauser, Tobias and Tr{\"{o}}ster, Gerhard},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Guitar,Keyboard,Piano,Position,Pressure,Sensor,Violin},
pages = {479--484},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Finger Position and Pressure Sensing Techniques for String and Keyboard Instruments}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{nime2014:jbowers1,
abstract = {In this paper we describe an artistic response to a collection of natural history museum artefacts, developed as part of a residency organised around a public participatory workshop. Drawing on a critical literature in studies of material culture, the work incorporated data sonification, image audification, field recordings and created a number of instruments for exploring geological artefacts and meterological data as aesthetic material. The residency culminated in an exhibition presented as a 'sensorium' for the sensory exploration of museum objects. In describing the methods and thinking behind the project this paper presents an alternative approach to engaging artists and audiences with local heritage and museum archives, which draws on research in NIME and allied literatures, and which is devoted to enlivening collections as occasions for varied interpretation, appropriation and aesthetic response.},
address = {London, United Kingdom},
author = {Bowers, John and Shaw, Tim},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {175--178},
publisher = {Goldsmiths, University of London},
title = {{Reappropriating Museum Collections: Performing Geology Specimens and Meterology Data as New Instruments for Musical Expression}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}429.pdf},
year = {2014}
}
@inproceedings{Mealla2011,
address = {Oslo, Norway},
author = {Mealla, Sebasti{\'{a}}n and V{\"{a}}aljam{\"{a}}ae, Aleksander and Bosi, Mathieu and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {also at specs laboratory,bci,collaboration,cscw,hci,multimodal interfaces,music,physiological computing,physiopucks,tabletops,universitat pompeu fabra},
number = {June},
pages = {149--154},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Listening to Your Brain : Implicit Interaction in Collaborative Music Performances}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}149.pdf},
year = {2011}
}
@inproceedings{DArcangelo2001,
address = {Seattle, WA},
author = {D'Arcangelo, Gideon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
pages = {42--45},
title = {{Creating Contexts of Creativity : Musical Composition with Modular Components}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}042.pdf},
year = {2001}
}
@inproceedings{Franinovic2011,
address = {Oslo, Norway},
author = {Franinovic, Karmen},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {exploration,gesture,habit,sonic interaction design},
number = {June},
pages = {448--452},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{The Flo)(ps : Negotiating Between Habitual and Explorative Gestures}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}448.pdf},
year = {2011}
}
@inproceedings{Hockman2009,
address = {Pittsburgh, PA, United States},
author = {Hockman, Jason A and Wanderley, Marcelo M and Fujinaga, Ichiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {exercise,nime,nime09,synchronization,time-scaling},
pages = {90--93},
title = {{Real-Time Phase Vocoder Manipulation by Runner's Pace}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}090.pdf},
year = {2009}
}
@inproceedings{Scavone2003,
address = {Montreal, QC, Canada},
author = {Scavone, Gary},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {15--18},
title = {{THE PIPE: Explorations with Breath Control}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}015.pdf},
year = {2003}
}
@inproceedings{Ciglar2010,
address = {Sydney, Australia},
author = {Ciglar, Miha},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {feedback,haptics,interface,nime10,nonlinear acoustics,parametric array,ultrasound,vibro-tactility},
number = {Nime},
pages = {19--22},
title = {{An Ultrasound Based Instrument Generating Audible and Tactile Sound}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}019.pdf},
year = {2010}
}
@inproceedings{Gillian2011,
address = {Oslo, Norway},
author = {Gillian, Nicholas and Knapp, Benjamin and O'Modhrain, Sile},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {computer interaction,dynamic time warping,gesture recognition,multivariate temporal gestures,musician-},
number = {June},
pages = {337--342},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Recognition Of Multivariate Temporal Musical Gestures Using N-Dimensional Dynamic Time Warping}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}337.pdf},
year = {2011}
}
@inproceedings{Topper2005,
address = {Vancouver, BC, Canada},
author = {Topper, David and Swendsen, Peter V},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {76--79},
title = {{Wireless Dance Control : PAIR and WISEAR}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}076.pdf},
year = {2005}
}
@inproceedings{DArcangelo:2004,
abstract = {This essay outlines a framework for understanding newmusical compositions and performances that utilizepre-existing sound recordings. In attempting toarticulate why musicians are increasingly using soundrecordings in their creative work, the ,
,
author calls fornew performance tools that enable the dynamic use ofpre-recorded music. },
address = {Hamamatsu, Japan},
author = {D'Arcangelo, Gideon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {Call and response,DJ tools,oral culture,turntablism},
pages = {55--58},
title = {{Recycling Music, Answering Back: Toward an Oral Tradition of Electronic Music}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}055.pdf},
year = {2004}
}
@inproceedings{nime2014:rgraham,
abstract = {This paper describes the theoretical underpinnings, design, and development of a hyper--instrumental performance system driven by gestural data obtained from an electric guitar. The system combines a multichannel audio feed from the guitar (which is parsed for its pitch, spectral content and note inter--onset time data to provide abstractions of sounded performance gestures) with motion tracking of the performer's larger--scale bodily movements using a Microsoft Xbox Kinect sensor. These gestural materials are used to provide the basis for the structures of relational mappings, informed by the embodied image schema structures of Lakoff and Johnson. These theoretical perspectives are refined via larger-scale ecological-embodied structural relationships in electroacoustic music outlined in Smalley's theory of spectromorphology, alongside the incorporation of an additional active-agential response structure through the use of the boids flocking algorithm by Reynolds to control the spatialization of outputs and other textural processes. The paper aims to advance a broadly-applicable 'performance gesture ecology', providing a shared spatial-relational mapping (a 'basic gestural space') which allows for creative (but still coherent) mappings from the performance gestures to the control of textural and spatial structures.},
address = {London, United Kingdom},
author = {Graham, Ricky and Bridges, Brian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {581--584},
publisher = {Goldsmiths, University of London},
title = {{Gesture and Embodied Metaphor in Spatial Music Performance Systems Design.}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}526.pdf},
year = {2014}
}
@inproceedings{Overholt2009,
address = {Pittsburgh, PA, United States},
author = {Overholt, Dan and Lahey, Byron and {Skriver Hansen}, Anne-Marie and Burleson, Winslow and {Norrgaard Jensen}, Camilla},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {339},
title = {{Pendaphonics}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}339.pdf},
year = {2009}
}
@inproceedings{Perrotin:2013,
abstract = {Touch user interfaces such as touchpad or pen tablet are often used forcontinuous pitch control in synthesis devices. Usually, pitch is set at thecontact point on the interface, thus introducing possible pitch inaccuracies atthe note onset. This paper proposes a new algorithm, based on an adaptiveattraction mapping, for improving initial pitch accuracy with touch userinterfaces with continuous control. At each new contact on the interface, thealgorithm adjusts the mapping to produce the most likely targeted note of thescale in the vicinity of the contact point. Then, pitch remains continuouslyadjustable as long as the contact is maintained, allowing for vibrato,portamento and other subtle melodic control. The results of experimentscomparing the users' pitch accuracy with and without the help of the algorithmshow that such a correction enables to play sharply in tune at the contact withthe interface, regardless the musical background of the player. Therefore, thedynamic mapping algorithm allows for a clean and accurate attack when playing touch user interfaces for controlling continuous pitch instruments like voicesynthesizers.},
address = {Daejeon, Republic of Korea},
author = {Perrotin, Olivier and D'Alessandro, Christophe},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Sound synthesis control,accuracy,automatic correction,pen tablet,precision,touch user interfaces},
pages = {186--189},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Adaptive mapping for improved pitch accuracy on touch user interfaces}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Godbehere2008,
address = {Genoa, Italy},
author = {Godbehere, Andrew B and Ward, Nathan J},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {15,4,802,accelerometers,digital musical controllers,gestural control,human-computer interaction,live,motion analysis,nime08,performance,personal area networks,wearable computing},
pages = {237--240},
title = {{Wearable Interfaces for Cyberphysical Musical Expression}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}237.pdf},
year = {2008}
}
@inproceedings{nime2014:fvisi,
abstract = {This work describes a new approach to gesture mapping in a performance with a traditional musical instrument and live electronics based upon theories of embodied music cognition (EMC) and musical gestures. Considerations on EMC and how gestures affect the experience of music inform different mapping strategies. Our intent is to enhance the expressiveness and the liveness of performance by tracking gestures via a multimodal motion capture system and to use motion data to control several features of the music. After a review of recent research in the field, a proposed application of such theories to a performance with electric guitar and live electronics will follow, focusing both on aspects of meaning formation and motion capturing.},
address = {London, United Kingdom},
author = {Visi, Federico and Schramm, Rodrigo and Miranda, Eduardo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {601--604},
publisher = {Goldsmiths, University of London},
title = {{Use of Body Motion to Enhance Traditional Musical Instruments}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}460.pdf},
year = {2014}
}
@inproceedings{Janssen2011,
address = {Oslo, Norway},
author = {Janssen, Berit},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {map-,multidimensional scaling,perception,reverberation},
number = {June},
pages = {68--71},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{A Reverberation Instrument Based on Perceptual Mapping}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}068.pdf},
year = {2011}
}
@inproceedings{Freed:2012,
abstract = {The Fingerphone, a reworking of the Stylophone in conductive paper, is presented as an example of new design approaches for sustainability and playability of electronic musical instruments.},
address = {Ann Arbor, Michigan},
author = {Freed, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Capacitive Sensing,Conductive Paper,Fingerphone,Plurifunctionality,Pressure Sensing,Stylophone,Sustainable Design,Touch Sensing},
publisher = {University of Michigan},
title = {{The Fingerphone: a Case Study of Sustainable Instrument Redesign}},
year = {2012}
}
@inproceedings{Jensenius:2013,
abstract = {"The paper presents the Kinectofon, an instrument for creating sounds throughfree-hand interaction in a 3D space. The instrument is based on the RGB anddepth image streams retrieved from a Microsoft Kinect sensor device. These twoimage streams are used to create different types of motiongrams, which, again,are used as the source material for a sonification process based on inverseFFT. The instrument is intuitive to play, allowing the performer to createsound by ""touching"" a virtual sound wall."},
address = {Daejeon, Republic of Korea},
author = {Jensenius, Alexander Refsum},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Kinect,motiongram,sonification,video analysis},
month = {may},
pages = {196--197},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Kinectofon: Performing with Shapes in Planes}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{dAlessandro2007,
address = {New York City, NY, United States},
author = {D'Alessandro, Nicolas and Dutoit, Thierry},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {bi-manual gestural control,fsr,nime07,pen tablet},
pages = {78--81},
title = {{HandSketch Bi-Manual Controller Investigation on Expressive Control Issues of an Augmented Tablet}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}078.pdf},
year = {2007}
}
@inproceedings{Torresen2012,
abstract = {We present a new wireless transceiver board for the CUI32 sensor interface, aimed at creating a solution that is flexible, reliable, and with little power consumption. Communication with the board is based on the ZigFlea protocol and it has been evaluated on a CUI32 using the StickOS oper- ating system. Experiments show that the total sensor data collection time is linearly increasing with the number of sensor samples used. A data rate of 0.8 kbit/s is achieved for wirelessly transmitting three axes of a 3D accelerometer. Although this data rate is low compared to other systems, our solution benefits from ease-of-use and stability, and is useful for applications that are not time-critical.},
address = {Ann Arbor, Michigan},
author = {Torresen, Jim and Hauback, {\O}yvind Nyborg and Overholt, Dan and Jensenius, Alexander Refsum},
booktitle = {Proceedings of the International Conference on New Interfaces For Musical Expression},
pages = {409--412},
title = {{Development and Evaluation of a {\{}Z{\}}ig{\{}F{\}}lea-based Wireless Transceiver Board for {\{}CUI32{\}}}},
url = {http://www.duo.uio.no/sok/work.html?WORKID=166399{\&}fid=99533},
year = {2012}
}
@inproceedings{Kimura:2012,
abstract = {As a 2010 Artist in Residence in Musical Research at IRCAM, Mari Kimura used the Augmented Violin to develop new compositional approaches, and new ways of creating interactive performances [1]. She contributed her empirical and historical knowledge of violin bowing technique, working with the Real Time Musical Interactions Team at IRCAM. Thanks to this residency, her ongoing long-distance collaboration with the team since 2007 dramatically accelerated, and led to solving several compositional and calibration issues of the Gesture Follower (GF) [2]. Kimura was also the first artist to develop projects between the two teams at IRCAM, using OMAX (Musical Representation Team) with GF. In the past year, the performance with Augmented Violin has been expanded in larger scale interactive audio/visual projects as well. In this paper, we report on the various techniques developed for the Augmented Violin and compositions by Kimura using them, offering specific examples and scores.},
address = {Ann Arbor, Michigan},
author = {Kimura, Mari and Rasamimanana, Nicolas and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Schnell, Norbert and Zamborlin, Bruno and Fl{\'{e}}ty, Emmanuel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Augmented Violin,Gesture Follower,Interactive Performance},
month = {may},
publisher = {University of Michigan},
title = {{Extracting Human Expression For Interactive Composition with the Augmented Violin}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}279.pdf},
year = {2012}
}
@inproceedings{Ryan2003,
address = {Montreal, QC, Canada},
author = {Ryan, Joel and Salter, Christopher L},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {87--90},
title = {{TGarden: Wearable Instruments and Augmented Physicality}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}087.pdf},
year = {2003}
}
@inproceedings{Modler2003,
abstract = {This paper describes the implementation of Time Delay NeuralNetworks (TDNN) to recognize gestures from video images.Video sources are used because they are non-invasive and do notinhibit performer's physical movement or require specialistdevices to be attached to the performer which experience hasshown to be a significant problem that impacts musiciansperformance and can focus musical rehearsals and performancesupon technical rather than musical concerns (Myatt 2003).We describe a set of hand gestures learned by an artificial neuralnetwork to control musical parameters expressively in real time.The set is made up of different types of gestures in order toinvestigate:-aspects of the recognition process-expressive musical control-schemes of parameter mapping-generalization issues for an extended set for musicalcontrolThe learning procedure of the Neural Network is describedwhich is based on variations by affine transformations of imagesequences of the hand gestures.The whole application including the gesture capturing isimplemented in jMax to achieve real time conditions and easyintegration into a musical environment to realize differentmappings and routings of the control stream.The system represents a practice-based research using actualmusic models like compositions and processes of compositionwhich will follow the work described in the paper.},
address = {Montreal},
author = {Modler, Paul and Myatt, Tony and Saup, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
keywords = {Artificial Neural Network,Expressive Control,Gesture Recognition,Real-time Interaction},
pages = {146--150},
title = {{An Experimental Set of Hand Gestures for Expressive Control of Musical Parameters in Realtime}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}146.pdf},
year = {2003}
}
@inproceedings{Sanganeria:2013,
abstract = {GrainProc is a touchscreen interface for real-time granular synthesis designedfor live performance. The user provides a real-time audio input (electricguitar, for example) as a granularization source and controls various synthesisparameters with their fingers or toes. The control parameters are designed togive the user access to intuitive and expressive live granular manipulations.},
address = {Daejeon, Republic of Korea},
author = {Sanganeria, Mayank and Werner, Kurt},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {CCRMA,Granular synthesis,real-time,toe control,touch screen interface},
pages = {223--226},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{GrainProc: a real-time granular synthesis interface for live performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Oda:2013,
abstract = {The Internet allows musicians and other artists to collaborate remotely.However, network latency presents a fundamental challenge for remotecollaborators who need to coordinate and respond to each other's performancein real time. In this paper, we investigate the viability of predictingpercussion hits before they have occurred, so that information about thepredicted drum hit can be sent over a network, and the sound can be synthesizedat a receiver's location at approximately the same moment the hit occurs atthe sender's location. Such a system would allow two percussionists to playin perfect synchrony despite the delays caused by computer networks. Toinvestigate the feasibility of such an approach, we record vibraphone malletstrikes with a high-speed camera and track the mallet head position. We showthat 30 ms before the strike occurs, it is possible to predict strike time andvelocity with acceptable accuracy. Our method fits a second-order polynomial tothe data to produce a strike time prediction that is within the bounds ofperceptual synchrony, and a velocity estimate that will enable the soundpressure level of the synthesized strike to be accurate within 3 dB.},
address = {Daejeon, Republic of Korea},
author = {Oda, Reid and Finkelstein, Adam and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Networked performance,computer vision,prediction},
pages = {94--97},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Towards Note-Level Prediction for Networked Music Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Choe2011,
abstract = {In this paper, we suggest a conceptual model of a Web application framework for the composition and documentation of soundscape and introduce corresponding prototype projects, SeoulSoundMap and SoundScape Composer. We also survey the current Web-based sound projects in terms of soundscape documentation. },
address = {Oslo, Norway},
author = {Choe, Souhwan and Lee, Kyogu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {sound archive,sound map,soundscape,soundscape composition,soundscape documentation.,web application framework},
pages = {533--534},
title = {{{\{}SW{\}}AF: Towards a Web Application Framework for Composition and Documentation of Soundscape}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}533.pdf},
year = {2011}
}
@inproceedings{Livingstone2005,
address = {Vancouver, BC, Canada},
author = {Livingstone, Dan and Miranda, Eduardo},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {65--69},
title = {{Orb3  Adaptive Interface Design for Real time Sound Synthesis {\&} Diffusion within Socially Mediated Spaces}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}065.pdf},
year = {2005}
}
@inproceedings{Donald2011,
abstract = {This paper outlines the formation of the Expanded Performance (EP) trio, a chamber ensemble comprised of electriccello with sensor bow, augmented digital percussion, anddigital turntable with mixer. Decisions relating to physical set-ups and control capabilities, sonic identities, andmappings of each instrument, as well as their roles withinthe ensemble, are explored. The contributions of these factors to the design of a coherent, expressive ensemble andits emerging performance practice are considered. The trioproposes solutions to creation, rehearsal and performanceissues in ensemble live electronics.},
address = {Oslo, Norway},
author = {Donald, Erika and Duinker, Ben and Britton, Eliot},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Live electronics,chamber music,digital performance,ensemble,instrument identity,mapping},
pages = {491--494},
title = {{Designing the EP Trio: Instrument Identities, Control and Performance Practice in an Electronic Chamber Music Ensemble}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}491.pdf},
year = {2011}
}
@inproceedings{Mudd:2013,
abstract = {This paper presents a system for exploring different dimensions of a soundthrough the use of haptic feedback. The Novint Falcon force feedback interfaceis used to scan through soundfiles as a subject moves their hand horizontallyfrom left to right, and to relay information about volume, frequency content,noisiness, or potentially any analysable parameter back to the subject throughforces acting on their hand. General practicalities of mapping sonic elements to physical forces areconsidered, such as the problem of representing detailed data through vaguephysical sensation, approaches to applying forces to the hand that do notinterfering with the smooth operation of the device, and the relative merits ofdiscreet and continuous mappings. Three approaches to generating the forcevector are discussed: 1) the use of simulated detents to identify areas of anaudio parameter over a certain threshold, 2) applying friction proportional tothe level of the audio parameter along the axis of movement, and 3) creatingforces perpendicular to the subject's hand movements.Presentation of audio information in this manner could be beneficial for`pre-feeling' as a method for selecting material to play during a liveperformance, assisting visually impaired audio engineers, and as a generalaugmentation of standard audio editing environments.},
address = {Daejeon, Republic of Korea},
author = {Mudd, Tom},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Haptics,force feedback,human-computer interaction,mapping},
pages = {369--372},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Feeling for Sound: Mapping Sonic Data to Haptic Perceptions}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{ihattwick1:2014,
abstract = {This paper presents results from the development of a digital musical instrument which uses audio feature extraction for the control of sound synthesis. Our implementation utilizes multi-band audio analysis to generate control signals. This technique is well-suited to instruments for which the gestural interface is intentionally weakly defined. We present a percussion instrument utilizing this technique in which the timbral characteristics of found objects are the primary source of audio for analysis.},
address = {London, United Kingdom},
author = {Hattwick, Ian and Beebe, Preston and Hale, Zachary and Wanderley, Marcelo and Leroux, Philippe and Marandola, Fabrice},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {597--600},
publisher = {Goldsmiths, University of London},
title = {{Unsounding Objects: Audio Feature Extraction for the Control of Sound Synthesis}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}369.pdf},
year = {2014}
}
@inproceedings{Soria:2013,
abstract = {This work presents a general framework method for cre-ating spatialization systems focused on electroacoustic andacousmatic music performance and creation. Although weused the logistic equation as orbit generator, any dynami-cal system could be suitable. The main idea lies on generating vectors of Rn with entriesfrom data series of di{\_}x000B{\_}erent orbits from an speci{\_}x000C{\_}c dynami-cal system. Such vectors will be called system vectors. Ourproposal is to create ordered paths between those pointsor system vectors using the Splines Quark library by Felix,1which allow us to generate smooth curves joining the points.Finally, interpolating that result with a {\_}x000C{\_}xed sample value,we are able to obtain speci{\_}x000C{\_}c and independent multidimen-sional panning trajectories for each speaker array and forany number of sound sources.Our contribution is intended to be at the very root of the compositionalprocess giving to the creator a method for exploring new ways for spatialsound placement over time for a wide range of speakers ar-rangements. The advantage of using controlled chaotic dy-namical systems like the logistic equation, lies on the factthat the composer can freely and consciously choose be-tween stable or irregular behaviour for the orbits that willgenerate his/her panning trajectories. Besides, with the useof isometries, it is possible to generate di{\_}x000B{\_}erent related or-bits with one single evaluation of the system. The use ofthe spline method in SuperCollider allows the possibilityof joining and relating those values from orbits into a wellde{\_}x000C{\_}ned and coherent general system. Further research willinclude controlling synthesis parameters in the same waywe created panning trajectories.},
address = {Daejeon, Republic of Korea},
author = {Soria, Edmar and Morales-Manzanares, Roberto},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {NIME,chaos,dynamical systems,spatialization},
pages = {79--83},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Multidimensional sound spatialization by means of chaotic dynamical systems}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Mital:2013,
abstract = {We present an interactive content-based MIR environment specifically designedto aid in the exploration of databases of experimental electronic music,particularly in cases where little or no metadata exist. In recent years,several rare archives of early experimental electronic music have becomeavailable. The Daphne Oram Collection contains one such archive, consisting ofapproximately 120 hours of 1/4 inch tape recordings and representing a perioddating from circa 1957. This collection is recognized as an importantmusicological resource, representing aspects of the evolution of electronicmusic practices, including early tape editing methods, experimental synthesistechniques and composition. However, it is extremely challenging to derivemeaningful information from this dataset, primarily for three reasons. First,the dataset is very large. Second, there is limited metadata - some titles,track lists, and occasional handwritten notes exist, but where this is true,the reliability of the annotations are unknown. Finally, and mostsignificantly, as this is a collection of early experimental electronic music,the sonic characteristics of the material are often not consistent withtraditional musical information. In other words, there is no score, no knowninstrumentation, and often no recognizable acoustic source. We present amethod for the construction of a frequency component dictionary derived fromthe collection via Probabilistic Latent Component Analysis (PLCA), anddemonstrate how an interactive 3D visualization of the relationships betweenthe PLCA-derived dictionary and the archive is facilitating researcher'sunderstanding of the data.},
address = {Daejeon, Republic of Korea},
author = {Mital, Parag Kumar and Grierson, Mick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {3d browser,content-based information retrieval,daphne oram,interactive visualization,mfcc,mir,plca},
pages = {227--232},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Mining Unlabeled Electronic Music Databases through 3D Interactive Visualization of Latent Component Relationships}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Hollinger2007,
address = {New York City, NY, United States},
author = {Hollinger, Avrum and Steele, Christopher and Penhune, Virginia and Zatorre, Robert and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {fmri,input device,motor learning,mri-compatible,nime07,optical},
pages = {246--249},
title = {{fMRI-Compatible Electronic Controllers}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}246.pdf},
year = {2007}
}
@inproceedings{Jackson:2012a,
address = {Ann Arbor, Michigan},
author = {Jackson, Jay Alan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
publisher = {University of Michigan},
title = {{Bubble Drum-agog-ing: Polyrhythm Games {\&} Other Inter Activities}},
year = {2012}
}
@inproceedings{Jensenius:2013e,
abstract = {The paper presents the Kinectofon, an instrument for creating sounds through free-hand interaction in a 3D space. The instrument is based on the RGB and depth image streams retrieved from a Microsoft Kinect sensor device. These two image streams are used to create different types of motiongrams, which, again, are used as the source material for a sonification process based on inverse FFT. The instrument is intuitive to play, allowing the performer to create sound by ``touching'' a virtual sound wall.},
address = {Daejeon, Korea},
annote = {Edited by:
Woon Seung Yeo Kyogu Lee Alexander Sigman Haru Ji
Graham Wakefield


Proceedings published by
Graduate School of Culture Technology,
Korea Advance Institute of Science and Technology (KAIST) 291 Daehak-ro, Yuseong-gu, Daejeon 305-701,
Republic of Korea
All copyrights remain with the authors.
Websites
www.nime2013.org / nime2013.kaist.ac.kr www.nime.org
Cover design
Samuel Lee
ISSN 2220-4806 (Online) ISSN 2220-4814 (USB)
ISBN 9788989453598 95600},
author = {Jensenius, Alexander Refsum},
booktitle = {Proceedings of the International Conference on New Interfaces For Musical Expression},
file = {::},
pages = {196--197},
title = {{Kinectofon: Performing with Shapes in Planes}},
url = {http://www.arj.no/wp-content/2013/05/Jensenius{\_}2013c.pdf},
year = {2013}
}
@inproceedings{cdominguez:2014,
abstract = {This paper presents a project that discusses a brief history of artistic systems that use photoresistors (light-dependent resistors) and results in the construction of an interface and performance controller. The controller combines an Arduino microcontroller with a grid of photoresistors set into a slab of wood covered with a thin acrylic sheet. A brief background on past uses of these components for music and film composition and instrument-building introduces a few different implementations and performance contexts for the controller. Topics such as implementation, construction, and performance possibilities (including electroacoustic and audio-visual performance) of the controller are also discussed.},
address = {London, United Kingdom},
author = {Dominguez, Carlos},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {78--79},
publisher = {Goldsmiths, University of London},
title = {{16$\backslash$mbox{\{}-{\}}{\{}C{\}}dS: A Surface Controller for the Simultaneous Manipulation of Multiple Analog Components}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}552.pdf},
year = {2014}
}
@inproceedings{Beilharz2012,
abstract = {In site-specific installation or situated media, a significant part of the "I" in NIME is the environment, the site and the implicit features of site such as humans, weather, materials, natural acoustics, etc. These could be viewed as design constraints, or features, even agency determining the outcome of responsive sound installation works. This paper discusses the notion of interface in public (especially outdoor) installation, starting with the authors' Sculpture by the Sea Windtraces work using this recent experience as the launch-pad, with reference to ways in which others have approached it (focusing on sensor, weather-activated outdoor installations in a brief traverse of related cases, e.g. works by Garth Paine, James Bulley and Daniel Jones, and David Bowen). This is a dialogical paper on the topic of interface and `site' as the aetiology of interaction/interface/instrument and its type of response (e.g. to environment and audience). While the focus here is on outdoor factors (particularly the climatic environment), indoor site- specific installation also experiences the effects of ambient noise, acoustic context, and audience as integral agents in the interface and perception of the work, its musical expression. The way in which features of the situation are integrated has relevance for others in the NIME community in the design of responsive spaces, art installation, and large-scale or installed instruments in which users, participants, acoustics play a significant role.},
address = {Ann Arbor, Michigan},
author = {Beilharz, Kirsty and Martin, Aengus},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {NIME,outdoor sound installation,site-specific installation},
publisher = {University of Michigan},
title = {{The `Interface' in Site-Specific Sound Installation}},
year = {2012}
}
@inproceedings{dAlessandro2009,
address = {Pittsburgh, PA, United States},
author = {D'Alessandro, Nicolas and Dutoit, Thierry},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {alessandro,circuit theory and signal,faculty of engineering,graphic tablet,mons,nicolas d,nime09,playing position,polytech,processing lab,techniques},
pages = {173--174},
title = {{Advanced Techniques for Vertical Tablet Playing A Overview of Two Years of Practicing the HandSketch 1.x}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}173.pdf},
year = {2009}
}
@inproceedings{Wolf:2013,
abstract = {As any computer user employs the Internet to accomplish everyday activities, a flow of data packets moves across the network, forming their own patterns in response to his or her actions. Artists and sound designers who are interested in accessing that data to make music must currently possess low-level knowledge of Internet protocols and spend signifi-cant effort working with low-level networking code. We have created SonNet, a new software tool that lowers these practical barriers to experimenting and composing with network data. SonNet executes packet-sniffng and network connection state analysis automatically, and it includes an easy-touse ChucK object that can be instantiated, customized, and queried from a user's own code. In this paper, we present the design and implementation of the SonNet system, and we discuss a pilot evaluation of the system with computer music composers. We also discuss compositional applications of SonNet and illustrate the use of the system in an example composition.},
address = {Daejeon, Republic of Korea},
author = {Wolf, KatieAnna E and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Sonification,compositional tools,network data},
month = {may},
pages = {503--506},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{SonNet: A Code Interface for Sonifying Computer Network Data}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Rigler2007,
address = {New York City, NY, United States},
author = {Rigler, Jane and Seldess, Zachary},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {disabilities,interactive performance,motion,music composition,music education,nime07,sensors,special education},
pages = {415--416},
title = {{The Music Cre8tor : an Interactive System for Musical Exploration and Education}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}415.pdf},
year = {2007}
}
@inproceedings{Janer2005,
abstract = {In this paper we present an example of the use of the singingvoice as a controller for digital music synthesis. The analysis of the voice with spectral processing techniques, derivedfrom the Short-Time Fourier Transform, provides ways ofdetermining a performer's vocal intentions. We demonstratea prototype, in which the extracted vocal features drive thesynthesis of a plucked bass guitar. The sound synthesis stageincludes two different synthesis techniques, Physical Modelsand Spectral Morph.},
address = {Vancouver, BC, Canada},
author = {Janer, Jordi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Singing voice,musical controller,sound synthesis,spectral processing.},
pages = {132--135},
title = {{Voice-controlled plucked bass guitar through two synthesis techniques}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}132.pdf},
year = {2005}
}
@inproceedings{Steiner2006,
address = {Paris, France},
author = {Steiner, Hans-Christoph},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
pages = {106--109},
title = {{Towards a Catalog and Software Library of Mapping Methods}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}106.pdf},
year = {2006}
}
@inproceedings{Chuchacz2007,
address = {New York City, NY, United States},
author = {Chuchacz, Katarzyna and O'Modhrain, Sile and Woods, Roger},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {electronic percussion instrument,fpga,nime07,physical model},
pages = {37--40},
title = {{Physical Models and Musical Controllers  Designing a Novel Electronic Percussion Instrument}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}037.pdf},
year = {2007}
}
@inproceedings{Gurevich2006,
address = {Paris, France},
author = {Gurevich, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Collaborative interface,but since the,interaction design,media space INTRODUCTION Most would agree that mu,network music,novice,remote jamming},
pages = {118--123},
title = {{JamSpace: Designing A Collaborative Networked Music Space for Novices}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}118.pdf},
year = {2006}
}
@inproceedings{Freed2006,
abstract = {A new sensor integration system and its first incarnation i sdescribed. As well as supporting existing analog sensorarrays a new architecture allows for easy integration of thenew generation of low-cost digital sensors used in computermusic performance instruments and installation art.},
address = {Paris, France},
author = {Freed, Adrian and Avizienis, Rimas and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {FPGA,Gesture,MEMS,OSC,configurability,network,sensor},
pages = {97--100},
title = {{Beyond 0-5{\{}V{\}}: Expanding Sensor Integration Architectures}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}097.pdf},
year = {2006}
}
@inproceedings{Keith2009,
address = {Pittsburgh, PA, United States},
author = {Keith, Sarah},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {afforded to those who,and musical,and possibilities,both technical,generative,incorporate live coding practice,laptop,nime09,or algorithmic processes in,overview of the problems,performance,popular music,would attempt to},
pages = {54--55},
title = {{Controlling Live Generative Electronic Music with Deviate}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}054.pdf},
year = {2009}
}
@inproceedings{Pellarin2005,
address = {Vancouver, BC, Canada},
author = {Pellarin, Lars and B{\"{o}}ttcher, Niels and Olsen, Jakob M and Gregersen, Ole and Serafin, Stefania and Guglielmi, Michel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Motion tracking,mapping strategies,multiple participants music interfaces.,public installation},
pages = {152--155},
title = {{Connecting Strangers at a Train Station}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}152.pdf},
year = {2005}
}
@inproceedings{Sjuve2008,
address = {Genoa, Italy},
author = {Sjuve, Eva},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {gestural interface,interactive,nime08,pure data,wireless controller},
pages = {362--363},
title = {{Prototype GO : Wireless Controller for Pure Data}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}362.pdf},
year = {2008}
}
@inproceedings{Maruyama2010,
address = {Sydney, Australia},
author = {Maruyama, Yutaro and Takegawa, Yoshinari and Terada, Tsutomu and Tsukamoto, Masahiko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Musical instruments,Script language},
pages = {7--12},
title = {{UnitInstrument : Easy Configurable Musical Instruments}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}007.pdf},
year = {2010}
}
@inproceedings{Kanda2009,
address = {Pittsburgh, PA, United States},
author = {Kanda, Ryo and Hashida, Mitsuyo and Katayose, Haruhiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {audience,gesture,interaction,nime09,performer,physical,sen-,sor,visualize},
pages = {45--47},
title = {{Mims : Interactive Multimedia Live Performance System}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}045.pdf},
year = {2009}
}
@inproceedings{Johannes2002,
address = {Dublin, Ireland},
author = {Johannes, Taelman},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
pages = {167--170},
title = {{A Low-cost Sonar for Unobtrusive Man-machine Interfacing}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}167.pdf},
year = {2002}
}
@inproceedings{Engum2011,
abstract = {This paper covers and also describes an ongoing research project focusing on new artistic possibilities by exchanging music technological methods and techniques between two distinct musical genres. Through my background as a guitarist and composer in an experimental metal band I have experienced a vast development in music technology during the last 20 years. This development has made a great impact in changing the procedures for composing and producing music within my genre without necessarily changing the strategies of how the technology is used. The transition from analogue to digital sound technology not only opened up new ways of manipulating and manoeuvring sound, it also opened up challenges in how to integrate and control the digital sound technology as a seamless part of my musical genre. By using techniques and methods known from electro-acoustic/computer music, and adapting them for use within my tradition, this research aims to find new strategies for composing and producing music within my genre. },
address = {Oslo, Norway},
author = {Engum, Trond},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Artistic research,convolution,environmental sounds,real time control,strategies for composition and production},
pages = {519--522},
title = {{Real-time Control and Creative Convolution}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}519.pdf},
year = {2011}
}
@inproceedings{Reben2009,
address = {Pittsburgh, PA, United States},
author = {Reben, Alexander and Laibowitz, Mat and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {dynamic interfaces,figure 1,haptics,nime09,robotics,slide device},
pages = {37--38},
title = {{Responsive Music Interfaces for Performance}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}037.pdf},
year = {2009}
}
@inproceedings{Won2004,
address = {Hamamatsu, Japan},
author = {Won, Sook Y and Chan, Humane and Liu, Jeremy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {controllers,light sensors,midi,pure data},
pages = {209--210},
title = {{Light Pipes: A Light Controlled MIDI Instrument}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}209.pdf},
year = {2004}
}
@inproceedings{Wessel2002,
address = {Dublin, Ireland},
author = {Wessel, David and Wright, Matthew and Schott, John},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {buchla thunder,expressive control,mapping gestures to acoustic,metaphors for musical control,results,tactex},
pages = {192--194},
title = {{Intimate Musical Control of Computers with a Variety of Controllers and Gesture Mapping Metaphors}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}192.pdf},
year = {2002}
}
@inproceedings{Kartadinata2006,
abstract = {The gluion is a sensor interface that was designed to overcomesome of the limitations of more traditional designs based onmicrocontrollers, which only provide a small, fixed number ofdigital modules such as counters and serial interfaces. These areoften required to handle sensors where the physical parametercannot easily be converted into a voltage. Other sensors arepacked into modules that include converters and communicatevia SPI or I2C. Finallly, many designs require outputcapabilities beyond simple on/off.The gluion approaches these challenges thru its FPGA-baseddesign which allows for a large number of digital I/O modules.It also provides superior flexibility regarding theirconfiguration, resolution, and functionality. In addition, theFPGA enables a software implementation of the host link - inthe case of the gluion the OSC protocol as well as theunderlying Ethernet layers.},
address = {Paris, France},
author = {Kartadinata, Sukandar},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Kartadinata - 2006 - The Gluion Advantages of an {\{}FPGA{\}}-based Sensor Interface.pdf:pdf},
keywords = {actuators,digital sensors,fpga,osc,sensor interfaces},
pages = {93--96},
title = {{The Gluion Advantages of an {\{}FPGA{\}}-based Sensor Interface}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}093.pdf},
year = {2006}
}
@inproceedings{Holm2010,
address = {Sydney, Australia},
author = {Holm, Jukka and Holm, Harri and Sepp{\"{a}}nen, Jarno},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {and musical genre,context,emoticon,emotion,facial expression,mood,music,music recommendation,nime10},
number = {Nime},
pages = {383--386},
title = {{Associating Emoticons with Musical Genres}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}383.pdf},
year = {2010}
}
@inproceedings{Coduys2004,
address = {Hamamatsu, Japan},
author = {Coduys, Thierry and Henry, Cyrille and Cont, Arshia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
pages = {205--206},
title = {{TOASTER and KROONDE: High-Resolution and High- Speed Real-time Sensor Interfaces}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}205.pdf},
year = {2004}
}
@inproceedings{Wang2004,
address = {Hamamatsu, Japan},
author = {Wang, Ge and Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {code as interface,compiler,concurrency,concurrent audio programming,on-the-fly programming,real-,synchronization,synthesis,time,timing,virtual machine},
pages = {138--143},
title = {{On-the-fly Programming: Using Code as an Expressive Musical Instrument}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}138.pdf},
year = {2004}
}
@inproceedings{nime2014:jlong,
abstract = {This paper describes the Robotic Taishogoto, a new robotic musical instrument for performance, musical installations, and educational purposes. The primary goals of its creation is to provide an easy to use, cost effective, compact and integrated acoustic instrument which is fully automated and controllable via standard MIDI commands. This paper describes the technical details of its design and implementation including the mechanics, electronics and firmware. It also outlines various control methodologies and use cases for the instrument.},
address = {London, United Kingdom},
author = {Long, Jason},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {479--482},
publisher = {Goldsmiths, University of London},
title = {{The Robotic Taishogoto: A New Plug 'n Play Desktop Performance Instrument}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}313.pdf},
year = {2014}
}
@inproceedings{Bortz:2013,
abstract = {Mountains and Valleys (an anonymous name for confidentiality) is a communal,site-specific installation that takes shape as a spatially-responsiveaudio-visual field. The public participates in the creation of theinstallation, resulting in shared ownership of the work between both theartists and participants. Furthermore, the installation takes new shape in eachrealization, both to incorporate the constraints and affordances of eachspecific site, as well as to address the lessons learned from the previousiteration. This paper describes the development and execution of Mountains andValleys over its most recent version, with an eye toward the next iteration ata prestigious art museum during a national festival in Washington, D.C.},
address = {Daejeon, Republic of Korea},
author = {Bortz, Brennon and Ishida, Aki and Bukvic, Ivica Ico and Knapp, R Benjamin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Japanese lanterns,Participatory creation,communal interaction,fields,interactive installation},
pages = {73--78},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Lantern Field: Exploring Participatory Design of a Communal, Spatially Responsive Installation}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{tmagnusson:2014,
abstract = {Live coding emphasises improvisation. It is an art practice that merges the act of musical composition and performance into a public act of projected writing. This paper introduces the Threnoscope system, which includes a live coding micro-language for drone-based microtonal composition. The paper discusses the aims and objectives of the system, elucidates the design decisions, and introduces in particular the code score feature present in the Threnoscope. The code score is a novel element in the design of live coding systems allowing for improvisation through a graphic score, rendering a visual representation of past and future events in a real-time performance. The paper demonstrates how the system's methods can be mapped ad hoc to GUIor hardware-based control.},
address = {London, United Kingdom},
author = {Magnusson, Thor},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {19--22},
publisher = {Goldsmiths, University of London},
title = {{Improvising with the Threnoscope: Integrating Code, Hardware, GUI, Network, and Graphic Scores}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}276.pdf},
year = {2014}
}
@inproceedings{cgeiger:2014,
abstract = {We present a system that allows users to experience singing without singing using gesture-based interaction techniques. We designed a set of body-related interaction and multi-modal feedback techniques and developed a singing voice synthesizer system that is controlled by the user's mouth shapes and arm gestures. Based on the adaption of a number of digital media-related techniques such as face and body tracking, 3D rendering, singing voice synthesis and physical computing, we developed a media installation that allows users to perform an aria without real singing and provide the look and feel from a 20th century performance of an opera singer. We evaluated this system preliminarily with users.},
address = {London, United Kingdom},
author = {Poepel, Cornelius and Feitsch, Jochen and Strobel, Marco and Geiger, Christian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {359--362},
publisher = {Goldsmiths, University of London},
title = {{Design and Evaluation of a Gesture Controlled Singing Voice Installation}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}439.pdf},
year = {2014}
}
@inproceedings{Bowers2006,
address = {Paris, France},
author = {Bowers, John and Villar, Nicolas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Ad hoc instruments,Pin{\&}Play,music performance,new interfaces for musical expression.,physical interfaces},
pages = {234--239},
title = {{Creating Ad Hoc Instruments with Pin{\&}Play{\&}Perform}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}234.pdf},
year = {2006}
}
@inproceedings{Garcia2011a,
address = {Oslo, Norway},
author = {Garcia, J{\'{e}}r{\'{e}}mie and Tsandilas, Theophanis and Agon, Carlos and Mackay, Wendy E},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {composer,creativity,design exploration,inksplorer,interac-,openmusic,technology probes,tive paper},
number = {June},
pages = {361--366},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{InkSplorer : Exploring Musical Ideas on Paper and Computer}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}361.pdf},
year = {2011}
}
@inproceedings{Schwarz:2012,
abstract = {Corpus-based concatenative synthesis is a fairly recent sound synthesis method, based on descriptor analysis of any number of existing or live-recorded sounds, and synthesis by selection of sound segments from the database matching given sound characteristics. It is well described in the literature, but has been rarely examined for its capacity as a new interface for musical expression. The interesting outcome of such an examination is that the actual instrument is the space of sound characteristics, through which the performer navigates with gestures captured by various input devices. We will take a look at different types of interaction modes and controllers (positional, inertial, audio analysis) and the gestures they afford, and provide a critical assessment of their musical and expressive capabilities, based on several years of musical experience, performing with the CataRT system for real-time CBCS.},
address = {Ann Arbor, Michigan},
author = {Schwarz, Diemo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {CataRT,corpus-based concatenative synthesis,gesture},
publisher = {University of Michigan},
title = {{The Sound Space as Musical Instrument: Playing Corpus-Based Concatenative Synthesis}},
year = {2012}
}
@inproceedings{Lamb2011,
address = {Oslo, Norway},
author = {Lamb, Roland and Robertson, Andrew},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {continuous and discrete,control,haptic feedback,hci,human-computer interaction,piano keyboard-related interface},
number = {June},
pages = {503--506},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Seaboard : a New Piano Keyboard-related Interface Combining Discrete and Continuous Control}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}503.pdf},
year = {2011}
}
@inproceedings{Pirro2011,
address = {Oslo, Norway},
author = {Pirr{\`{o}}, David and Eckel, Gerhard},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {embod-,enactive interfaces,has been ap-,iment,interaction,motion tracking,of sound and music,physical modelling,to movement and gesture},
number = {June},
pages = {461--464},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Physical Modelling Enabling Enaction: an Example}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}461.pdf},
year = {2011}
}
@inproceedings{Pigott2011,
address = {Oslo, Norway},
author = {Pigott, Jon},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {electromechanical sonic art,infinite spring,kinetic sound art,prepared,speakers},
number = {June},
pages = {84--87},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Vibration , Volts and Sonic Art: A Practice and Theory of Electromechanical Sound}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}084.pdf},
year = {2011}
}
@inproceedings{Tindale2005,
abstract = {Drum controllers designed by researchers and commercialcompanies use a variety of techniques for capturing percussive gestures. It is challenging to obtain both quick responsetimes and low-level data (such as position) that contain expressive information. This research is a comprehensive studyof current methods to evaluate the available strategies andtechnologies. This study aims to demonstrate the benefitsand detriments of the current state of percussion controllersas well as yield tools for those who would wish to conductthis type of study in the future.},
address = {Vancouver, BC, Canada},
author = {Tindale, Adam R and Kapur, Ajay and Tzanetakis, George and Driessen, Peter and Schloss, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Electronic Percussion,Percussion Controllers,Sensors for Interface Design,Timbre-recognition based instruments},
pages = {200--203},
title = {{A Comparison of Sensor Strategies for Capturing Percussive Gestures}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}200.pdf},
year = {2005}
}
@inproceedings{Rosselet:2013,
abstract = {This paper presents the musical interactions aspects of the design anddevelopment of a web-based interactive music collaboration system called JamOn. Following a design science approach, this system is being built accordingto principles taken from usability engineering and human computer interaction(HCI). The goal of the system is to allow people with no to little musicalbackground to play a song collaboratively. The musicians control the musicalcontent and structure of the song thanks to an interface relying on the freeinking metaphor. One contribution of this interface is that it displays musicalpatterns of different lengths in the same space. The design of Jam On is basedon a list of performance criteria aimed at ensuring the musicality of theperformance and the interactivity of the technical system. The paper comparestwo alternative interfaces used for the system and explores the various stagesof the design process aimed at making the system as musical and interactive aspossible.},
address = {Daejeon, Republic of Korea},
author = {Rosselet, Ulysse and Renaud, Alain},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Networked performance,interface design,mapping,web-based music application},
pages = {394--399},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Jam On: a new interface for web-based collective music performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Hansen2008,
abstract = {This paper describes a project started for implementing DJscratching techniques on the reactable. By interacting withobjects representing scratch patterns commonly performedon the turntable and the crossfader, the musician can playwith DJ techniques and manipulate how they are executedin a performance. This is a novel approach to the digital DJapplications and hardware. Two expert musicians practisedand performed on the reactable in order to both evaluate theplayability and improve the design of the DJ techniques.},
address = {Genoa, Italy},
author = {Hansen, Kjetil F and Alonso, Marcos},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {dj scratch techniques,interfaces,nime08,playability,reactable},
pages = {207--210},
title = {{More DJ Techniques on the reactable}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}207.pdf},
year = {2008}
}
@inproceedings{Newton-Dunn2003,
abstract = {In this paper, we introduce Block Jam, a Tangible UserInterface that controls a dynamic polyrhythmic sequencerusing 26 physical artifacts. These physical artifacts, that wecall blocks, are a new type of input device for manipulatingan interactive music system. The blocks' functional andtopological statuses are tightly coupled to an ad hocsequencer, interpreting the user's arrangement of the blocksas meaningful musical phrases and structures.We demonstrate that we have created both a tangible andvisual language that enables both the novice and musicallytrained users by taking advantage of both their explorativeand intuitive abilities. The tangible nature of the blocks andthe intuitive interface promotes face-to-face collaborationand social interaction within a single system. The principleof collaboration is further extended by linking two BlockJam systems together to create a network.We discuss our project vision, design rational, relatedworks, and the implementation of Block Jam prototypes.Figure 1. A cluster of blocks, note the mother block on thebottom right},
address = {Montreal},
author = {Newton-Dunn, Henry and Nakano, Hiroaki and Gibson, James},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
keywords = {Tangible interface,and musical composition will extend to interactio,but a dynamic structure,modular system,polyrhythmic sequencer. VISION We believe in a fu},
pages = {170--177},
title = {{Block Jam: A Tangible Interface for Interactive Music}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}170.pdf},
year = {2003}
}
@inproceedings{Liebman2010,
address = {Sydney, Australia},
author = {Liebman, Noah and Nagara, Michael and Spiewla, Jacek and Zolkosky, Erin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {audio,control surfaces,mixing board,multitouch,nime10,screen,sound,theatre,touch-,user-centered design},
pages = {51--56},
title = {{Cuebert : A New Mixing Board Concept for Musical Theatre}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}051.pdf},
year = {2010}
}
@inproceedings{Birchfield2006,
abstract = {Physically situated public art poses significant challenges for the design and realization of interactive, electronic sound works. Consideration of diverse audiences, environmental sensitivity, exhibition conditions, and logistics must guide the artwork. We describe our work in this area, using a recently installed public piece, Transition Soundings, as a case study that reveals a specialized interface and open-ended approach to interactive music making. This case study serves as a vehicle for examination of the real world challenges posed by public art and its outcomes. },
address = {Paris, France},
author = {Birchfield, David and Phillips, Kelly and Kidan{\'{e}}, Assegid and Lorig, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Arts,Embedded Electronics.,Installation Art,Interactivity,Music,Network Systems,Public Art,Sculpture,Sound},
pages = {43--48},
title = {{Interactive Public Sound Art: a case study}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}043.pdf},
year = {2006}
}
@inproceedings{Hong:2013,
abstract = {Laptap is a laptop-based, real-time sound synthesis/control system for musicand multimedia performance. The system produces unique sounds by positive audiofeedback between the on-board microphone and the speaker of a laptop com-puter. Users can make a variety of sounds by touching the laptop computer inseveral different ways, and control their timbre with the gestures of the otherhand above the mi- crophone and the speaker to manipulate the characteristicsof the acoustic feedback path. We introduce the basic con- cept of this audiofeedback system, describe its features for sound generation and manipulation,and discuss the result of an experimental performance. Finally we suggest somerelevant research topics that might follow in the future.},
address = {Daejeon, Republic of Korea},
author = {Hong, Dae Ryong and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Laptop music,audio feedback,audio visualization,gestural control,hand gesture,laptop computer,musical mapping,musical notation},
pages = {233--236},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Laptap: Laptop Computer as a Musical Instrument using Audio Feedback}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Lugo2005,
abstract = {This paper describes the design and implementation of BeatBoxing, a percussive gestural interface for the liveperformance of electronic music and control of computerbased games and musical activities.},
address = {Vancouver, BC, Canada},
author = {Lugo, Robert and Damondrick, Jack},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Gestural Mapping,Human- Computer Interaction,Music Controller,OSC,Performance,PureData (Pd)},
pages = {246--247},
title = {{Beat Boxing : Expressive Control for Electronic Music Performance and Musical Applications}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}246.pdf},
year = {2005}
}
@inproceedings{Schnell2002,
address = {Dublin, Ireland},
author = {Schnell, Norbert and Battier, Marc},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
pages = {156--160},
title = {{Introducing Composed Instruments, Technical and Musicological Implications}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}156.pdf},
year = {2002}
}
@inproceedings{Honigman:2013,
abstract = {This paper describes a new framework for music creation using 3D audio andvisual techniques. It describes the Third Room, which uses a Kinect to placeusers in a virtual environment to interact with new instruments for musicalexpression. Users can also interact with smart objects, including the Ember(modified mbira digital interface) and the Fluid (a wireless six degrees offreedom and touch controller). This project also includes new techniques for 3Daudio connected to a 3D virtual space using multi-channel speakers anddistributed robotic instruments.},
address = {Daejeon, Republic of Korea},
author = {Honigman, Colin and Walton, Andrew and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Arduino,Interface,Kinect Camera,Natural Interaction,Robotics,Third Space,Virtual Reality},
pages = {29--34},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{The Third Room: A 3D Virtual Music Framework}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{nime2014:chutchins,
abstract = {Government spying on internet traffic has seemingly become ubiquitous. Not to be left out, the private sector tracks our online footprint via our ISP or with a little help from facebook. Web services, such as advertisement servers and Google track our progress as we surf the net and click on links. The Mozilla plugin, Lightbeam (formerly Collusion), shows the user a visual map of every site a surfer sends data to. A interconnected web of advertisers and other (otherwise) invisible data-gatherers quickly builds during normal usage. We propose modifying this plugin so that as the graph builds, its state is broadcast visa OSC. Members of BiLE will receive and interpret those OSC messages in SuperCollider and PD. We will act as a translational object in a process of live-sonification. The collected data is the material with which we will develop a set of music tracks based on patterns we may discover. The findings of our data collection and the developed music will be presented in the form of an audiovisual live performance. Snippets of collected text and URLs will both form the basis of our audio interpretation, but also be projected on to a screen, so an audience can voyeuristically experience the actions taken on their behalf by governments and advertisers. After the concert, all of the scripts and documentation related to the data collection and sharing in the piece will be posted to github under a GPL license.},
address = {London, United Kingdom},
author = {Hutchins, Charles and Ballweg, Holger and Knotts, Shelly and Hummel, Jonas and Roberts, Antonio},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {497--498},
publisher = {Goldsmiths, University of London},
title = {{Soundbeam: A Platform for Sonyfing Web Tracking}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}447.pdf},
year = {2014}
}
@inproceedings{Lehrman2005,
address = {Vancouver, BC, Canada},
author = {Lehrman, Paul D and Ryan, Todd M},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {136--139},
title = {{Bridging the Gap Between Art and Science Education Through Teaching Electronic Musical Instrument Design}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}136.pdf},
year = {2005}
}
@inproceedings{Martin2010a,
abstract = {In 2009 the cross artform group, Last Man to Die, presenteda series of performances using new interfaces and networkedperformance to integrate the three artforms of its members(actor, Hanna Cormick, visual artist, Benjamin Forster andpercussionist, Charles Martin). This paper explains ourartistic motivations and design for a computer vision surfaceand networked heartbeat sensor as well as the experience ofmounting our first major work, Vital LMTD.},
address = {Sydney, Australia},
author = {Martin, Charles and Forster, Benjamin and Cormick, Hanna},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {cross-artform performance,networked performance,physi- cal computing},
pages = {204--207},
title = {{Cross-Artform Performance Using Networked Interfaces : Last Man to Die's Vital LMTD}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}204.pdf},
year = {2010}
}
@inproceedings{nime2014:fhashimoto,
abstract = {In recent years, there has been an increase in the number of artists who make use of automated music performances in their music and live concerts. Automated music performance is a form of music production using programmed musical notes. Some artists who introduce automated music performance operate parameters of the sound in their performance for production of their music. In this paper, we focus on the music production aspects and describe a method that realizes operation of the sound parameters via computer. Further, in this study, the probability distribution of the action (i.e., variation of parameters) is obtained within the music, using Bayesian filters. The probability distribution of each piece of music is transformed by passing through a Markov model. After the probability distribution is obtained, sound parameters can be automatically controlled. We have developed a system to reproduce the musical expressions of humans and confirmed the possibilities of our method.},
address = {London, United Kingdom},
author = {Hashimoto, Fumito and Miura, Motoki},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {347--350},
publisher = {Goldsmiths, University of London},
title = {{Operating Sound Parameters Using Markov Model and Bayesian Filters in Automated Music Performance}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}380.pdf},
year = {2014}
}
@inproceedings{Cook2007,
address = {New York City, NY, United States},
author = {Cook, Andrew A and Pullin, Graham},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {1,affordance,background and problem space,cultural probes,design research,improvisation,interaction design,nime07,performance},
pages = {285--288},
title = {{Tactophonics : Your Favourite Thing Wants to Sing}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}285.pdf},
year = {2007}
}
@inproceedings{Leider2009,
address = {Pittsburgh, PA, United States},
author = {Leider, Colby},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {333},
title = {{Afflux/Reflux}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}333.pdf},
year = {2009}
}
@inproceedings{nime2014:slee1,
abstract = {This work introduces a way to distribute mobile applications using mobile ad-hoc network in the context of audience participation. The goal is to minimize user configuration so that the process is highly accessible for casual smartphone users. The prototype mobile applications utilize WiFiDirect and Service Discovery Protocol to distribute code. With the aid of these two technologies, the prototype system requires no infrastructure and minimum user configuration.},
address = {London, United Kingdom},
author = {Lee, Sang Won and Essl, Georg and Mao, Z Morley},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {533--536},
publisher = {Goldsmiths, University of London},
title = {{Distributing Mobile Music Applications for Audience Participation Using Mobile Ad-hoc Network (MANET)}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}546.pdf},
year = {2014}
}
@inproceedings{Dubrau2008,
address = {Genoa, Italy},
author = {Dubrau, Josh and Havryliv, Mark},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {nime08},
pages = {164--167},
title = {{P[a]ra[pra]xis : Poetry in Motion}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}164.pdf},
year = {2008}
}
@inproceedings{Donnarumma:2013,
abstract = {We present the first combined use of the electromyogram (EMG) andmechanomyogram (MMG), two biosignals that result from muscular activity, forinteractive music applications. We exploit differences between these twosignals, as reported in the biomedical literature, to create bi-modalsonification and sound synthesis mappings that allow performers to distinguishthe two components in a single complex arm gesture. We study non-expertplayers' ability to articulate the different modalities. Results show thatpurposely designed gestures and mapping techniques enable novices to rapidlylearn to independently control the two biosignals.},
address = {Daejeon, Republic of Korea},
author = {Donnarumma, Marco and Caramiaux, Baptiste and Tanaka, Atau},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {EMG,MMG,NIME,biosignal,mapping,multimodal,sensorimotor system},
pages = {128--131},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Muscular Interactions. Combining EMG and MMG sensing for musical practice}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Cont2004,
address = {Hamamatsu, Japan},
author = {Cont, Arshia and Coduys, Thierry and Henry, Cyrille},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {adaptive interfaces,algorithms and intelligent controllers,musical mapping,pure data,real-time gesture control,sensor and actuator,technologies for musical applications},
pages = {39--42},
title = {{Real-time Gesture Mapping in Pd Environment using Neural Networks}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}039.pdf},
year = {2004}
}
@inproceedings{Smyth2006,
address = {Paris, France},
author = {Smyth, Tamara},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {khaen,mapping,musical acoustics,sound synthesis control},
pages = {314--317},
title = {{Handheld Acoustic Filter Bank for Musical Control}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}314.pdf},
year = {2006}
}
@inproceedings{Buchla2005,
address = {Vancouver, BC, Canada},
author = {Buchla, Don},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
number = {1965},
pages = {1},
title = {{A History of Buchla's Musical Instruments}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}001.pdf},
year = {2005}
}
@inproceedings{nime2014:aandersson,
abstract = {In this paper we explore how we compose sound for an interactive tangible and mobile interface; where the goal is to improve health and well-being for families with children with disabilities. We describe the composition process from how we decompose a linear beat-based and vocal sound material; recompose it with real-time audio synthesis and composition rules into interactive Scenes. Scenes that make it possible for the user to select, explore and recreate different ``sound worlds" with the tangible interface as an instrument; create and play with it as a friend; improvise and create; or relax with it as an ambient sounding furniture. We continue discussing a user story, how the Scenes are recreated by amateur users, persons with severe disabilities and family members; improvising with the mobile tangibles. We discuss composition techniques for mixing sound, tangible-physical and lighting elements in the Scenes. Based on observations we explore how a diverse audience in the family and at school can recreate and improvise their own sound experience and play together with others. We conclude by discussing the possible impact of our findings for the NIME-community; how the techniques of decomposing, recomposing and recreating sound, based on a relational perspective, could contribute to the design of new instruments for musical expression.},
address = {London, United Kingdom},
author = {Andersson, Anders-Petter and Cappelen, Birgitta and Olofsson, Fredrik},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {529--532},
publisher = {Goldsmiths, University of London},
title = {{Designing Sound for Recreation and Well-Being}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}572.pdf},
year = {2014}
}
@inproceedings{Miyama2009,
address = {Pittsburgh, PA, United States},
author = {Miyama, Chikashi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {326},
title = {{Angry Sparrow}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}326.pdf},
year = {2009}
}
@inproceedings{Rosselet:2013,
abstract = {This paper presents the musical interactions aspects of the design anddevelopment of a web-based interactive music collaboration system called JamOn. Following a design science approach, this system is being built accordingto principles taken from usability engineering and human computer interaction(HCI). The goal of the system is to allow people with no to little musicalbackground to play a song collaboratively. The musicians control the musicalcontent and structure of the song thanks to an interface relying on the freeinking metaphor. One contribution of this interface is that it displays musicalpatterns of different lengths in the same space. The design of Jam On is basedon a list of performance criteria aimed at ensuring the musicality of theperformance and the interactivity of the technical system. The paper comparestwo alternative interfaces used for the system and explores the various stagesof the design process aimed at making the system as musical and interactive aspossible.},
address = {Daejeon, Republic of Korea},
author = {Rosselet, Ulysse and Renaud, Alain},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Networked performance,interface design,mapping,web-based music application},
pages = {394--399},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Jam On: a new interface for web-based collective music performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Lahdeoja2009,
address = {Pittsburgh, PA, United States},
author = {L{\"{a}}hdeoja, Otso},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {augmented instrument,chordophone,contact microphone systems,electric,electronic percussion,even with,guitar,leaving the instrument body,nime09,there is always a,trade-off,virtually mute},
pages = {102--105},
title = {{Augmenting Chordophones with Hybrid Percussive Sound Possibilities}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}102.pdf},
year = {2009}
}
@inproceedings{Trail2012,
abstract = {The Gyil is a pentatonic African wooden xylophone with 14-15 keys. The work described in this paper has been motivated by three applications: computer analysis of Gyil performance, live improvised electro-acoustic music incorporating the Gyil, and hybrid sampling and physical mod- eling. In all three of these cases, detailed information about what is played on the Gyil needs to be digitally captured in real-time. We describe a direct sensing apparatus that can be used to achieve this. It is based on contact microphones and is informed by the specific characteristics of the Gyil. An alternative approach based on indirect acquisition is to apply polyphonic transcription on the signal acquired by a microphone without requiring the instrument to be modified. The direct sensing apparatus we have developed can be used to acquire ground truth for evaluating different approaches to polyphonic transcription and help create a ``surrogate'' sensor. Some initial results comparing different strategies to polyphonic transcription are presented.},
address = {Ann Arbor, Michigan},
author = {Trail, Shawn and Tavares, Tiago Fernandes and Godlovitch, Dan and Tzanetakis, George},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {computational ethnomusicology,hyperinstruments,indirect acquisition,perfor- mance analysis,physical modeling,surrogate sensors},
publisher = {University of Michigan},
title = {{Direct and surrogate sensing for the Gyil african xylophone}},
year = {2012}
}
@inproceedings{McPherson:2013,
abstract = {This paper presents a portable optical measurement system for capturingcontinuous key motion on any piano. Very few concert venues have MIDI-enabledpianos, and many performers depend on the versatile but discontinued MoogPianoBar to provide MIDI from a conventional acoustic instrument. The scannerhardware presented in this paper addresses the growing need for alternativesolutions while surpassing existing systems in the level of detail measured.Continuous key position on both black and white keys is gathered at 1kHz samplerate. Software extracts traditional and novel features of keyboard touch fromeach note, which can be flexibly mapped to sound using MIDI or Open SoundControl. RGB LEDs provide rich visual feedback to assist the performer ininteracting with more complex sound mapping arrangements. An application ispresented to the magnetic resonator piano, an electromagnetically-augmentedacoustic grand piano which is performed using continuous key positionmeasurements.},
address = {Daejeon, Republic of Korea},
author = {McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Piano,gesture sensing,keyboard,magnetic resonator piano,mapping,optical sensing,visual feedback},
month = {may},
pages = {152--157},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Portable Measurement and Mapping of Continuous Piano Gesture}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{nime2014:ahadjakos1,
abstract = {Physical computing platforms such as the Arduino have significantly simplified developing physical musical interfaces. However, those platforms typically target everyday programmers rather than composers and media artists. On the other hand, tangible user interface (TUI) toolkits, which provide an integrated, easy-to-use solution have not gained momentum in modern music creation. We propose a concept that hybridizes physical computing and TUI toolkit approaches. This helps to tackle typical TUI toolkit weaknesses, namely quick sensor obsolescence and limited choices. We developed a physical realization based on the idea of "universal pins," which can be configured to perform a variety of duties, making it possible to connect different sensor breakouts and modules. We evaluated our prototype by making performance measurements and conducting a user study demonstrating the feasibility of our approach.},
address = {London, United Kingdom},
author = {Hadjakos, Aristotelis and Waloschek, Simon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
keywords = {evaluation,usability},
mendeley-tags = {evaluation,usability},
pages = {625--628},
publisher = {Goldsmiths, University of London},
title = {{SPINE: A TUI Toolkit and Physical Computing Hybrid}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}430.pdf},
year = {2014}
}
@inproceedings{Grosshauser2008,
address = {Genoa, Italy},
author = {Grosshauser, Tobias},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {cramping,finger,force,gesture and,left hand,nime08,piano,pressure measurement,right hand,sensor,strings,time line,violin},
pages = {97--102},
title = {{Low Force Pressure Measurement : Pressure Sensor Matrices for Gesture Analysis , Stiffness Recognition and Augmented Instruments}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}097.pdf},
year = {2008}
}
@inproceedings{nime2014:xxiao,
abstract = {We present Andante, a representation of music as animated characters walking along the piano keyboard that appear to play the physical keys with each step. Based on a view of music pedagogy that emphasizes expressive, full-body communication early in the learning process, Andante promotes an understanding of the music rooted in the body, taking advantage of walking as one of the most fundamental human rhythms. We describe three example visualizations on a preliminary prototype as well as applications extending our examples for practice feedback, improvisation and com- position. Through our project, we reflect on some high level considerations for the NIME community.},
address = {London, United Kingdom},
author = {Xiao, Xiao and Tome, Basheer and Ishii, Hiroshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {629--632},
publisher = {Goldsmiths, University of London},
title = {{Andante: Walking Figures on the Piano Keyboard to Visualize Musical Motion}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}467.pdf},
year = {2014}
}
@inproceedings{Hindman2009,
address = {Pittsburgh, PA, United States},
author = {Hindman, David and Drummond, Evan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {332},
title = {{Performance: Modal Kombat Plays PONG}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}332.pdf},
year = {2009}
}
@inproceedings{Wang2005,
address = {Vancouver, BC, Canada},
author = {Wang, Derek},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {264--265},
title = {{Bubbaboard and Mommaspeaker: Creating Digital Tonal Sounds from an Acoustic Percussive Instrument}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}264.pdf},
year = {2005}
}
@inproceedings{Park:2013c,
abstract = {In this paper we present a highly expressive, robust, and easy-to-build systemthat provides force-feedback interaction for mobile computing devices (MCD).Our system, which we call Fortissimo (ff), utilizes standard built-inaccelerometer measurements in conjunction with generic foam padding that can beeasily placed under a device to render an expressive force-feedback performancesetup. Fortissimo allows for musically expressive user-interaction with addedforce-feedback which is integral for any musical controller --a feature that isabsent for touchscreen-centric MCDs. This paper details ff core concepts,hardware and software designs, and expressivity of musical features.},
address = {Daejeon, Republic of Korea},
author = {Park, Tae Hong and Nieto, Oriol},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {expression,force-feedback,mobile computing devices,mobile music},
pages = {291--294},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Fortissimo: Force-Feedback for Mobile Devices}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Uozumi2007,
address = {New York City, NY, United States},
author = {Uozumi, Yuta and Takahashi, Masato and Kobayashi, Ryoho},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {426},
title = {{Bd : A Sound Installation with Swarming Robots}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}426.pdf},
year = {2007}
}
@inproceedings{nime2014:mbretan,
abstract = {As robots become more pervasive in the world we think about how this might influence the way in which people experience music. We introduce the concept of a "robotic musical companion'' (RMC) in the form of Shimi, a smart-phone enabled five degree-of-freedom (DoF) robotic platform. We discuss experiences individuals tend to have with music as consumers and performers and explore how these experiences can be modified, aided, or improved by the inherent synergies between a human and robot. An overview of several applications developed for Shimi is provided. These applications place Shimi in various roles and enable human-robotic interactions (HRIs) that are highlighted by more personable social communications using natural language and other forms of communication.},
address = {London, United Kingdom},
author = {Bretan, Mason and Weinberg, Gil},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {315--318},
publisher = {Goldsmiths, University of London},
title = {{Chronicles of a Robotic Musical Companion}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}303.pdf},
year = {2014}
}
@inproceedings{Benning2007,
address = {New York City, NY, United States},
author = {Benning, Manjinder S and McGuire, Michael and Driessen, Peter},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {gesture tracking,interacting,kalman filtering,nime07,radiodrum},
pages = {334--337},
title = {{Improved Position Tracking of a 3-D Gesture-Based Musical Controller Using a Kalman Filter}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}334.pdf},
year = {2007}
}
@inproceedings{Johnston2009,
address = {Pittsburgh, PA, United States},
author = {Johnston, Andrew and Candy, Linda and Edmonds, Ernest},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {acoustic sounds captured via,gestures,in force being applied,instruments,interaction,microphone are the source,music,nime09,of,these musical gestures result,to,virtual instruments,which act upon the},
pages = {207--212},
title = {{Designing for Conversational Interaction}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}207.pdf},
year = {2009}
}
@inproceedings{Fornari2007,
address = {New York City, NY, United States},
author = {Fornari, Jose and Maia, Adolfo Jr. and Manzolli, Jonatas},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {adaptation,evolutionary,interactive,nime07,sound,spatialization},
pages = {293--298},
title = {{Interactive Spatialization and Sound Design using an Evolutionary System}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}293.pdf},
year = {2007}
}
@inproceedings{Hayafuchi2008,
address = {Genoa, Italy},
author = {Hayafuchi, Kouki and Suzuki, Kenji},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {body,embodied sound media,gestures,motion,music controller,musical interface,nime08},
pages = {241--244},
title = {{MusicGlove : A Wearable Musical Controller for Massive Media Library}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}241.pdf},
year = {2008}
}
@inproceedings{Tarakajian:2013,
abstract = {Mira is an iPad app for controlling Max patchers in real time with minimalconfiguration. This submission includes a paper describing Mira's design andimplementation, as well as a demo showing how Mira works with Max.The Mira iPad app discovers open Max patchers automatically using the Bonjourprotocol, connects to them over WiFi and negotiates a description of the Maxpatcher. As objects change position and appearance, Mira makes sure that theinterface on the iPad is kept up to date. Mira eliminates the need for anexplicit mapping step between the interface and the system being controlled.The user is never asked to input an IP address, nor to configure the mappingbetween interface objects on the iPad and those in the Max patcher. So theprototyping composer is free to rapidly configure and reconfigure theinterface.},
address = {Daejeon, Republic of Korea},
author = {Tarakajian, Sam and Zicarelli, David and Clayton, Joshua},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Max/MSP/Jitter,Mira,NIME,bonjour,ipad,osc,zeroconf},
pages = {421--426},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Mira: Liveness in iPad Controllers for Max/MSP}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Parson2009,
address = {Pittsburgh, PA, United States},
author = {Parson, Dale E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {algorithmic composition,chess,chuck,improvisation,mapping software within the,max,msp,nime09,playing chess,supercollider,system allows two players,to create music by,using networked computers},
pages = {157--158},
title = {{Chess-Based Composition and Improvisation for Non-Musicians}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}157.pdf},
year = {2009}
}
@inproceedings{Camurri2007,
address = {New York City, NY, United States},
author = {Camurri, Antonio and Canepa, Corrado and Volpe, Gualtiero},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {active listening of music,analysis and expressive gesture,expressive interfaces,full-body motion,multimodal interactive,nime07,performing arts applications,processing,systems for music and},
pages = {56--61},
title = {{Active Listening to a Virtual Orchestra Through an Expressive Gestural Interface : The Orchestra Explorer}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}056.pdf},
year = {2007}
}
@inproceedings{Allison2012,
abstract = {Aural - of or relateing to the ear or hearing Aura - an invisible breath, emanation, or radiation AR - Augmented Reality AuRal is an environmental audio system in which individual participants form ad hoc ensembles based on geolocation and affect the overall sound of the music associated with the location that they are in. The AuRal environment binds physical location and the choices of multiple, simultaneous performers to act as the generative force of music tied to the region. Through a mobile device interface, musical participants, or agents, have a degree of input into the generated music essentially defining the sound of a given region. The audio landscape is superimposed onto the physical one. The resultant musical experience is not tied simply to the passage of time, but through the incorporation of participants over time and spatial proximity, it becomes an aural location as much as a piece of music. As a result, walking through the same location at different times results in unique collaborative listening experiences.},
address = {Ann Arbor, Michigan},
author = {Allison, Jesse and Dell, Christian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {AuRal,android,distributed performance system,mobile music,ruby on rails,sonic environment,supercollider},
publisher = {University of Michigan},
title = {{AuRal: A Mobile Interactive System for Geo-Locative Audio Synthesis}},
year = {2012}
}
@inproceedings{Fels2009,
address = {Pittsburgh, PA, United States},
author = {Fels, Sidney S and Pritchard, Bob and Lenters, Allison},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {274--275},
title = {{ForTouch : A Wearable Digital Ventriloquized Actor}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}274.pdf},
year = {2009}
}
@inproceedings{Leider2009a,
abstract = {We describe initial prototypes and a design strategy for new, user-customized audio-manipulation and editing tools. These tools are designed to enable intuitive control of audio-processing tasks while anthropomorphically matching the target user. },
address = {Pittsburgh, PA, United States},
author = {Leider, Colby and Mann, Doug and Plazas, Daniel and Battaglia, Michael and Draper, Reid},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {user customization,user modeling},
pages = {147--148},
title = {{The elBo and footPad : Toward Personalized Hardware for Audio Manipulation}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}147.pdf},
year = {2009}
}
@inproceedings{Fernstrom2007,
address = {New York City, NY, United States},
author = {Torre, Giuseppe and Fernstr{\"{o}}m, Mikael and O'Flynn, Brendan and Angove, Philip},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {dance performance,graphical object,imu,inertial measurement unit,interactive,mapping,nime07,position tracking},
pages = {205--208},
title = {{Celeritas : Wearable Wireless System}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}205.pdf},
year = {2007}
}
@inproceedings{Rodrigues2005,
address = {Vancouver, BC, Canada},
author = {Rodrigues, Paulo Maria and Gir{\~{a}}o, Luis Miguel and Gehlhaar, Rolf},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {164--167},
title = {{CyberSong}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}164.pdf},
year = {2005}
}
@inproceedings{Tarakajian:2013,
abstract = {Mira is an iPad app for controlling Max patchers in real time with minimalconfiguration. This submission includes a paper describing Mira's design andimplementation, as well as a demo showing how Mira works with Max.The Mira iPad app discovers open Max patchers automatically using the Bonjourprotocol, connects to them over WiFi and negotiates a description of the Maxpatcher. As objects change position and appearance, Mira makes sure that theinterface on the iPad is kept up to date. Mira eliminates the need for anexplicit mapping step between the interface and the system being controlled.The user is never asked to input an IP address, nor to configure the mappingbetween interface objects on the iPad and those in the Max patcher. So theprototyping composer is free to rapidly configure and reconfigure theinterface.},
address = {Daejeon, Republic of Korea},
author = {Tarakajian, Sam and Zicarelli, David and Clayton, Joshua},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Max/MSP/Jitter,Mira,NIME,bonjour,ipad,osc,zeroconf},
month = {may},
pages = {421--426},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Mira: Liveness in iPad Controllers for Max/MSP}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Hattwick:2012a,
abstract = {The Physical Computing Ensemble was created in order to determine the viability of an approach to musical performance which focuses on the relationships and interactions of the performers. Three performance systems utilizing gestural controllers were designed and implemented, each with a different strategy for performer interaction.
These strategies took advantage of the opportunities for collaborative performance inherent in digital musical instruments due to their networking abilities and reconfigurable software. These characteristics allow for the easy implementation of varying approaches to collaborative performance. Ensembles who utilize digital musical instruments provide a fertile environment for the design, testing, and utilization of collaborative performance systems.
The three strategies discussed in this paper are the parameterization of musical elements, turn-based collaborative control of sound, and the interaction of musical systems created by multiple performers. Design principles, implementation, and a performance using these strategies are discussed, and the conclusion is drawn that performer interaction and collaboration as a primary focus for system design, composition, and performance is viable.},
address = {Ann Arbor, Michigan},
author = {Hattwick, Ian and Umezaki, Kojiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Collaborative performance,Wii,digital music ensemble,digital musical instruments,gestural controller,interaction},
publisher = {University of Michigan},
title = {{Approaches to Interaction in a Digital Music Ensemble}},
year = {2012}
}
@inproceedings{Hashida2008,
address = {Genoa, Italy},
author = {Hashida, Mitsuyo and Ito, Yosuke and Katayose, Haruhiro},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {2,case-based approach,interface of a case-based,nime08,performance rendering,performance rendering system,user interface},
pages = {277--280},
title = {{A Directable Performance Rendering System : Itopul}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}277.pdf},
year = {2008}
}
@inproceedings{Bianchi2009,
address = {Pittsburgh, PA, United States},
author = {Bianchi, Andrea and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {2,collaborative interface,expression,figure 1,game control,motivations and related work,multiplayer,musical,musical control,nime09,nintendo ds,on a nintendo ds,the drummer client running},
pages = {316--319},
title = {{The Drummer : a Collaborative Musical Interface with Mobility}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}316.pdf},
year = {2009}
}
@inproceedings{Henriques:2012,
abstract = {The Sonik Spring is a portable and wireless digital instrument, created for real-time synthesis and control of sound. It brings together different types of sensory input, linking gestural motion and kinesthetic feedback to the production of sound. The interface consists of a 15-inch spring with unique flexibility, which allows multiple degrees of variation in its shape and length. The design of the instrument is described and its features discussed. Three performance modes are detailed highlighting the instrument's expressive potential and wide range of functionality. },
address = {Ann Arbor, Michigan},
author = {Henriques, Tomas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Gestural control of sound,Interface for sound and music,Kinesthetic and visual feedback},
publisher = {University of Michigan},
title = {{SONIK SPRING}},
year = {2012}
}
@inproceedings{Lippit2006,
abstract = {Turntable musicians have yet to explore new expressions with digital technology. New higher-level development tools open possibilities for these artists to build their own instruments that can achieve artistic goals commercial products cannot. This paper will present a rough overview on the practice and recent development of turntable music, followed by descriptions of two projects by the ,
,
author. },
address = {Paris, France},
author = {Lippit, Takuro M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {DJ,Max/MSP,PIC Microcontroller,Physical Computing,Turntable music,improvisation,turntablist},
pages = {71--74},
title = {{Turntable Music in the Digital Era: Designing Alternative Tools for New Turntable Expression}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}071.pdf},
year = {2006}
}
@inproceedings{Orio2003,
address = {Montreal, QC, Canada},
author = {Orio, Nicola and Lemouton, Serge and Schwarz, Diemo},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {36--41},
title = {{Score Following: State of the Art and New Developments}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}036.pdf},
year = {2003}
}
@inproceedings{pvandertorren2014,
abstract = {The Striso is a new expressive music instrument with an acoustic feel, which is designed to be intuitive to play and playable everywhere. The sound of every note can be precisely controlled using the direction and pressure sensitive buttons, combined with instrument motion like tilting or shaking. It works standalone, with an internal speaker and battery, and is meant as a self contained instrument with its own distinct sound, but can also be connected to a computer to control other synthesizers. The notes are arranged in an easy and systematic way, according to the new DCompose note layout that is also presented in this paper. The DCompose note layout is designed to be compact, ergonomic, easy to learn, and closely bound to the harmonic properties of the notes.},
address = {London, United Kingdom},
author = {van der Torren, Piers Titus},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {615--620},
publisher = {Goldsmiths, University of London},
title = {{Striso, a Compact Expressive Instrument Based on a New Isomorphic Note Layout}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}442.pdf},
year = {2014}
}
@inproceedings{Ahmaniemi2010,
address = {Sydney},
author = {Ahmaniemi, Teemu},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {gesture,motor control,nime10,tactile feedback,virtual instrument},
number = {Nime},
pages = {485--488},
title = {{Gesture Controlled Virtual Instrument with Dynamic Vibrotactile Feedback}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}485.pdf},
year = {2010}
}
@inproceedings{Hollinger:2012,
abstract = {A modular and reconfigurable hardware platform for analog optoelectronic signal acquisition is presented. Its intended application is for fiber optic sensing in electronic musical interfaces, however the flexible design enables its use with a wide range of analog and digital sensors. Multiple gain and multiplexing stages as well as programmable analog and digital hardware blocks allow for the acquisition, processing, and communication of single-ended and differential signals. Along with a hub board, multiple acquisition boards can be connected to modularly extend the system's capabilities to suit the needs of the application. Fiber optic sensors and their application in DMIs are briefly discussed, as well as the use of the hardware platform with specific musical interfaces.},
address = {Ann Arbor, Michigan},
author = {Hollinger, Avrum and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {MRI-compatible,analog signal acquisition,fiber optic sensing,musical interface},
publisher = {University of Michigan},
title = {{Optoelectronic Acquisition and Control Board for Musical Applications}},
year = {2012}
}
@inproceedings{Carey:2012,
abstract = {This paper presents the author's derivations system, an interactive performance system for solo improvising instrumentalist. The system makes use of a combination of real- time audio analysis, live sampling and spectral re-synthesis to build a vocabulary of possible performative responses to live instrumental input throughout an improvisatory performance. A form of timbral matching is employed to form a link between the live performer and an expanding database of musical materials. In addition, the system takes into account the unique nature of the rehearsal/practice space in musical performance through the implementation of performer-configurable cumulative rehearsal databases into the final design. This paper discusses the system in detail with reference to related work in the field, making specific reference to the system's interactive potential both inside and outside of a real-time performance context.},
address = {Ann Arbor, Michigan},
author = {Carey, Benjamin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Interactivity,improvisation,performance systems},
publisher = {University of Michigan},
title = {{Designing for Cumulative Interactivity: The {\_}derivations System}},
year = {2012}
}
@inproceedings{Nash2008,
abstract = {Over the last century, composers have made increasingly ambitious experiments with musical time, but have been impeded in expressing more temporally-complex musical processes by the limitations of both music notations and human performers. In this paper, we describe a computer-based notation and gestural control system for independently manipulating the tempi of musical parts within a piece, at performance time. We describe how the problem was approached, drawing upon feedback and suggestions from consultations across multiple disciplines, seeking analogous problems in other fields. Throughout, our approach is guided and, ultimately, assessed by an established professional composer, who was able to interact with a working prototype of the system. },
address = {Genoa, Italy},
author = {Nash, Chris and Blackwell, Alan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {composition,gesture,nime08,performance,polytempi,realtime,tempo},
pages = {28--33},
title = {{Realtime Representation and Gestural Control of Musical Polytempi}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}028.pdf},
year = {2008}
}
@inproceedings{Grosshauser:2013,
abstract = {Several new technologies to capture motion, gesture and forces for musical instrument players' analyses have been developed in the last years. In research and for augmented instruments one parameter is underrepresented so far. It is finger position and pressure measurement, applied by the musician while playing the musical instrument. In this paper we show a flexible linear-potentiometer and forcesensitive-resistor (FSR) based solution for position, pressure and force sensing between the contact point of the fingers and the musical instrument. A flexible matrix printed circuit board (PCB) is fixed on a piano key. We further introduce linear potentiometer based left hand finger position sensing for string instruments, integrated into a violin and a guitar finger board. Several calibration and measurement scenarios are shown. The violin sensor was evaluated with 13 music students regarding playability and robustness of the system. Main focus was a the integration of the sensors into these two traditional musical instruments as unobtrusively as possible to keep natural haptic playing sensation. The musicians playing the violin in different performance situations stated good playability and no differences in the haptic sensation while playing. The piano sensor is rated, due to interviews after testing it in a conventional keyboard quite unobtrusive, too, but still evokes a different haptic sensation.},
address = {Daejeon, Republic of Korea},
author = {Grosshauser, Tobias and Tr{\"{o}}ster, Gerhard},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Guitar,Keyboard,Piano,Position,Pressure,Sensor,Violin},
pages = {479--484},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Finger Position and Pressure Sensing Techniques for String and Keyboard Instruments}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Hornof2007,
abstract = {In this project, eye tracking researchers and computer music composers collaborate to create musical compositions that are played with the eyes. A commercial eye tracker (LC Technologies Eyegaze) is connected to a music and multimedia ,
,
authoring environment (Max/MSP/Jitter). The project addresses issues of both noise and control: How will the performance benefit from the noise inherent in eye trackers and eye movements, and to what extent should the composition encourage the performer to try to control a specific musical outcome? Providing one set of answers to these two questions, the ,
,
authors create an eye-controlled composition, EyeMusic v1.0, which was selected by juries for live performance at computer music conferences.,
,
author KeywordsComputer music, eye tracking, new media art, performance.ACM Classification },
address = {New York City, NY, United States},
author = {Hornof, Anthony J and Rogers, Troy and Halverson, Tim},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {H.5.2 [Information Interfaces and Presentation] Us,interaction styles. J.5 [Arts and Humanities] Fin,performing arts.},
pages = {299--300},
title = {{EyeMusic : Performing Live Music and Multimedia Compositions with Eye Movements}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}299.pdf},
year = {2007}
}
@inproceedings{Schlessinger:2012,
abstract = {We have developed a prototype wireless microphone that provides vocalists with control over their vocal effects directly from the body of the microphone. A wireless microphone has been augmented with six momentary switches, one fader, and three axes of motion and position sensors, all of which provide MIDI output from the wireless receiver. The MIDI data is used to control external vocal effects units such as live loopers, reverbs, distortion pedals, etc. The goal was to to provide dramatically increased expressive control to vocal performances, and address some of the shortcomings of pedal-controlled effects. The addition of gestural controls from the motion sensors opens up new performance possibilities such as panning the voice simply by pointing the microphone in one direction or another. The result is a hybrid microphone-musical instrument which has recieved extremely positive results from vocalists in numerous infor- mal workshops.},
address = {Ann Arbor, Michigan},
author = {Schlessinger, Dan Moses},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Concept Tahoe,MIDI,NIME,Sennheiser,control,microphone},
publisher = {University of Michigan},
title = {{Concept Tahoe: Microphone Midi Control}},
year = {2012}
}
@inproceedings{Yoo2011,
address = {Oslo},
author = {Yoo, Min-Joon and Beak, Jin-Wook and Lee, In-Kwon},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {adlib generation,gaming interface,kinect,sound generation},
number = {June},
pages = {324--325},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Creating Musical Expression using Kinect}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}324.pdf},
year = {2011}
}
@inproceedings{Murphy:2013,
abstract = {This paper provides a history of robotic guitars and bass guitars as well as adiscussion of the design, construction, and evaluation of two new roboticinstruments. Throughout the paper, a focus is made on different techniques toextend the expressivity of robotic guitars. Swivel and MechBass, two newrobots, are built and discussed. Construction techniques of likely interest toother musical roboticists are included. These robots use a variety oftechniques, both new and inspired by prior work, to afford composers andperformers with the ability to precisely control pitch and plucking parameters.Both new robots are evaluated to test their precision, repeatability, andspeed. The paper closes with a discussion of the compositional and performativeimplications of such levels of control, and how it might affect humans who wishto interface with the systems.},
address = {Daejeon, Republic of Korea},
author = {Murphy, Jim and McVay, James and Kapur, Ajay and Carnegie, Dale},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {kinetic sculpture,mechatronics,musical robotics},
pages = {557--562},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Designing and Building Expressive Robotic Guitars}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Gillian2011,
abstract = {This paper presents a novel algorithm that has been specifically designed for the recognition of multivariate temporal musical gestures. The algorithm is based on DynamicTime Warping and has been extended to classify any N dimensional signal, automatically compute a classificationthreshold to reject any data that is not a valid gesture andbe quickly trained with a low number of training examples.The algorithm is evaluated using a database of 10 temporalgestures performed by 10 participants achieving an averagecross-validation result of 99{\%}.},
address = {Oslo, Norway},
author = {Gillian, Nicholas and Knapp, Benjamin and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Dynamic Time Warping,Gesture Recognition,Multivariate Temporal Gestures,Musician-Computer Interaction},
pages = {337--342},
title = {{Recognition Of Multivariate Temporal Musical Gestures Using N-Dimensional Dynamic Time Warping}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}337.pdf},
year = {2011}
}
@inproceedings{Freed2009,
abstract = {A compendium of foundational circuits for interfacing resistive pressure and position sensors is presented with example applications for music controllers and tangible interfaces. },
address = {Pittsburgh, PA, United States},
author = {Freed, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Piezoresistive Touch Sensor Pressure Sensing Curre},
pages = {230--235},
title = {{Novel and Forgotten Current-steering Techniques for Resistive Multitouch, Duotouch, and Polytouch Position Sensing with Pressure}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}230.pdf},
year = {2009}
}
@inproceedings{Johnston:2012,
abstract = {Mapping between musical interfaces, and sound engines, is integral to the nature of an interface [3]. Traditionally, mu- sical applications for touch surfaces have directly mapped touch coordinates to control parameters. However, recent work [9] is looking at new methods of control that use rela- tional multi-point analysis. Instead of directly using touch coordinates, which are related to a global screen space, an initial touch is used as an `anchor' to create a local coor- dinate space in which subsequent touches can be located and compared. This local coordinate space frees touches from being locked to one single relationship, and allows for more complex interaction between touch events. So far, this method has only been implemented on Apple computer's small capacitive touch pads. Additionally, there has yet to be a user study that directly compares [9] against mappings of touch events within global coordinate spaces. With this in mind, we have developed and evaluated two interfaces with the aim of determining and quantifying some of these differences within the context of our custom large mutli- touch surfaces [1].},
address = {Ann Arbor, Michigan},
author = {Johnston, Blake and Vallis, Owen and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Multi-Touch,Relational-point interface,User Study},
publisher = {University of Michigan},
title = {{A Comparative User Study of Two Methods of Control on a Multi-Touch Surface for Musical Expression}},
year = {2012}
}
@inproceedings{Jo2008,
abstract = {Monalisa is a software platform that enables to "see the sound, hear the image". It consists of three software: Monalisa Application, Monalisa-Audio Unit, and Monalisa-Image Unit, and an installation: Monalisa "shadow of the sound". In this paper, we describe the implementation of each software and installation with the explanation of the basic algorithms to treat the image data and the sound data transparently.},
address = {Genoa, Italy},
author = {Jo, Kazuhiro and Nagano, Norihisa},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Installation,Plug-in,Sound and Image Processing Software},
pages = {315--318},
title = {{Monalisa : "See the Sound , Hear the Image"}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}315.pdf},
year = {2008}
}
@inproceedings{Chadabe2002,
address = {Dublin, Ireland},
author = {Chadabe, Joel},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {generated by the instrument,has defined the,musician can maintain control,over the sound,sound generator,through which a performing},
pages = {38--42},
title = {{The Limitations of Mapping as a Structural Descriptive in Electronic Instruments}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}038.pdf},
year = {2002}
}
@inproceedings{El-Shimy:2013,
abstract = {For a number of years, musicians in different locations have been able toperform with one another over a network as though present on the same stage.However, rather than attempt to re-create an environment for Network MusicPerformance (NMP) that mimics co-present performance as closely as possible, wepropose focusing on providing musicians with additional controls that can helpincrease the level of interaction between them. To this end, we have developeda reactive environment for distributed performance that provides participantsdynamic, real-time control over several aspects of their performance, enablingthem to change volume levels and experience exaggerated stereo panning. Inaddition, our reactive environment reinforces a feeling of a ``shared space''between musicians. It differs most notably from standard ventures into thedesign of novel musical interfaces and installations in its reliance onuser-centric methodologies borrowed from the field of Human-ComputerInteraction (HCI). Not only does this research enable us to closely examine thecommunicative aspects of performance, it also allows us to explore newinterpretations of the network as a performance space. This paper describes themotivation and background behind our project, the work that has been undertakentowards its realization and the future steps that have yet to be explored.},
address = {Daejeon, Republic of Korea},
author = {El-Shimy, Dalia and Cooperstock, Jeremy R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
pages = {158--163},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Reactive Environment for Network Music Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Han2007,
address = {New York City, NY, United States},
author = {Han, Chang Min},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {1,1872-1915,alexander scriabin,historic background,nime07,sonification,synaesthesia,touch screen,was a russian composer,whose},
pages = {388--389},
title = {{Project Scriabin v.3}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}388.pdf},
year = {2007}
}
@inproceedings{Hochenbaum2010,
address = {Sydney, Australia},
author = {Hochenbaum, Jordan and Kapur, Ajay and Wright, Matthew},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {esitar,hci,hyperinstrument,learning,machine,multimodal,nime10,performer recognition},
number = {Nime},
pages = {233--237},
title = {{Multimodal Musician Recognition}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}233.pdf},
year = {2010}
}
@inproceedings{Rohs2007,
address = {New York City, NY, United States},
author = {Rohs, Michael and Essl, Georg},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {camera phone,collaboration,mobile,mobile phone,music performance,nime07,sensing-based interaction,sound generation},
pages = {160--163},
title = {{CaMus 2  Optical Flow and Collaboration in Camera Phone Music Performance}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}160.pdf},
year = {2007}
}
@inproceedings{Perez2007,
abstract = {In this paper, we describe the composition of a piece for choir and Integral Music Controller. We focus more on the aesthetic, conceptual, and practical aspects of the interface and less on the technological details. We especially stress the influence that the designed interface poses on the compositional process and how we approach the expressive organisation of musical materials during the composition of the piece, as well as the addition of nuances (personal real-time expression) by the musicians at performance time. },
address = {New York City, NY, United States},
author = {P{\'{e}}rez, Miguel A and Knapp, Benjamin and Alcorn, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Composition,Emotion measurement,Integral Music Controller,Physiological Measurement,Spatialisation.},
pages = {289--292},
title = {{D{\'{i}}amair : Composing for Choir and Integral Music Controller}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}289.pdf},
year = {2007}
}
@inproceedings{Hadjakos:2013,
abstract = {Music ensembles have to synchronize themselves with a very high precision inorder to achieve the desired musical results. For that purpose the musicians donot only rely on their auditory perception but also perceive and interpret themovements and gestures of their ensemble colleges. In this paper we present aKinect-based method to analyze ensemble play based on head tracking. We discussfirst experimental results with a violin duo performance.},
address = {Daejeon, Republic of Korea},
author = {Hadjakos, Aristotelis and Grosshauser, Tobias},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Cross-Correlogram,Ensemble,Functional Data Analysis,Kinect,Strings,Synchronization},
pages = {106--110},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Motion and Synchronization Analysis of Musical Ensembles with the Kinect}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Stahl2010,
abstract = {Maintaining a sense of personal connection between increasingly synthetic performers and increasingly diffuse audiences is vital to storytelling and entertainment. Sonic intimacy is important, because voice is one of the highestbandwidth channels for expressing our real and imagined selves.New tools for highly focused spatialization could help improve acoustical clarity, encourage audience engagement, reduce noise pollution and inspire creative expression. We have a particular interest in embodied, embedded systems for vocal performance enhancement and transformation. This short paper describes work in progress on a toolkit for high-quality wearable sound suits. Design goals include tailored directionality and resonance, full bandwidth, and sensible ergonomics. Engineering details to accompany a demonstration of recent prototypes are presented, highlighting a novel magnetostrictive flextensional transducer. Based on initial observations we suggest that vocal acoustic output from the torso, and spatial perception of situated low frequency sources, are two areas deserving greater attention and further study.},
address = {Sydney, Australia},
author = {Stahl, Alex and Clemens, Patricia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {magnetostrictive flextensional transducer,nime10,paralinguistics,sound reinforcement,spatialization,speech enhancement,transformation,voice,wearable systems},
pages = {427--430},
title = {{Auditory Masquing : Wearable Sound Systems for Diegetic Character Voices}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}427.pdf},
year = {2010}
}
@inproceedings{Hamilton2006,
address = {Paris, France},
author = {Hamilton, Robert},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Bioinformatics,composition,real-time score generation.},
pages = {338--341},
title = {{Bioinformatic Feedback: Performer Bio-data as a Driver for Real-time Composition}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}338.pdf},
year = {2006}
}
@inproceedings{Laurson2008,
address = {Genoa, Italy},
author = {Laurson, Mikael and Kuuskankare, Mika},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {expressive timing,nime08,playing styles,synthesis control},
pages = {8--11},
title = {{Towards Idiomatic and Flexible Score-based Gestural Control with a Scripting Language}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}034.pdf},
year = {2008}
}
@inproceedings{Steiner:2004,
abstract = {StickMusic is an instrument comprised of two haptic devices, a joystick and a mouse, which control a phase vocoder in real time. The purpose is to experiment with ideas of how to apply haptic feedback when controlling synthesis algorithms that have no direct analogy to methods of generating sound in the physical world. },
address = {Hamamatsu, Japan},
author = {Steiner, Hans-Christoph},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {gestural control,haptic feedback,joystick,mouse,performance},
pages = {203--204},
title = {{StickMusic: Using Haptic Feedback with a Phase Vocoder}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}203.pdf},
year = {2004}
}
@inproceedings{Tveit2014,
abstract = {Reunion2012 is a work for electronically modified chess- board, chess players and electronic instruments. The work is based on---but also departs from---John Cage's Reunion, which premiered at the Sightsoundsystems Festival, Toronto, 1968. In the original performance, Cage and Marcel Duchamp played chess on an electronic board constructed by Lowell Cross. The board `conducted' various electronic sound sources played by Cross, Gordon Mumma, David Tudor, and David Behrman, using photoresistors fitted under the squares [1]. Reunion2012, on the other hand, utilises magnet sensors via an Arduino. Like in Cage's Variations V, this resulted in a musical situation where the improvis- ing musicians had full control over their own sound, but no control regarding when their sound may be heard. In addition to a concert version, this paper also describes an interactive installation based on the same hardware.},
address = {London, United Kingdom},
author = {Tveit, Anders and Wilmers, Hans and Thelle, Notto and Bugge, Magnus and Johansen, Thom and S{\ae}ther, Eskil Muan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {561--564},
publisher = {Goldsmiths, University of London},
title = {{Reunion2012: A Novel Interface for Sound Producing Actions Through the Game of Chess}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}443.pdf},
year = {2014}
}
@inproceedings{Erkut2011,
address = {Oslo, Norway},
author = {Erkut, Cumhur and Jylh{\"{a}}, Antti and Discioglu, Reha},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {multimodal displays,rhythmic interaction,sonification,uml},
number = {June},
pages = {477--480},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{A Structured Design and Evaluation Model with Application to Rhythmic Interaction Displays}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}477.pdf},
year = {2011}
}
@inproceedings{Grierson:2013,
abstract = {NoiseBear is a wireless malleable controller designed for, and in participationwith, physically and cognitively disabled children. The aim of the project wasto produce a musical controller that was robust, and flexible enough to be usedin a wide range of interactive scenarios in participatory design workshops. NoiseBear demonstrates an open ended system for designing wireless malleablecontrollers in different shapes. It uses pressure sensitive material made fromconductive thread and polyester cushion stuffing, to give the feel of a softtoy. The sensor networks with other devices using the Bluetooth Low Energyprotocol, running on a BlueGiga BLE112 chip. This contains an embedded 8051processor which manages the sensor. NoiseBear has undergone an initialformative evaluation in a workshop session with four autistic children, andcontinues to evolve in series of participatory design workshops. The evaluationshowed that controller could be engaging for the children to use, andhighlighted some technical limitations of the design. Solutions to theselimitations are discussed, along with plans for future design iterations.},
address = {Daejeon, Republic of Korea},
author = {Grierson, Mick and Kiefer, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {assistive technology,malleable controllers,multiparametric mapping},
pages = {413--416},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{NoiseBear: A Malleable Wireless Controller Designed In Participation with Disabled Children}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Forsyth2011,
abstract = {Remixing audio samples is a common technique for the creation of electronic music, and there are a wide variety oftools available to edit, process, and recombine pre-recordedaudio into new compositions. However, all of these toolsconceive of the timeline of the pre-recorded audio and theplayback timeline as identical. In this paper, we introducea dual time axis representation in which these two timelines are described explicitly. We also discuss the randomaccess remix application for the iPad, an audio sample editor based on this representation. We describe an initialuser study with 15 high school students that indicates thatthe random access remix application has the potential todevelop into a useful and interesting tool for composers andperformers of electronic music.},
address = {Oslo, Norway},
author = {Forsyth, Jon and Glennon, Aron and Bello, Juan P},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {iPad,interactive systems,multi-touch,remix,sample editor},
pages = {487--490},
title = {{Random Access Remixing on the iPad}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}487.pdf},
year = {2011}
}
@inproceedings{Verplank2011,
address = {Oslo, Norway},
author = {Verplank, Bill and Georg, Francesco},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {haptics,microprocessors,music controllers,nime},
number = {June},
pages = {539--540},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Can Haptics Make New Music ? - Fader and Plank Demos}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}539.pdf},
year = {2011}
}
@inproceedings{nime2014:ogreen,
abstract = {To engage with questions of musicality is to invite into consideration a complex network of topics beyond the mechanics of soundful interaction with our interfaces. Drawing on the work of Born, I sketch an outline of the reach of these topics. I suggest that practice-led methods, by dint of focussing on the lived experience where many of these topics converge, may be able to serve as a useful methodological `glue' for NIME by helping stimulate useful agonistic discussion on our objects of study, and map the untidy contours of contemporary practices. I contextualise this discussion by presenting two recently developed improvisation systems and drawing from these some starting suggestions for how attention to the grain of lived practice could usefully contribute to considerations for designers in terms of the pursuit of musicality and the care required in considering performances in evaluation.},
address = {London, United Kingdom},
author = {Green, Owen},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {1--6},
publisher = {Goldsmiths, University of London},
title = {{NIME, Musicality and Practice-led Methods}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}434.pdf},
year = {2014}
}
@inproceedings{Baalman2009,
address = {Pittsburgh, PA, United States},
author = {Baalman, Marije A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {329},
title = {{Code LiveCode Live, or livecode Embodied}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}329.pdf},
year = {2009}
}
@inproceedings{Fraietta2005,
address = {Vancouver, BC, Canada},
author = {Fraietta, Angelo},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {260--261},
title = {{Smart Controller / Bell Garden Demo}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}260.pdf},
year = {2005}
}
@inproceedings{nime2014:avantklooster,
abstract = {Emotion is a complex topic much studied in music and arguably equally central to the visual arts where this is usually referred to with the overarching label of aesthetics. This paper explores how music and the arts have incorporated the study of emotion. We then introduce the development of a live audio visual interface entitled In A State that detects emotion from live audio (in this case a piano performance) and generates visuals and electro acoustic music in response.},
address = {London, United Kingdom},
author = {{van 't Klooster}, Adinda and Collins, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {545--548},
publisher = {Goldsmiths, University of London},
title = {{In A State: Live Emotion Detection and Visualisation for Music Performance}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}469.pdf},
year = {2014}
}
@inproceedings{DeJong2006,
address = {Paris, France},
author = {de Jong, Staas},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
pages = {79--80},
title = {{A Tactile Closed-Loop Device for Musical Interaction}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}079.pdf},
year = {2006}
}
@inproceedings{Pelletier2008,
address = {Genoa, Italy},
author = {Pelletier, Jean-Marc},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {computer vision,control field,image analysis,imaging,mapping,microsound,motion flow,nime08,sonification,synthesis},
pages = {158--163},
title = {{Sonified Motion Flow Fields as a Means of Musical Expression}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}158.pdf},
year = {2008}
}
@inproceedings{Scavone2005,
address = {Vancouver, BC, Canada},
author = {Scavone, Gary and Silva, Andrey R},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Scavone, Silva - 2005 - Frequency Content of Breath Pressure and Implications for Use in Control.pdf:pdf},
pages = {93--96},
title = {{Frequency Content of Breath Pressure and Implications for Use in Control}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}093.pdf},
year = {2005}
}
@inproceedings{Magnusson2010,
address = {Sydney, Australia},
author = {Magnusson, Thor},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {analysis,dimension space,epistemic tools,music theory,nime10},
number = {Nime},
pages = {43--46},
title = {{An Epistemic Dimension Space for Musical Devices}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}043.pdf},
year = {2010}
}
@inproceedings{Schacher2010,
address = {Sydney, Australia},
author = {Schacher, Jan C},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {artistic strategies,computer vision,delaunay tessellation,mapping,mapping tools,motion sensing,nime10,splines,wearable sensors},
number = {Nime},
pages = {250--254},
title = {{Motion To Gesture To Sound : Mapping For Interactive Dance}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}250.pdf},
year = {2010}
}
@inproceedings{Shatin2004,
address = {Hamamatsu, Japan},
author = {Shatin, Judith and Topper, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {composition,gui controllers,interactive systems,new interfaces,open source,real time audio,video tracking},
pages = {51--54},
title = {{Tree Music: Composing with GAIA}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}051.pdf},
year = {2004}
}
@inproceedings{Chordia2008,
abstract = {We describe a system that can listen to a performance of Indian music and recognize the raag, the fundamental melodicframework that Indian classical musicians improvise within.In addition to determining the most likely raag being performed, the system displays the estimated the likelihoodof each of the other possible raags, visualizing the changesover time. The system computes the pitch-class distributionand uses a Bayesian decision rule to classify the resultingtwelve dimensional feature vector, where each feature represents the relative use of each pitch class. We show that thesystem achieves high performance on a variety of sources,making it a viable tool for interactive performance.},
address = {Genoa, Italy},
author = {Chordia, Parag and Rae, Alex},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {automatic recognition,indian music,nime08,raag,raga},
pages = {331--334},
title = {{Real-Time Raag Recognition for Interactive Music}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}331.pdf},
year = {2008}
}
@inproceedings{Singer2003a,
address = {Montreal, QC, Canada},
author = {Singer, Eric and Larke, Kevin and Bianciardi, David},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {188--191},
title = {{LEMUR GuitarBot: MIDI Robotic String Instrument}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}188.pdf},
year = {2003}
}
@inproceedings{Lo:2013,
abstract = {Mobile DJ is a music-listening system that allows multiple users to interactand collaboratively contribute to a single song over a social network. Activelistening through a tangible interface facilitates users to manipulate musicaleffects, such as incorporating chords or ``scratching'' the record. Acommunication and interaction server further enables multiple users to connectover the Internet and collaborate and interact through their music. User testsindicate that the device is successful at facilitating user immersion into theactive listening experience, and that users enjoy the added sensory input aswell as the novel way of interacting with the music and each other.},
address = {Daejeon, Republic of Korea},
author = {Lo, Kenneth W K and Lau, Chi Kin and Huang, Michael Xuelin and Tang, Wai Wa and Ngai, Grace and Chan, Stephen C F},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Mobile,interaction design,music,tangible user interface},
pages = {217--222},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Mobile DJ: a Tangible, Mobile Platform for Active and Collaborative Music Listening}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Dezfouli:2013,
abstract = {Notesaaz is both a new physical interface meant for musical performance and aproposal for a three-stage process where the controller is used to navigatewithin a graphical score that on its turn controls the sound generation. It canbe seen as a dynamic and understandable way of using dynamic mapping betweenthe sensor input and the sound generation. Furthermore by presenting thegraphical score to both the performer and the audience a new engagement of theaudience can be established.},
address = {Daejeon, Republic of Korea},
author = {Dezfouli, Erfan Abdi and van der Heide, Edwin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {custom controller,dynamic score,gestural input,musical instrument},
pages = {115--117},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Notesaaz: a new controller and performance idiom}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Park:2013c,
abstract = {In this paper we present a highly expressive, robust, and easy-to-build systemthat provides force-feedback interaction for mobile computing devices (MCD).Our system, which we call Fortissimo (ff), utilizes standard built-inaccelerometer measurements in conjunction with generic foam padding that can beeasily placed under a device to render an expressive force-feedback performancesetup. Fortissimo allows for musically expressive user-interaction with addedforce-feedback which is integral for any musical controller --a feature that isabsent for touchscreen-centric MCDs. This paper details ff core concepts,hardware and software designs, and expressivity of musical features.},
address = {Daejeon, Republic of Korea},
author = {Park, Tae Hong and Nieto, Oriol},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {expression,force-feedback,mobile computing devices,mobile music},
pages = {291--294},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Fortissimo: Force-Feedback for Mobile Devices}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Schacher2011,
abstract = {In this paper the relationship between body, motion and sound is addressed. The comparison with traditional instruments and dance is shown with regards to basic types of motion. The difference between gesture and movement is outlined and some of the models used in dance for structuring motion sequences are described. In order to identify expressive aspects of motion sequences a test scenario is devised. After the description of the methods and tools used in a series of measurements, two types of data-display are shown and the applied in the interpretation. One salient feature is recognized and put into perspective with regards to movement and gestalt perception. Finally the merits of the technical means that were applied are compared and a model-based approach to motion-sound mapping is proposed. },
address = {Oslo, Norway},
author = {Schacher, Jan C and Stoecklin, Angela},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Interactive Dance,Mapping,Motion Perception,Motion and Gesture,Sonification},
pages = {292--295},
title = {{Traces -- Body, Motion and Sound}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}292.pdf},
year = {2011}
}
@inproceedings{Caramiaux2011,
abstract = {This paper presents a prototypical tool for sound selection driven by users' gestures. Sound selection by gesturesis a particular case of "query by content" in multimediadatabases. Gesture-to-Sound matching is based on computing the similarity between both gesture and sound parameters' temporal evolution. The tool presents three algorithms for matching gesture query to sound target. Thesystem leads to several applications in sound design, virtualinstrument design and interactive installation.},
address = {Oslo, Norway},
author = {Caramiaux, Baptiste and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Schnell, Norbert},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Query by Gesture,Sonic Interaction,Time Series Analysis},
pages = {329--330},
title = {{Sound Selection by Gestures}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}329.pdf},
year = {2011}
}
@inproceedings{Johnston:2013,
abstract = {This paper describes an audio-visual performance system based on real-timefluid simulation. The aim is to provide a rich environment for works whichblur the boundaries between dance and instrumental performance -- and sound andvisuals -- while maintaining transparency for audiences and new performers. The system uses infra-red motion tracking to allow performers to manipulate areal-time fluid simulation, which in turn provides control data forcomputer-generated audio and visuals. It also provides a control andconfiguration system which allows the behaviour of the interactive system to bechanged over time, enabling the structure within which interactions take placeto be `composed'.},
address = {Daejeon, Republic of Korea},
author = {Johnston, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {composition,dance,fluid simulation,performance},
pages = {132--135},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Fluid Simulation as Full Body Audio-Visual Instrument}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Pardue:2013a,
abstract = {This paper explores the potential of near-field optical reflective sensing formusical instrument gesture capture. Near-field optical sensors are inexpensive,portable and non-intrusive, and their high spatial and temporal resolutionmakes them ideal for tracking the finer motions of instrumental performance.The paper discusses general optical sensor performance with detailedinvestigations of three sensor models. An application is presented to violinbow position tracking using reflective sensors mounted on the stick. Bowtracking remains a difficult task, and many existing solutions are expensive,bulky, or offer limited temporal resolution. Initial results indicate that bowposition and pressure can be derived from optical measurements of thehair-string distance, and that similar techniques may be used to measure bowtilt.},
address = {Daejeon, Republic of Korea},
author = {Pardue, Laurel and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {LED,bow tracking,gesture,near-field sensing,optical sensor,photodiode,phototransistor,reflectance,violin},
pages = {363--368},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Near-Field Optical Reflective Sensing for Bow Tracking}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Knorig2007,
address = {New York City, NY, United States},
author = {Kn{\"{o}}rig, Andr{\'{e}} and M{\"{u}}ller, Boris and Wettach, Reto},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {control,education,expressive gesture,musical,musical expression,musical interface,natural interface,nime07,painterly interfaces for media},
pages = {384--385},
title = {{Articulated Paint : Musical Expression for Non-Musicians}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}384.pdf},
volume = {4},
year = {2007}
}
@inproceedings{Muth2003,
abstract = {Sodaconductor is a musical interface for generating OSCcontrol data based on the dynamic physical simulation toolSodaconstructor as it can be seen and heard onhttp://www.sodaplay.com.},
address = {Montreal},
author = {Muth, David and Burton, Ed},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
pages = {222--224},
title = {{Sodaconductor}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}222.pdf},
year = {2003}
}
@inproceedings{nime2014:doverholt,
abstract = {We introduce and describe the initial evaluation of a new low-cost augmented violin prototype, with research focused on the user experience when playing such hybrid physical-digital instruments, and the exploration of novel interactive performance techniques. Another goal of this work is wider platform accessibility for players, via a simple `do-it-yourself' approach described by the design herein. While the hardware and software elements are open source, the build process can nonetheless require non-insignificant investments of time and money, as well as basic electronics construction skills. These have been kept to a minimum wherever possible. Our initial prototype is based upon an inexpensive electric violin that is widely available online for approximately 200 USD. This serves as the starting point for construction, to which the design adds local Digital Signal Processing (DSP), gestural sensing, and sound output. Real-time DSP algorithms are running on a mobile device, which also incorporates orientation/gesture sensors for parameter mapping, with the resulting sound amplified and rendered via small loudspeakers mounted on the instrument. The platform combines all necessary elements for digitally-mediated interactive performance; the need for a traditional computer only arises when developing new DSP algorithms for the platform. An initial exploratory evaluation with users is presented, in which performers explore different possibilities with the proposed platform (various DSP implementations, mapping schemes, physical setups, etc.) in order to better establish the needs of the performing artist. Based on these results, future work is outlined leading towards the development of a complete quartet of instruments.},
address = {London, United Kingdom},
author = {Overholt, Dan and Gelineck, Steven},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {122--125},
publisher = {Goldsmiths, University of London},
title = {{Design {\&} Evaluation of an Accessible Hybrid Violin Platform}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}470.pdf},
year = {2014}
}
@inproceedings{Magnusson2007,
address = {New York City, NY, United States},
author = {Magnusson, Thor and Mendieta, Enrike H},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {affordances,audio,constraints,embodiment,ergonomics,instrumental entropy,mapping,musical instruments,nime07,survey,usability},
pages = {94--99},
title = {{The Acoustic, the Digital and the Body : A Survey on Musical Instruments}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}094.pdf},
year = {2007}
}
@inproceedings{Harriman2011,
address = {Oslo, Norway},
author = {Harriman, Jiffer and Casey, Locky and Melvin, Linden},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {continuous,demonstration,electronic,instrument,lead,membrane,nime,pedal-steel,polyphony,ribbon,slide},
number = {June},
pages = {529--530},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Quadrofeelia  A New Instrument for Sliding into Notes}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}529.pdf},
year = {2011}
}
@inproceedings{Wang2009,
address = {Pittsburgh, PA, United States},
author = {Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {chuck,design,in,in real-time,interface,iphone,mobile music,multitouch,nime09,ocarina,pulsing waves,social,sonically and onscreen and,sound synthesis takes place,the breath is visualized},
pages = {303--307},
title = {{Designing Smule's Ocarina : The iPhone's Magic Flute}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}303.pdf},
year = {2009}
}
@inproceedings{Torresen2013,
abstract = {A custom designed WLAN (Wireless Local Area Network) based sensor interface ispresented in this paper. It is aimed at wirelessly interfacing a large varietyof sensors to supplement built-in sensors in smart phones and media players.The target application area is collection of human related motions andcondition to be applied in musical applications. The interface is based oncommercially available units and allows for up to nine sensors. The benefit ofusing WLAN based communication is high data rate with low latency. Ourexperiments show that the average transmission time is less than 2ms for asingle sensor. Further, it is operational for a whole day without batteryrecharging.},
address = {Daejeon, Republic of Korea},
author = {Torresen, Jim and Hafting, Yngve and Nymoen, Kristian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Torresen, Hafting, Nymoen - 2013 - A New Wi-Fi based Platform for Wireless Sensor Data Collection.pdf:pdf},
keywords = {Arduino,WLAN,sensor data collection,wireless communication},
pages = {337--340},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A New Wi-Fi based Platform for Wireless Sensor Data Collection}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{McPherson:2012a,
abstract = {There is growing interest in the field of augmented musical instruments, which extend traditional acoustic instruments using new sensors and actuators. Several designs use electromagnetic actuation to induce vibrations in the acoustic mechanism, manipulating the traditional sound of the in- strument without external speakers. This paper presents techniques and guidelines for the use of electromagnetic actuation in augmented instruments, including actuator design and selection, interfacing with the instrument, and cir- cuits for driving the actuators. The material in this pa- per forms the basis of the magnetic resonator piano, an electromagnetically-augmented acoustic grand piano now in its second design iteration. In addition to discussing applications to the piano, this paper aims to provide a toolbox to accelerate the design of new hybrid acoustic-electronic instruments.},
address = {Ann Arbor, Michigan},
author = {McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {augmented instruments,circuit design,electromagnetic actuation,hardware},
publisher = {University of Michigan},
title = {{Techniques and Circuits for Electromagnetic Instrument Actuation}},
year = {2012}
}
@inproceedings{MillsIII2010,
author = {{Mills III}, John Anderson and {Di Fede}, Damien and Brix, Nicolas},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Dutoit, Thierry and Macq, Beno$\backslash${\^{}}it},
file = {::},
keywords = {audio library,minim,mu-,music programming,processing},
number = {4},
organization = {numediart Research Program on Digital Art Technologies},
pages = {105--111},
title = {{Music Programming in Minim}},
url = {http://www.numediart.org/docs/numediart{\_}2009{\_}s08{\_}p1{\_}report.pdf},
volume = {2},
year = {2010}
}
@inproceedings{nime2014:ailsar,
abstract = {This paper discusses one particular mapping for a new gestural instrument called the AirSticks. This mapping was designed to be used for improvised or rehearsed duos and restricts the performer to only utilising the sound source of one other musician playing an acoustic instrument. Several pieces with different musicians were performed and documented, musicians were observed and interviews with these musicians were transcribed. In this paper we will examine the thoughts of these musicians to gather a better understanding of how to design effective ensemble instruments of this type.},
address = {London, United Kingdom},
author = {Ilsar, Alon and Havryliv, Mark and Johnston, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
keywords = {evaluation},
mendeley-tags = {evaluation},
pages = {339--342},
publisher = {Goldsmiths, University of London},
title = {{Evaluating the Performance of a New Gestural Instrument Within an Ensemble}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}363.pdf},
year = {2014}
}
@inproceedings{Hindman2006,
address = {Paris, France},
author = {Hindman, David},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
pages = {296--299},
title = {{Modal Kombat: Competition and Choreography in Synesthetic Musical Performance}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}296.pdf},
year = {2006}
}
@inproceedings{Marchini2011,
address = {Oslo, Norway},
author = {Marchini, Marco and Papiotis, Panos and P{\'{e}}rez, Alfonso and Maestre, Esteban},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {6dof,bow force,bow pressing force,bow simplified physical model,force,hair ribbon,playing,pressing force,violin},
number = {June},
pages = {481--486},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{A Hair Ribbon Deflection Model for Low-intrusiveness Measurement of Bow Force in Violin Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}481.pdf},
year = {2011}
}
@inproceedings{Schedel2011,
abstract = {In this paper we discuss how the band 000000Swan uses machine learning to parse complex sensor data and create intricate artistic systems for live performance. Using the Wekinator software for interactive machine learning, we have created discrete and continuous models for controlling audio and visual environments using human gestures sensed by a commercially-available sensor bow and the Microsoft Kinect. In particular, we have employed machine learning to quickly and easily prototype complex relationships between performer gesture and performative outcome. },
address = {Oslo, Norway},
author = {Schedel, Margaret and Perry, Phoenix and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Animation,Bow Articulation,Interactive,K-Bow,Kinect,Machine Learning,Motion-Tracking,Multimedia,Wekinator},
pages = {453--456},
title = {{Wekinating 000000{\{}S{\}}wan : Using Machine Learning to Create and Control Complex Artistic Systems}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}453.pdf},
year = {2011}
}
@inproceedings{Pelletier2005,
address = {Vancouver, BC, Canada},
author = {Pelletier, Jean-Marc},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {89--92},
title = {{A Graphical Interface for Real-Time Signal Routing}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}089.pdf},
year = {2005}
}
@inproceedings{Freed2010,
abstract = {We present two complementary approaches for the visualization and interaction of dimensionally reduced data setsusing hybridization interfaces. Our implementations privilege syncretic systems allowing one to explore combinations(hybrids) of disparate elements of a data set through theirplacement in a 2-D space. The first approach allows for theplacement of data points anywhere on the plane accordingto an anticipated performance strategy. The contribution(weight) of each data point varies according to a power function of the distance from the control cursor. The secondapproach uses constrained vertex colored triangulations ofmanifolds with labels placed at the vertices of triangulartiles. Weights are computed by barycentric projection ofthe control cursor position.},
address = {Sydney, Australia},
author = {Freed, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Interpolation,dimension reduction,radial basis functions,triangular mesh},
pages = {343--347},
title = {{Visualizations and Interaction Strategies for Hybridization Interfaces}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}343.pdf},
year = {2010}
}
@inproceedings{Corness2007,
address = {New York City, NY, United States},
author = {Seo, Jinsil and Corness, Greg},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {2007},
title = {{nite{\_}aura : An Audio-Visual Interactive Immersive Installation}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}431.pdf},
year = {2007}
}
@inproceedings{Coghlan2008,
address = {Genoa, Italy},
author = {Coghlan, Niall and Knapp, Benjamin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Ubiquitous computing,affective computing,biosignals.,chairs,context -awareness,digital artefacts,embedded systems,emotional state sensing,networking},
pages = {233--236},
title = {{Sensory Chairs : A System for Biosignal Research and Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}233.pdf},
year = {2008}
}
@inproceedings{Place2008,
address = {Genoa, Italy},
author = {Place, Timothy and Lossius, Trond and Jensenius, Alexander R and Peters, Nils},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {jamoma,namespace,nime08,osc,standardization},
pages = {181--184},
title = {{Addressing Classes by Differentiating Values and Properties in OSC}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}181.pdf},
year = {2008}
}
@inproceedings{Michon:2013,
abstract = {Black Box is a site based installation that allows users to create uniquesounds through physical interaction. The installation consists of a geodesicdome, surround sound speakers, and a custom instrument suspended from the apexof thedome. Audience members entering the space are able to create sound by strikingor rubbing the cube, and are able to control a delay system by moving the cubewithin the space.},
address = {Daejeon, Republic of Korea},
author = {Michon, Romain and Borins, Myles and Meisenholder, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Beagleboard,Embedded-Linux,Faust,Open Sound Control,PureData,Satellite CCRMA},
month = {may},
pages = {464--465},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{The Black Box}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{ldahl:2014,
abstract = {Motion sensing technologies enable musical interfaces where a performer moves their body "in the air" without manipulating or contacting a physical object. These interfaces work well when the movement and sound are smooth and continuous, but it has proven difficult to design a system which triggers discrete sounds with precision that allows for complex rhythmic performance. We conducted a study where participants perform ``air-drumming" gestures in time to rhythmic sounds. These movements are recorded, and the timing of various movement features with respect to the onset of audio events is analyzed. A novel algorithm for detecting sudden changes in direction is used to find the end of the strike gesture. We find that these occur on average after the audio onset and that this timing varies with the tempo of the movement. Sharp peaks in magnitude acceleration occur before the audio onset and do not vary with tempo. These results suggest that detecting peaks in acceleration will lead to more naturally responsive air gesture instruments.},
address = {London, United Kingdom},
author = {Dahl, Luke},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {201--206},
publisher = {Goldsmiths, University of London},
title = {{Triggering Sounds from Discrete Air Gestures: What Movement Feature Has the Best Timing?}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}514.pdf},
year = {2014}
}
@inproceedings{Andersson:2013,
abstract = {Our voice and body are important parts of our self-experience, andcommunication and relational possibilities. They gradually become moreimportant for Interaction Design, due to increased development of tangibleinteraction and mobile communication. In this paper we present and discuss ourwork with voice and tangible interaction in our ongoing research project XXXXX.The goal is to improve health for families, adults and children withdisabilities through use of collaborative, musical, tangible media. We build onuse of voice in Music Therapy and on a humanistic health approach. Ourchallenge is to design vocal and tangible interactive media that through usereduce isolation and passivity and increase empowerment for the users. We usesound recognition, generative sound synthesis, vibrations and cross-mediatechniques, to create rhythms, melodies and harmonic chords to stimulatebody-voice connections, positive emotions and structures for actions.},
address = {Daejeon, Republic of Korea},
author = {Andersson, Anders-Petter and Cappelen, Birgitta},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Empowerment,Music {\&} Health,Music Therapy,Resource-Oriented,Tangible Interaction,Vocal Interaction,Voice},
pages = {406--412},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Designing Empowering Vocal and Tangible Interaction}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Milne2011,
address = {Oslo, Norway},
author = {Milne, Andrew J and Xamb{\'{o}}, Anna and Laney, Robin and Sharp, David B and Prechtl, Anthony and Holland, Simon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {generalized keyboard,iPad,isomorphic layout,microtonality,multi-touch surface,musical interface design,tablet},
pages = {244--247},
title = {{Hex Player --- A Virtual Musical Controller}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}244.pdf},
year = {2011}
}
@inproceedings{Lee2005,
address = {Vancouver, BC, Canada},
author = {Lee, Eric and Borchers, Jan},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {204--207},
title = {{The Role of Time in Engineering Computer Music Systems}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}204.pdf},
year = {2005}
}
@inproceedings{Lu:2012,
abstract = {WIS platform is a wireless interactive sensor platform de- signed to support dynamic and interactive applications. The platform consists of a capture system which includes multi- ple on-body Zigbee compatible motion sensors, a processing unit and an audio-visual display control unit. It has a com- plete open architecture and provides interfaces to interact with other user-designed applications. Therefore, WIS plat- form is highly extensible. Through gesture recognitions by on-body sensor nodes and data processing, WIS platform can offer real-time audio and visual experiences to the users. Based on this platform, we set up a multimedia installation that presents a new interaction model between the partic- ipants and the audio-visual environment. Furthermore, we are also trying to apply WIS platform to other installations and performances.},
address = {Ann Arbor, Michigan},
author = {Lu, Jia-Liang and Fang, Da-Lei and Qin, Yi and Tang, Jiu-Qiang},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Audio-visual experience,Interactive},
publisher = {University of Michigan},
title = {{Wireless Interactive Sensor Platform for Real-Time Audio-Visual Experience}},
year = {2012}
}
@inproceedings{Essl2010,
address = {Sydney, Australia},
author = {Essl, Georg and M{\"{u}}ller, Alexander},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {design,mapping,meta-environment,mobile music making,nime10},
number = {Nime},
pages = {76--81},
title = {{Designing Mobile Musical Instruments and Environments with urMus}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}076.pdf},
year = {2010}
}
@inproceedings{nime2014:dschlienger,
abstract = {This paper provides a rationale for choosing acoustic localisation techniques as an alternative to other principles to provide spatial positions in interactive locative audio applications (ILAA). By comparing positioning technology in existing ILAAs to the expected performance of acoustic positioning systems (APS), we can evaluate if APS would perform equivalently in a particular application. In this paper, the titles of NIME conference proceedings from 2001 to 2013 were searched for presentations on ILAA using positioning technology. Over 80 relevant articles were found. For each of the systems we evaluated if and why APS would be a contender or not. The results showed that for over 73 percent of the reviewed applications, APS could possibly provide competitive alternatives and at very low cost.},
address = {London, United Kingdom},
author = {Schlienger, Dominik and Tervo, Sakari},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {439--442},
publisher = {Goldsmiths, University of London},
title = {{Acoustic Localisation as an Alternative to Positioning Principles in Applications presented at NIME 2001-2013}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}501.pdf},
year = {2014}
}
@inproceedings{Zaborowski2004,
address = {Hamamatsu, Japan},
author = {Zaborowski, Philippe S},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {computing,handheld devices,hci,isometric joystick,mobile,musical instrument,one-thumb input device},
pages = {112--115},
title = {{ThumbTec: A New Handheld Input Device}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}112.pdf},
year = {2004}
}
@inproceedings{Grosshauser:2012,
abstract = {From a technical point of view, instrumental music mak-ing involves audible, visible and hidden playing parameters. Hidden parameters like force, pressure and fast movements, happening within milliseconds are particularly difficult to capture. Here, we present data focusing on movement coordination parameters of the left hand fingers with the bow hand in violinists and between two violinists in group playing. Data was recorded with different position sensors, a micro camcorder fixed on a violin and an acceleration sensor placed on the bow. Sensor measurements were obtained at a high sampling rate, gathering the data with a small mi-crocontroller unit, connected with a laptop computer. To capture bow's position, rotation and angle directly on the bow to string contact point, the micro camcorder was fixed near the bridge. Main focuses of interest were the changes of the left hand finger, the temporal synchronization between left hand fingers with the right hand, the close up view to the bow to string contact point and the contact of the left hand finger and/or string to the fingerboard. Seven violinists, from beginners to master class students played scales in different rhythms, speeds and bowings and music excerpts of free choice while being recorded. One measure-ment with 2 violinists was made to see the time differences between two musicians while playing together. For simple integration of a conventional violin into electronic music environments, left hand sensor data were exemplary converted to MIDI and OSC.},
address = {Ann Arbor, Michigan},
author = {Grosshauser, Tobias and Candia, Victor and Hildebrand, Horst and Tr{\"{o}}ster, Gerhard},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Strings,coordination,finger,hand,left,right,violin},
month = {may},
publisher = {University of Michigan},
title = {{Sensor Based Measurements of Musicians' Synchronization Issues}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}256.pdf},
year = {2012}
}
@inproceedings{Hadjakos2009,
address = {Pittsburgh, PA, United States},
author = {Hadjakos, Aristotelis and Aitenbichler, Erwin and M{\"{u}}hlh{\"{a}}user, Max},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {arm movement,augmented instrument,classification,gesture,inertial sensing,nime09,piano},
number = {section 9},
pages = {7--12},
title = {{Probabilistic Model of Pianists' Arm Touch Movements}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}007.pdf},
year = {2009}
}
@inproceedings{Buxton2005,
address = {Vancouver, BC, Canada},
author = {Buxton, Bill},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {4},
title = {{Causality and Striking the Right Note}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}004.pdf},
year = {2005}
}
@inproceedings{Schoonderwaldt2006,
abstract = {A cost-effective method was developed for the estimation of the bow velocity in violin playing, using an accelerometer on the bow in combination with point tracking using a standard video camera. The video data are used to detect the moments of bow direction changes. This information is used for piece-wise integration of the accelerometer signal, resulting in a drift-free reconstructed velocity signal with a high temporal resolution. The method was evaluated using a 3D motion capturing system, providing a reliable reference of the actual bow velocity. The method showed good results when the accelerometer and video stream are synchronized. Additional latency and jitter of the camera stream can importantly decrease the performance of the method, depending on the bow stroke type. },
address = {Paris, France},
author = {Schoonderwaldt, Erwin and Rasamimanana, Nicolas and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Bowing gestures,accelerometer,bow velocity,bowed string,video tracking.,violin},
pages = {200--203},
title = {{Combining Accelerometer and Video Camera: Reconstruction of Bow Velocity Profiles}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}200.pdf},
year = {2006}
}
@inproceedings{Collins2010a,
address = {Sydney, Australia},
author = {Collins, Nick and Kiefer, Chris and Patoli, Zeeshan and White, Martin},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {agile de-,mapping,motion capture,musical controller,nime10},
number = {June},
pages = {15--18},
title = {{Musical Exoskeletons : Experiments with a Motion Capture Suit}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}455.pdf},
year = {2010}
}
@inproceedings{Bouillot2008,
address = {Genoa, Italy},
author = {Bouillot, Nicolas and Wozniewski, Mike and Settel, Zack and Cooperstock, Jeremy R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {nime08},
pages = {189--192},
title = {{A Mobile Wireless Augmented Guitar}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}189.pdf},
year = {2008}
}
@inproceedings{Zbyszynski2008,
address = {Genoa, Italy},
author = {Zbyszynski, Michael},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {digitizing tablet,mapping,nime08,pedagogy,practice,wacom tablet},
pages = {245--248},
title = {{An Elementary Method for Tablet}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}245.pdf},
year = {2008}
}
@inproceedings{Tanaka2002,
address = {Dublin, Ireland},
author = {Tanaka, Atau and Knapp, Benjamin},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::;::},
keywords = {electromyogram,human computer interaction,musical controllers,position sensing,sensor instruments},
pages = {171--176},
title = {{Multimodal Interaction in Music Using the Electromyogram and Relative Position Sensing}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}171.pdf},
year = {2002}
}
@inproceedings{Hiraga2004,
address = {Hamamatsu, Japan},
author = {Hiraga, Rumi and Bresin, Roberto and Hirata, Keiji and Katayose, Haruhiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {musical expression,performance ren-,rencon,turing test},
pages = {120--123},
title = {{Rencon 2004: Turing Test for Musical Expression}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}120.pdf},
year = {2004}
}
@inproceedings{Umetani2010,
address = {Sydney, Australia},
author = {Umetani, Nobuyuki and Mitani, Jun and Igarashi, Takeo},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {cation,edu-,eling,methods and applications -,modeling - cad,modeling - geometric mod-,modeling - modeling interfaces,nime10,real-time fem},
number = {Nime},
pages = {26--30},
title = {{Designing Custom-made Metallophone with Concurrent Eigenanalysis}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}026.pdf},
year = {2010}
}
@inproceedings{Rebelo2006,
abstract = {The culture of laptop improvisation has grown tremendously in recent years. The development of personalized software instruments presents interesting issues in the context of improvised group performances. This paper examines an approach that is aimed at increasing the modes of interactivity between laptop performers and at the same time suggests ways in which audiences can better discern and identify the sonic characteristics of each laptop performer. We refer to software implementation that was developed for the BLISS networked laptop ensemble with view to designing a shared format for the exchange of messages within local and internet based networks. },
address = {Paris, France},
author = {Rebelo, Pedro and Renaud, Alain B},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Networked audio technologies,centralized audio server,improvisation,laptop ensemble},
pages = {53--56},
title = {{The Frequencyliator -- Distributing Structures for Networked Laptop Improvisation}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}053.pdf},
year = {2006}
}
@inproceedings{Nugroho2010,
address = {Sydney, Australia},
author = {Nugroho, Jeremiah and Beilharz, Kirsty},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {body,nime10,user-centered design,wearable expressions},
number = {Nime},
pages = {327--330},
title = {{Understanding and Evaluating User Centred Design in Wearable Expressions}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}327.pdf},
year = {2010}
}
@inproceedings{Serafin2006,
abstract = {In this paper we introduce the Croaker, a novel input deviceinspired by Russolo's Intonarumori. We describe the components of the controller and the sound synthesis engine whichallows to reproduce several everyday sounds.},
address = {Paris, France},
author = {Serafin, Stefania and de G{\"{o}}tzen, Amalia and B{\"{o}}ttcher, Niels and Gelineck, Steven},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Noise machines,everyday sounds,physical models.},
pages = {240--245},
title = {{Synthesis and Control of Everyday Sounds Reconstructing Russolo{\{}$\backslash$textquoteright{\}}s Intonarumori}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}240.pdf},
year = {2006}
}
@inproceedings{Berdahl2008a,
address = {Genoa, Italy},
author = {Berdahl, Edgar and Steiner, Hans-Christoph and Oldham, Collin},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {actuator,cellomobo,embedded,haptic,haptic drum,immersion,nime08,practical,rate,sampling,woofer},
pages = {61--66},
title = {{Practical Hardware and Algorithms for Creating Haptic Musical Instruments}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}061.pdf},
year = {2008}
}
@inproceedings{Ulyate2001,
address = {Seattle, WA, United States},
author = {Ulyate, Ryan and Bianciardi, David},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
file = {::},
pages = {54--56},
title = {{The Interactive Dance Club : Avoiding Chaos In A Multi Participant Environment}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}054.pdf},
year = {2001}
}
@inproceedings{Montag2011,
address = {Oslo, Norway},
author = {Montag, Matthew and Sullivan, Stefan and Dickey, Scott and Leider, Colby},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {DIY,frustrated total internal reflection,haptics,latency,multi-touch,music composition,music performance},
pages = {8--13},
title = {{A Low-Cost, Low-Latency Multi-Touch Table with Haptic Feedback for Musical Applications}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}008.pdf},
year = {2011}
}
@inproceedings{Bull2007,
address = {New York City, NY, United States},
author = {Bull, Steve and Gresham-Lancaster, Scot and Mintchev, Kalin and Svoboda, Terese},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {420},
title = {{Cellphonia : WET}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}420.pdf},
year = {2007}
}
@inproceedings{Luhtala:2012,
abstract = {This paper introduces `The Aesthetic Experience Prism', a framework for studying how components of aesthetic experience materialize in the model's of interaction of novel musical interfaces as well as how the role of aesthetics could be made more explicit in the processes of designing interaction for musical technologies. The Aesthetic Experience Prism makes use of Arthur Danto's framework of aesthetic experience that consists of three conceptual entities: (1) metaphor; (2) expression; and (3) style. In this paper we present key questions driving the research, theoretical background, artistic research approach and user research activities.
In the DIYSE project a proof-of-concept music creation system prototype was developed in a collaborative design setting. The prototype provides means to the performer to create music with minimum effort while allowing for versatile interaction. We argue that by using an artistic research approach specifically targeting designing for aesthetic experience we were able to transform the knowledge from early design ideas to resulting technology products in which model's of interaction metaphors, expression and style are in an apparent role.},
address = {Ann Arbor, Michigan},
author = {Luhtala, Matti and Niemel{\"{a}}inen, Ilkka and Plomp, Johan and Turunen, Markku and Tuomisto, Julius},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Aesthetics,Artistic Research,Exploration,Interaction Design},
month = {may},
publisher = {University of Michigan},
title = {{Studying Aesthetics in a Musical Interface Design Process Through `Aesthetic Experience Prism'}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}226.pdf},
year = {2012}
}
@inproceedings{Cannon2010,
address = {Sydney, Australia},
author = {Cannon, Joanne and Favilla, Stuart},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {augmented instruments,expressive spatial,nime10,playable instruments},
pages = {120--124},
title = {{Expression and Spatial Motion : Playable Ambisonics}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}120.pdf},
year = {2010}
}
@inproceedings{Solis2010,
abstract = {Since 2007, our research is related to the development of an anthropomorphic saxophonist robot, which it has been designed to imitate the saxophonist playing by mechanically reproducing the organs involved for playing a saxophone. Our research aims in understanding the motor control from an engineering point of view and enabling the communication. In this paper, the Waseda Saxophone Robot No. 2 (WAS-2) which is composed by 22-DOFs is detailed. The lip mechanism of WAS-2 has been designed with 3-DOFs to control the motion of the lower, upper and sideway lips. In addition, a human-like hand (16 DOF-s) has been designed to enable to play all the keys of the instrument. Regarding the improvement of the control system, a feed-forward control system with dead-time compensation has been implemented to assure the accurate control of the air pressure. In addition, the implementation of an auditory feedback control system has been proposed and implemented in order to adjust the positioning of the physical parameters of the components of the robot by providing a pitch feedback and defining a recovery position (off-line). A set of experiments were carried out to verify the mechanical design improvements and the dynamic response of the air pressure. As a result, the range of sound pressure has been increased and the proposed control system improved the dynamic response of the air pressure control. },
address = {Sydney, Australia},
author = {Solis, Jorge and Petersen, Klaus and Yamamoto, Tetsuro and Takeuchi, Masaki and Ishikawa, Shimpei and Takanishi, Atsuo and Hashimoto, Kunimatsu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Auditory Feedback,Humanoid Robot,Music,Saxophone.},
pages = {156--161},
title = {{Development of the Waseda Saxophonist Robot and Implementation of an Auditory Feedback Control}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}156.pdf},
year = {2010}
}
@inproceedings{Park:2013,
abstract = {"Since Euler's development of the Tonnetz in 1739, musicians, composers andinstrument designers have been fascinated with the concept of musicalisomorphism, the idea that by arranging tones by their harmonic relationshipsrather than by their physical properties, the common shapes of musicalconstructs will appear, facilitating learning and new ways of exploringharmonic spaces. The construction of isomorphic instruments, beyond limitedsquare isomorphisms present in many stringed instruments, has been a challengein the past for two reasons: The first problem, that of re-arranging noteactuators from their sounding elements, has been solved by digital instrumentdesign. The second, more conceptual problem, is that only a single isomorphismcan be designed for any one instrument, requiring the instrument designer (aswell as composer and performer) to ""lock in"" to a single isomorphism, or tohave a different instrument for each isomorphism in order to experiment. Musix(an iOS application) and Rainboard (a physical device) are two new musicalinstruments built to overcome this and other limitations of existing isomorphicinstruments. Musix was developed to allow experimentation with a wide varietyof different isomorphic layouts, to assess the advantages and disadvantages ofeach. The Rainboard consists of a hexagonal array of arcade buttons embeddedwith RGB-LEDs, which are used to indicate characteristics of the isomorphismcurrently in use on the Rainboard. The creation of these two instruments /experimentation platforms allows for isomorphic layouts to be explored in waysthat are not possible with existing instruments."},
address = {Daejeon, Republic of Korea},
author = {Park, Brett and Gerhard, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {hexagon,isomorphic,keyboard,mobile application},
pages = {319--324},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Rainboard and Musix: Building dynamic isomorphic interfaces}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{dAlessandro2006,
address = {Paris, France},
author = {D'Alessandro, Nicolas and D'Alessandro, Christophe and {Le Beux}, Sylvain and Doval, Boris},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {4,formant synthesis,gestural control,instrument,links between spec-,model,singing synthesis,source,spectral,spectral modelling of voice,voice quality,voice source},
pages = {266--271},
title = {{Real-time CALM Synthesizer: New Approaches in Hands-Controlled Voice Synthesis}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}266.pdf},
year = {2006}
}
@inproceedings{Kikukawa:2012,
abstract = {We developed original solenoid actuator units with several built-in sensors, and produced a box-shaped musical inter- face ``PocoPoco'' using 16 units of them as a universal input/output device. We applied up-and-down movement of the solenoid-units and user's intuitive input to musical interface. Using transformation of the physical interface, we can apply movement of the units to new interaction design. At the same time we intend to suggest a new interface whose movement itself can attract the user.},
address = {Ann Arbor, Michigan},
author = {Kikukawa, Yuya and Kanai, Takaharu and Suzuki, Tatsuhiko and Yoshiike, Toshiki and Baba, Tetsuaki and Kushiyama, Kumiko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {interaction design,kinetic,moving,musical interface,tactile},
publisher = {University of Michigan},
title = {{PocoPoco: A Kinetic Musical Interface With Electro-Magnetic Levitation Units}},
year = {2012}
}
@inproceedings{Carlson2011,
address = {Oslo, Norway},
author = {Carlson, Chris and Marschner, Eli and Mccurry, Hunter},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {arduino,beagleboard,ccrma,force feedback,haptics,jack,linux audio,multi-channel audio,nime,pd,pure data,satellite ccrma,sound spatialization},
number = {June},
pages = {138--139},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{The Sound Flinger : A Haptic Spatializer}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}138.pdf},
year = {2011}
}
@inproceedings{Valle2008a,
address = {Genoa, Italy},
author = {Valle, Andrea},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {algorithmic composition,automatic notation,nime08},
pages = {253--256},
title = {{Integrated Algorithmic Composition Fluid systems for including notation in music composition cycle}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}253.pdf},
year = {2008}
}
@inproceedings{Bonardi2006,
address = {Paris, France},
author = {Bonardi, Alain and Truck, Isis and Akdag, Herman},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {emotion detector,fuzzy classes,intents,performance,stage director,virtual assistant},
pages = {326--329},
title = {{Towards a Virtual Assistant for Performers and Stage Directors}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}326.pdf},
year = {2006}
}
@inproceedings{Fyfe:2012,
abstract = {Message mapping between control interfaces and sound engines is an important task that could benefit from tools that streamline development. A new Open Sound Control (OSC) namespace called Nexus Data Exchange Format (NDEF) streamlines message mapping by offering developers the ability to manage sound engines as network nodes and to query those nodes for the messages in their OSC address spaces. By using NDEF, developers will have an eas- ier time managing nodes and their messages, especially for scenarios in which a single application or interface controls multiple sound engines. NDEF is currently implemented in the JunctionBox interaction toolkit but could easily be implemented in other toolkits.},
address = {Ann Arbor, Michigan},
author = {Fyfe, Lawrence and Tindale, Adam and Carpendale, Sheelagh},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {OSC,interaction,namespace,node},
publisher = {University of Michigan},
title = {{Node and Message Management with the JunctionBox Interaction Toolkit}},
year = {2012}
}
@inproceedings{John:2013,
abstract = {This paper reviews the mobile music projects that have been presented at NIMEin the past ten years in order to assess whether the changes in technology haveaffected the activities of mobile music research. An overview of mobile musicprojects is presented using the categories that describe the main activities:projects that explore the influence and make use of location; applications thatshare audio or promote collaborative composition; interaction using wearabledevices; the use of mobile phones as performance devices; projects that exploreHCI design issues. The relative activity between different types of activity isassessed in order to identify trends. The classification according totechnological, social or geographic showed an overwhelming bias to thetechnological, followed by social investigations. An alternative classificationof survey product, or artifact reveals an increase in the number of productsdescribed with a corresponding decline in the number of surveys and artisticprojects. The increase in technical papers appears to be due to an enthusiasmto make use of increased capability of mobile phones, although there are signsthat the initial interest has already peaked, and researchers are againinterested to explore technologies and artistic expression beyond existingmobile phones.},
address = {Daejeon, Republic of Korea},
author = {John, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Mobile Music,interaction design,interactive music,mobile phone performance,proximity sensing,wearable devices},
pages = {301--306},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Updating the Classifications of Mobile Music Projects}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{nime2014:ylim,
abstract = {Smartphone-based music conducting is a convenient and effective approach to conducting practice that aims to overcome the practical limitations of traditional conducting practice and provide enhanced user experience compared to those of previous virtual conducting examples. This work introduces the v-Maestro, a smartphone application for music conducting. Powered by the Gyroscope of the device, the v-Maestro analyzes conducting motions that allows the user to not only control the tempo but also simulate ``cueing" for different instruments. Results from user tests show that, in spite of certain ergonomic problems, new conducting practice with the v-Maestro is more satisfactory than traditional methods and has a strong potential as a conducting practice tool.},
address = {London, United Kingdom},
author = {Lim, Yang Kyu and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {573--576},
publisher = {Goldsmiths, University of London},
title = {{Smartphone-based Music Conducting}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}281.pdf},
year = {2014}
}
@inproceedings{Smallwood2011,
address = {Oslo, Norway},
author = {Smallwood, Scott},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {acoustic ecology,art,circuit bending,electroacoustics,hardware hacking,human-computer interface design,laptop orchestra,pv technology,solar sound arts,sound},
number = {June},
pages = {28--31},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Solar Sound Arts: Creating Instruments and Devices Powered by Photovoltaic Technologies}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}028.pdf},
year = {2011}
}
@inproceedings{Grosshauser:2012,
abstract = {From a technical point of view, instrumental music mak- ing involves audible, visible and hidden playing parameters. Hidden parameters like force, pressure and fast movements, happening within milliseconds are particularly difficult to capture. Here, we present data focusing on movement coordination parameters of the left hand fingers with the bow hand in violinists and between two violinists in group playing. Data was recorded with different position sensors, a micro camcorder fixed on a violin and an acceleration sensor placed on the bow. Sensor measurements were obtained at a high sampling rate, gathering the data with a small mi- crocontroller unit, connected with a laptop computer. To capture bow's position, rotation and angle directly on the bow to string contact point, the micro camcorder was fixed near the bridge. Main focuses of interest were the changes of the left hand finger, the temporal synchronization between left hand fingers with the right hand, the close up view to the bow to string contact point and the contact of the left hand finger and/or string to the fingerboard. Seven violinists, from beginners to master class students played scales in different rhythms, speeds and bowings and music excerpts of free choice while being recorded. One measure- ment with 2 violinists was made to see the time differences between two musicians while playing together. For simple integration of a conventional violin into electronic music environments, left hand sensor data were exemplary converted to MIDI and OSC.},
address = {Ann Arbor, Michigan},
author = {Grosshauser, Tobias and Candia, Victor and Hildebrand, Horst and Tr{\"{o}}ster, Gerhard},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {Strings,coordination,finger,hand,left,right,violin},
publisher = {University of Michigan},
title = {{Sensor Based Measurements of Musicians' Synchronization Issues}},
year = {2012}
}
@inproceedings{Richards2006,
address = {Paris, France},
author = {Richards, John},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {bastardisation,dirty electronics,diy,ebay,live,modular,performance,portability,post-digital,punktronics},
pages = {283--287},
title = {{32kg: Performance Systems for a Post-Digital Age}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}283.pdf},
year = {2006}
}
@inproceedings{Oliver2007,
address = {New York City, NY, United States},
author = {Oliver, Julian and Pickles, Steven},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {430},
title = {{Fijuu2 : A Game-Based Audio-Visual Performance and Composition Engine}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}430.pdf},
year = {2007}
}
@inproceedings{Cont:2004,
abstract = {In this paper, we describe an adaptive approach to gesture mapping for musical applications which serves as a mapping system for music instrument design. A neural network approach is chosen for this goal and all the required interfaces and abstractions are developed and demonstrated in the Pure Data environment. In this paper, we will focus on neural network representation and implementation in a real-time musical environment. This adaptive mapping is evaluated in different static and dynamic situations by a network of sensors sampled at a rate of 200Hz in real-time. Finally, some remarks are given on the network design and future works. },
address = {Hamamatsu, Japan},
author = {Cont, Arshia and Coduys, Thierry and Henry, Cyrille},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {Musical mapping algorithms and intelligent contro,Pure Data.,Real-time gesture control,Sensor and actuator technologies for musical appl,adaptive interfaces},
pages = {39--42},
title = {{Real-time Gesture Mapping in Pd Environment using Neural Networks}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}039.pdf},
year = {2004}
}
@inproceedings{Stead:2012,
abstract = {We describe a system that allows non-programmers to specify the grammar for a novel graphic score notation of their own design, defining performance notations suitable for drawing in live situations on a surface such as a whiteboard. Thescore can be interpreted via the camera of a smartphone,interactively scanned over the whiteboard to control the parameters of synthesisers implemented in Overtone. The visual grammar of the score, and its correspondence to the sound parameters, can be defined by the user with a simple visual condition-action language. This language can be edited on the touchscreen of an Android phone, allowing the grammar to be modified live in performance situations.Interactive scanning of the score is visible to the audience asa performance interface, with a colour classifier and visual feature recogniser causing the grammar-specified events to be sent using OSC messages via Wi-Fi from the hand-held smartphone to an audio workstation.},
address = {Ann Arbor, Michigan},
author = {Stead, Alistair G and Blackwell, Alan F and Aaron, Samual},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Com-puter Vision,Disposable Notation,Graphic Notation,Live Coding,Mobile Music},
month = {may},
publisher = {University of Michigan},
title = {{Graphic Score Grammars for End-Users}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}77.pdf},
year = {2012}
}
@inproceedings{Moody2007,
address = {New York City, NY, United States},
author = {Moody, Niall and Fells, Nick and Bailey, Nicholas},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {audiovisual,instrument,mappings,nime07,synchresis,x3d},
pages = {148--153},
title = {{Ashitaka : An Audiovisual Instrument}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}148.pdf},
year = {2007}
}
@inproceedings{McPherson:2012,
abstract = {Capacitive touch sensing is increasingly used in musical con- trollers, particularly those based on multi-touch screen interfaces. However, in contrast to the venerable piano-style keyboard, touch screen controllers lack the tactile feedback many performers find crucial. This paper presents an augmentation system for acoustic and electronic keyboards in which multi-touch capacitive sensors are added to the surface of each key. Each key records the position of fingers on the surface, and by combining this data with MIDI note onsets and aftertouch from the host keyboard, the system functions as a multidimensional polyphonic controller for a wide variety of synthesis software. The paper will discuss general capacitive touch sensor design, keyboard-specific implementation strategies, and the development of a flexible mapping engine using OSC and MIDI.},
address = {Ann Arbor, Michigan},
author = {McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {augmented instruments,capacitive sensing,keyboard,multitouch},
publisher = {University of Michigan},
title = {{TouchKeys: Capacitive Multi-Touch Sensing on a Physical Keyboard}},
year = {2012}
}
@inproceedings{Berdahl2008a,
abstract = {The music community has long had a strong interest in haptic technology. Recently, more effort has been put into making it more and more accessible to instrument designers.This paper covers some of these technologies with the aimof helping instrument designers add haptic feedback to theirinstruments. We begin by giving a brief overview of practicalactuators. Next, we compare and contrast using embeddedmicrocontrollers versus general purpose computers as controllers. Along the way, we mention some common softwareenvironments for implementing control algorithms. Then wediscuss the fundamental haptic control algorithms as well assome more complex ones. Finally, we present two practicaland effective haptic musical instruments: the haptic drumand the Cellomobo.},
address = {Genoa, Italy},
author = {Berdahl, Edgar and Steiner, Hans-Christoph and Oldham, Collin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Cellomobo,actuator,embedded,haptic,haptic drum,immersion,practical,sampling rate,woofer},
pages = {61--66},
title = {{Practical Hardware and Algorithms for Creating Haptic Musical Instruments}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}061.pdf},
year = {2008}
}
@inproceedings{Nagashima2007,
address = {New York City, NY, United States},
author = {Nagashima, Yoichi},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {kendang,media arts,new instruments,nime07,sound and light},
pages = {250--253},
title = {{GHI project and "Cyber Kendang"}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}250.pdf},
year = {2007}
}
@inproceedings{Heinz2010,
address = {Sydney, Australia},
author = {Heinz, Sebastian and O'Modhrain, Sile},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {collaborative performances,nime10,social,tangible user interfaces},
number = {Nime},
pages = {339--342},
title = {{Designing a Shareable Musical TUI}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}339.pdf},
year = {2010}
}
@inproceedings{Wilson2003,
address = {Montreal, QC, Canada},
author = {Wilson, Scott and Gurevich, Michael and Verplank, Bill and Stang, Pascal},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {24--29},
title = {{Microcontrollers in Music HCI Instruction: Reflections on our Switch to the Atmel AVR Platform}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}024.pdf},
year = {2003}
}
@inproceedings{McKinney:2013,
abstract = {In this paper we present Shoggoth, a 3D graphics based program for performingnetwork music. In Shoggoth, users utilize video game style controls to navigateand manipulate a grid of malleable height maps. Sequences can be created bydefining paths through the maps which trigger and modulate audio playback. Withrespect to a context of computer music performance, and specific problems innetwork music, design goals and technical challenges are outlined. The systemis evaluated through established taxonomies for describing interfaces, followedby an enumeration of the merits of 3D graphics in networked performance. Indiscussing proposed improvements to Shoggoth, design suggestions for otherdevelopers and network musicians are drawn out.},
address = {Daejeon, Republic of Korea},
author = {McKinney, Chad and Collins, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {3D,Environment,Generative,Network},
pages = {400--405},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{An Interactive 3D Network Music Space}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Lee:2012b,
abstract = {Tok! is a collaborative acoustic instrument application for iOS devices aimed at real time percussive music making in a colocated setup. It utilizes the mobility of hand-held devices and transforms them into drumsticks to tap on flat surfaces and produce acoustic music. Tok! is also networked and consists of a shared interactive music score to which the players tap their phones, creating a percussion ensemble. Through their social interaction and real-time modifications to the music score, and through their creative selection of tapping surfaces, the players can collaborate and dynamically create interesting rhythmic music with a variety of timbres.},
address = {Ann Arbor, Michigan},
author = {Lee, Sang Won and Srinivasamurthy, Ajay and Tronel, Gregoire and Shen, Weibin and Freeman, Jason},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Acoustic Musical Instrument,Collaboration,Mobile Phones,Social Interaction},
publisher = {University of Michigan},
title = {{Tok! : A Collaborative Acoustic Instrument using Mobile Phones}},
year = {2012}
}
@inproceedings{Wiley2009,
address = {Pittsburgh, PA, United States},
author = {Wiley, Meason and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {digital gamelan,digital tanpura,lasers,nime09,open,photocell sensor,source controller design,ultrasound},
pages = {43--44},
title = {{Multi-Laser Gestural Interface  Solutions for Cost-Effective and Open Source Controllers}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}043.pdf},
year = {2009}
}
@inproceedings{KITA:2013,
abstract = {When people learn using Web-based educational resources such as an LMS(Learning Management System) or other e-learning related systems, they aresitting in front of their own computer at home and are often physicallyisolated from other online learners. In some courses they are typically gettingin touch online with each others for doing some particular group workassignments, but most of the time they must do their own learning tasks alone.In other courses simply the individual assignments and quizzes are provided, sothe learners are alone all the time from the beginning until the end of thecourse.In order to keep the learners' motivation, it helps to feel other learnersdoing the same learning activities and belonging to the same course.Communicating formally or informally with other learners via Social NetworkingServices or something is one way for learners to get such a feeling, though ina way it might sometimes disturb their learning. Sonification of the access logof the e-learning system could be another indirect way to provide such afeeling.},
address = {Daejeon, Republic of Korea},
author = {KITA, Toshihiro and Osaka, Naotoshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Csound,Moodle,OSC (Open Sound Control),e-learning,online learners,realtime sonification},
pages = {198--199},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Providing a feeling of other remote learners' presence in an online learning environment via realtime sonification of Moodle access log}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Cappelen2011,
address = {Oslo, Norway},
author = {Cappelen, Birgitta and Anderson, Anders-Petter},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {design,genre,interaction,interactive installation,music instrument,musicking,narrative,open,role,sound art},
number = {June},
pages = {511--514},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Expanding the Role of the Instrument}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}511.pdf},
year = {2011}
}
@inproceedings{Lee2005,
abstract = {Discussion of time in interactive computer music systems engineering has been largely limited to data acquisition rates and latency.Since music is an inherently time-based medium, we believe thattime plays a more important role in both the usability and implementation of these systems. In this paper, we present a time designspace, which we use to expose some of the challenges of developing computer music systems with time-based interaction. Wedescribe and analyze the time-related issues we encountered whilstdesigning and building a series of interactive music exhibits thatfall into this design space. These issues often occur because ofthe varying and sometimes conflicting conceptual models of timein the three domains of user, application (music), and engineering.We present some of our latest work in conducting gesture interpretation and frameworks for digital audio, which attempt to analyzeand address these conflicts in temporal conceptual models.},
address = {Vancouver, BC},
author = {Lee, Eric and Borchers, Jan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {204--207},
title = {{The Role of Time in Engineering Computer Music Systems}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}204.pdf},
year = {2005}
}
@inproceedings{Eigenfeldt2008,
address = {Genoa, Italy},
author = {Eigenfeldt, Arne and Kapur, Ajay},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {agents,machine musicianship,nime08,robotic musical instruments},
pages = {144--149},
title = {{An Agent-based System for Robotic Musical Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}144.pdf},
year = {2008}
}
@inproceedings{Buxton2005,
address = {Vancouver, BC, Canada},
author = {Buxton, Bill},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {4},
title = {{Causality and Striking the Right Note}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}004.pdf},
year = {2005}
}
@inproceedings{Burtner:2002,
address = {Dublin},
author = {Burtner, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Burtner - 2002 - Noisegate 67 for Metasaxophone Composition and Performance Considerations of a New Computer Music Controller.pdf:pdf},
pages = {24--29},
title = {{Noisegate 67 for Metasaxophone: Composition and Performance Considerations of a New Computer Music Controller}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}024.pdf},
year = {2002}
}
@inproceedings{Kimura2008,
address = {Genoa, Italy},
author = {Kimura, Mari},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {augmented violin,gesture tracking,interactive performance,nime08},
pages = {219--220},
title = {{Making of VITESSIMO for Augmented Violin : Compositional Process and Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}219.pdf},
year = {2008}
}
@inproceedings{Gallin2011,
address = {Oslo, Norway},
author = {Gallin, Emmanuelle and Sirguy, Marc},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Gallin, Sirguy - 2011 - Eobody3 a Ready-to-use Pre-mapped {\&} Multi-protocol Sensor Interface.pdf:pdf},
keywords = {a,computer music,controller,cv,d converter,dmx,interface,midi,osc,sensor,usb},
number = {June},
pages = {437--440},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Eobody3: a Ready-to-use Pre-mapped {\&} Multi-protocol Sensor Interface}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}437.pdf},
year = {2011}
}
@inproceedings{Kim:2012,
abstract = {In this paper we introduce an interactive mobile music performance system using the digital compass of mobile phones. Compass-based interface can detect the aiming orientation of performers on stage, allowing us to obtain information on interactions between performers and use it for both musical mappings and visualizations on screen for the audience. We document and discuss the result of a compass- based mobile music performance, Where Are You Standing, and present an algorithm for a new app to track down the performers' positions in real-time.},
address = {Ann Arbor, Michigan},
author = {Kim, Bongjun and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Mobile music,aiming gesture,compass,magnetometer,mobile phone,musical mapping,musical sonification,smartphone},
publisher = {University of Michigan},
title = {{Interactive Mobile Music Performance with Digital Compass}},
year = {2012}
}
@inproceedings{Dobrian2003,
address = {Montreal, QC, Canada},
author = {Dobrian, Christopher and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {161--163},
title = {{Gestural Control of Music Using the Vicon 8 Motion Capture System}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}161.pdf},
year = {2003}
}
@inproceedings{Loscos2005,
address = {Vancouver, BC, Canada},
author = {Loscos, Alex and Aussenac, Thomas},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {172--175},
title = {{The wahwactor: a voice controlled wah-wah pedal}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}172.pdf},
year = {2005}
}
@inproceedings{Forsyth2011,
address = {Oslo, Norway},
author = {Forsyth, Jon and Glennon, Aron and Bello, Juan P},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {interactive systems,ipad,multi-touch,remix,sample editor},
number = {June},
pages = {487--490},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Random Access Remixing on the iPad}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}487.pdf},
year = {2011}
}
@inproceedings{Singer2005,
address = {Vancouver, BC, Canada},
author = {Singer, Eric and Feddersen, Jeff and Bowen, Bil},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {50--55},
title = {{A Large-Scale Networked Robotic Musical Instrument Installation}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}050.pdf},
year = {2005}
}
@inproceedings{Everett:2012,
address = {Ann Arbor, Michigan},
author = {Everett, Steve},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
publisher = {University of Michigan},
title = {{First Life - Imagining the Chemical Origins of Life}},
year = {2012}
}
@inproceedings{GreshamLancaster2009,
address = {Pittsburgh, PA, United States},
author = {Gresham-Lancaster, Scot and Bull, Steve},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {338},
title = {{Cellphonia: 4'33"}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}338.pdf},
year = {2009}
}
@inproceedings{Zamborlin2011,
address = {Oslo, Norway},
author = {Zamborlin, Bruno and Partesana, Giorgio and Liuni, Marco},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {gesture recognition,landscape,mapping gesture-audio-video},
number = {June},
pages = {537--538},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{(LAND)MOVES}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}537.pdf},
year = {2011}
}
@inproceedings{Nagashima2006,
address = {Paris, France},
author = {Nagashima, Yoichi},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {interactive installation,media arts,sensors,studio reports},
pages = {330--333},
title = {{Students' Projects of Interactive Media-installations in SUAC}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}330.pdf},
year = {2006}
}
@inproceedings{Endo:2012a,
abstract = {Tweet Harp is a musical instrument using Twitter and a laser harp. This instrument features the use of the human voice speaking tweets in Twitter as sounds for music. It is played by touching the six harp strings of laser beams. Tweet Harp gets the latest tweets from Twitter in real-time, and it creates music like a song with unexpected words. It also creates animation displaying the texts at the same time. The audience can visually enjoy this performance by sounds synchronized with animation. If the audience has a Twitter account, they can participate in the performance by tweeting.},
address = {Ann Arbor, Michigan},
author = {Endo, Ayaka and Moriyama, Takuma and Kuhara, Yasuo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {AppleScript,Arduino,Max/MSP,Quartz Composer,TTS,Twitter,laser harp,speech,text,voice},
publisher = {University of Michigan},
title = {{Tweet Harp: Laser Harp Generating Voice and Text of Real-time Tweets in Twitter}},
year = {2012}
}
@inproceedings{Jorda2004,
address = {Hamamatsu, Japan},
author = {Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Jord{\`{a}} - 2004 - Digital Instruments and Players Part I  Efficiency and Apprenticeship.pdf:pdf},
keywords = {apprenticeship,learning curve,musical instruments design},
pages = {59--63},
title = {{Digital Instruments and Players: Part I  Efficiency and Apprenticeship}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}059.pdf},
year = {2004}
}
@inproceedings{Burtner2004,
address = {Hamamatsu, Japan},
author = {Burtner, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {composition,multimedia,music,music and video controllers,mythology,new interface design,shamanism},
pages = {193--196},
title = {{A Theory of Modulated Objects for New Shamanic Controller Design}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}193.pdf},
year = {2004}
}
@inproceedings{Nagashima2010,
abstract = {This paper is a report on the development of a new musical instrument in which the main concept is "Untouchable". The key concept of this instrument is "sound generation by body gesture (both hands)" and "sound generation by kneading with hands". The new composition project had completed as the premiere of a new work "controllable untouchableness" with this new instrument in December 2009.},
address = {Sydney, Australia},
author = {Nagashima, Yoichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Propeller processor,Theremin,distance sensor,untouchable},
pages = {387--390},
title = {{Untouchable Instrument "Peller-Min"}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}387.pdf},
year = {2010}
}
@inproceedings{Carlile2005,
address = {Vancouver, BC, Canada},
author = {Carlile, Jennifer and Hartmann, Bj{\"{o}}rn},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {250--251},
title = {{OROBORO: A Collaborative Controller with Interpersonal Haptic Feedback}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}250.pdf},
year = {2005}
}
@inproceedings{Subramanian:2012,
abstract = {This paper describes a recent addition to LOLC, a text-based environment for collaborative improvisation for laptop ensembles, incorporating a machine musician that plays along with human performers. The machine musician LOLbot analyses the patterns created by human performers and the composite music they create as they are layered in performance. Based on user specified settings, LOLbot chooses appropriate patterns to play with the ensemble, either to add contrast to the existing performance or to be coherent with the rhythmic structure of the performance. The paper describes the background and motivations of the project, outlines the design of the original LOLC environment and describes the architecture and implementation of LOLbot.},
address = {Ann Arbor, Michigan},
author = {Subramanian, Sidharth and Freeman, Jason and McCoid, Scott},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Laptop Orchestra,Live Coding,Machine Musicianship},
publisher = {University of Michigan},
title = {{LOLbot: Machine Musicianship in Laptop Ensembles}},
year = {2012}
}
@inproceedings{Donnarumma:2013,
abstract = {We present the first combined use of the electromyogram (EMG) andmechanomyogram (MMG), two biosignals that result from muscular activity, forinteractive music applications. We exploit differences between these twosignals, as reported in the biomedical literature, to create bi-modalsonification and sound synthesis mappings that allow performers to distinguishthe two components in a single complex arm gesture. We study non-expertplayers' ability to articulate the different modalities. Results show thatpurposely designed gestures and mapping techniques enable novices to rapidlylearn to independently control the two biosignals.},
address = {Daejeon, Republic of Korea},
author = {Donnarumma, Marco and Caramiaux, Baptiste and Tanaka, Atau},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {EMG,MMG,NIME,biosignal,mapping,multimodal,sensorimotor system},
pages = {128--131},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Muscular Interactions. Combining EMG and MMG sensing for musical practice}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Freed2011,
abstract = {An effective programming style for gesture signal processing is described using a new library that brings efficient run-time polymorphism, functional and instance-based object-oriented programming to Max/MSP. By introducing better support for generic programming and composability Max/MSP becomes a more productive environment for managing the growing scale and complexity of gesture sensing systems for musical instruments and interactive installations. },
address = {Oslo, Norway},
author = {Freed, Adrian and MacCallum, John and Schmeder, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {composability,delegation,functional programming,gesture signal,max,msp,object,object-,open sound control,oriented programming,processing},
pages = {308--311},
title = {{Composability for Musical Gesture Signal Processing using new OSC-based Object and Functional Programming Extensions to Max/MSP}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}308.pdf},
year = {2011}
}
@inproceedings{Lehrman2006,
address = {Paris, France},
author = {Lehrman, Paul D and Singer, Eric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Dadaism.,MIDI,Robotics,computer control,mechanical music,percussion,player pianos,sound effects},
pages = {300--303},
title = {{A "Ballet M{\'{e}}canique" for the 21{\{}s{\}}t Century: Performing George Antheil's Dadaist Masterpiece with Robots}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}300.pdf},
year = {2006}
}
@inproceedings{Mase2001,
abstract = {In this paper, we introduce our research challenges for creating new musical instruments using everyday-life media with intimate interfaces, such as the self-body, clothes, water and stuffed toys. Various sensor technologies including image processing and general touch sensitive devices are employed to exploit these interaction media. The focus of our effort is to provide user-friendly and enjoyable experiences for new music and sound performances. Multimodality of musical instruments is explored in each attempt. The degree of controllability in the performance and the richness of expressions are also discussed for each installation. },
address = {Seattle, WA},
author = {Mase, Kenji and Yonezawa, Tomoko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
keywords = {New interface,dance,image processing,music controller,stuffed toy,water interface},
pages = {34--37},
title = {{Body , Clothes , Water and Toys : Media Towards Natural Music Expressions with Digital Sounds}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}034.pdf},
year = {2001}
}
@inproceedings{Baba2007,
address = {New York City, NY, United States},
author = {Baba, Tetsuaki and Ushiama, Taketoshi and Tomimatsu, Kiyoshi},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::;::},
keywords = {a child playing the,figure 1,freqtric drums,interac-,interpersonal communication,musical instrument,nime07,skin contact,tion design,touch},
pages = {386--387},
title = {{Freqtric Drums : A Musical Instrument that Uses Skin Contact as an Interface}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}386.pdf},
year = {2007}
}
@inproceedings{Lebel2006,
address = {Paris, France},
author = {Lebel, Denis and Malloch, Joseph},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Digital musical instrument,kinesthetic feedback},
pages = {85--88},
title = {{The G-Spring Controller}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}085.pdf},
year = {2006}
}
@inproceedings{Gaye:2004,
abstract = {Sonic City is a wearable system enabling the use of the urban environment as an interface for real-time electronic music making, when walking through and interacting with a city. The device senses everyday interactions and surrounding contexts, and maps this information in real time to the sound processing of urban sounds. We conducted a short-term study with various participants using our prototype in everyday settings. This paper describes the course of the study and preliminary results in terms of how the participants used and experienced the system. These results showed that the city was perceived as the main performer but that the user improvised different tactics and ad hoc interventions to actively influence and participate in how the music was created. },
address = {Hamamatsu, Japan},
author = {Gaye, Lalya and Holmquist, Lars E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {User study,context-awareness.,interactive music,mobility,new interface for musical expression,wearable computing},
pages = {161--164},
title = {{In Duet with Everyday Urban Settings: A User Study of Sonic City}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}161.pdf},
year = {2004}
}
@inproceedings{Marier2010,
abstract = {The sponge is an interface that allows a clear link to beestablished between gesture and sound in electroacousticmusic. The goals in developing the sponge were to reintroduce the pleasure of playing and to improve the interaction between the composer/performer and the audience. Ithas been argued that expenditure of effort or energy is required to obtain expressive interfaces. The sponge favors anenergy-sound relationship in two ways : 1) it senses acceleration, which is closely related to energy; and 2) it is madeout of a flexible material (foam) that requires effort to besqueezed or twisted. Some of the mapping strategies usedin a performance context with the sponge are discussed.},
address = {Sydney, Australia},
author = {Marier, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Interface,electroacoustic music,expressivity,mapping,performance},
pages = {356--359},
title = {{The Sponge A Flexible Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}356.pdf},
year = {2010}
}
@inproceedings{Diakopoulos:2013,
abstract = {Netpixl is a new micro-toolkit built to network devices within interactiveinstallations and environments. Using a familiar client-server model, Netpixlcentrally wraps an important aspect of ubiquitous computing: real-timemessaging. In the context of sound and music computing, the role of Netpixl isto fluidly integrate endpoints like OSC and MIDI within a larger multi-usersystem. This paper considers useful design principles that may be applied totoolkits like Netpixl while also emphasizing recent approaches to applicationdevelopment via HTML5 and Javascript, highlighting an evolution in networkedcreative computing.},
address = {Daejeon, Republic of Korea},
author = {Diakopoulos, Dimitri and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {html5,networking,toolkits,ubiquitious computing},
pages = {206--209},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Netpixl: Towards a New Paradigm for Networked Application Development}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Siwiak2009,
abstract = {Catch Your Breath is an interactive audiovisual bio-feedbacksystem adapted from a project designed to reduce respiratory irregularity in patients undergoing 4D CT scans for oncological diagnosis. The system is currently implementedand assessed as a potential means to reduce motion-induceddistortion in CT images.A museum installation based on the same principle wascreated in which an inexpensive wall-mounted web camera tracks an IR sensor embedded into a pendant worn bythe user. The motion of the subjects breathing is trackedand interpreted as a real-time variable tempo adjustment toa stored musical file. The subject can then adjust his/herbreathing to synchronize with a separate accompanimentline. When the breathing is regular and is at the desiredtempo, the audible result sounds synchronous and harmonious. The accompaniment's tempo progresses and gradually decrease which causes the breathing to synchronize andslow down, thus increasing relaxation.},
address = {Pittsburgh, PA, United States},
author = {Siwiak, Diana and Berger, Jonathan and Yang, Yao},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {auditory display.,music,sensor},
pages = {153--154},
title = {{Catch Your Breath - Musical Biofeedback for Breathing Regulation}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}153.pdf},
year = {2009}
}
@inproceedings{Moriwaki2007,
address = {New York City, NY, United States},
author = {Moriwaki, Katherine and Brucken-Cohen, Jonah},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {168--172},
title = {{MIDI Scrapyard Challenge Workshops}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}168.pdf},
year = {2007}
}
@inproceedings{Makelberge:2012,
abstract = {"Perfect Take" is a public installation out of networked acoustic instruments that let composers from all over the world exhibit their MIDI-works by means of the Internet. The primary aim of this system is to offer composers a way to have works exhibited and recorded in venues and with technologies not accessible to him/her under normal circumstances. The Secondary aim of this research is to highlight experience design as a complement to interaction design, and a shift of focus from functionality of a specific gestural controller, towards the environments, events and processes that they are part of.},
address = {Ann Arbor, Michigan},
author = {Makelberge, Nicolas and Barbosa, {\'{A}}lvaro and Perrotta, Andr{\'{e}} and Ferreira, Lu{\'{i}}s Sarmento},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Disklavier,MIDI,NIME,Networked Music,creativity,music collaboration},
publisher = {University of Michigan},
title = {{Perfect Take: Experience design and new interfaces for musical expression}},
year = {2012}
}
@inproceedings{Grossmann2010,
address = {Sydney, Australia},
author = {Grossmann, Cesar M},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {live processing,new instruments,nime10,recorder},
number = {Nime},
pages = {223--228},
title = {{Developing a Hybrid Contrabass Recorder Resistances, Expression, Gestures and Rhetoric}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}223.pdf},
year = {2010}
}
@inproceedings{Kanebako2007,
address = {New York City, NY, United States},
author = {Kanebako, Junichi and Gibson, James and Mignonneau, Laurent},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {guitar instrument,intuitive interaction,midi to sensor mapping,musical expression,nime07,physical computing},
pages = {396--398},
title = {{Mountain Guitar : a Musical Instrument for Everyone}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}396.pdf},
year = {2007}
}
@inproceedings{Ouzounian:2012,
abstract = {Music for Sleeping {\&} Waking Minds (2011-2012) is a new, overnight work in which four performers fall asleep while wearing custom designed EEG sensors which monitor their brainwave activity. The data gathered from the EEG sensors is applied in real time to different audio and image signal processing functions, resulting in continuously evolving multi- channel sound environment and visual projection. This material serves as an audiovisual description of the individual and collective neurophysiological state of the ensemble. Audiences are invited to experience the work in different states of attention: while alert and asleep, resting and awakening.},
address = {Ann Arbor, Michigan},
author = {Ouzounian, Gascia and Knapp, R Benjamin and Lyon, Eric and DuBois, Luke},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {BCI,EEG,bio art,biosignals,consciousness,dream,sleep},
publisher = {University of Michigan},
title = {{Music for Sleeping {\&} Waking Minds (paper submission)}},
year = {2012}
}
@inproceedings{Nishibori2006,
abstract = {Development of a musical interface which allows people to play music intuitively and create music visibly. },
address = {Paris, France},
author = {Nishibori, Yu and Iwai, Toshio},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
pages = {172--175},
title = {{TENORI-ON}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}172.pdf},
year = {2006}
}
@inproceedings{Aimi2007,
abstract = {This paper describes several example hybrid acoustic / electronic percussion instruments using realtime convolution toaugment and modify the apparent acoustics of damped physical objects. Examples of cymbal, frame drum, practice pad,brush, and bass drum controllers are described.},
address = {New York City, NY, United States},
author = {Aimi, Roberto},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Musical controllers,extended acoustic instruments},
pages = {154--159},
title = {{Percussion Instruments Using Realtime Convolution : Physical Controllers}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}154.pdf},
year = {2007}
}
@inproceedings{Favilla2006,
address = {Paris, France},
author = {Favilla, Stuart and Cannon, Joanne},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
pages = {370--375},
title = {{Children of Grainger: Leather Instruments for Free Music}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}370.pdf},
year = {2006}
}
@inproceedings{Bevilacqua2005,
address = {Vancouver, BC, Canada},
author = {Bevilacqua, Fr{\'{e}}d{\'{e}}ric and M{\"{u}}ller, R{\'{e}}my and Schnell, Norbert},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {85--88},
title = {{MnM: a Max/MSP mapping toolbox}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}085.pdf},
year = {2005}
}
@inproceedings{Roberts2010,
address = {Sydney, Australia},
author = {Roberts, Charles and Wright, Matthew and Kuchera-Morin, JoAnn and Putnam, Lance},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {allosphere,hci,interactivity,mapping,multi-user,network,nime10,osc,performance,tual reality,vir-},
number = {Nime},
pages = {57--62},
title = {{Dynamic Interactivity Inside the AlloSphere}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}057.pdf},
year = {2010}
}
@inproceedings{Bernard2002,
address = {Dublin, Ireland},
author = {Bernard, David},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {audio-visual synchronisation,controllers,copyright,experimental instruments,live electronic music,midi,skins digital hand drum},
number = {1996},
pages = {10--11},
title = {{Experimental Controllers for Live Electronic Music Performance (vs. Copyright).}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}010.pdf},
year = {2002}
}
@inproceedings{Eigenfeldt2008,
abstract = {This paper presents an agent-based architecture for robotic musical instruments that generate polyphonic rhythmic patterns that continuously evolve and develop in a musically "intelligent" manner. Agent-based software offers a new method for real-time composition that allows for complex interactions between individual voices while requiring very little user interaction or supervision. The system described, Kinetic Engine, is an environment in which individual software agents, emulate drummers improvising within a percussion ensemble. Player agents assume roles and personalities within the ensemble, and communicate with one another to create complex rhythmic interactions. In this project, the ensemble is comprised of a 12-armed musical robot, MahaDeviBot, in which each limb has its own software agent controlling what it performs. },
address = {Genoa, Italy},
author = {Eigenfeldt, Arne and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Agents,Machine Musicianship.,Robotic Musical Instruments},
pages = {144--149},
title = {{An Agent-based System for Robotic Musical Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}144.pdf},
year = {2008}
}
@inproceedings{Marquez-Borbon2011,
abstract = {As NIME's focus has expanded beyond the design reportswhich were pervasive in the early days to include studies andexperiments involving music control devices, we report on aparticular area of activity that has been overlooked: designsof music devices in experimental contexts. We demonstratethis is distinct from designing for artistic performances, witha unique set of novel challenges. A survey of methodologicalapproaches to experiments in NIME reveals a tendency torely on existing instruments or evaluations of new devicesdesigned for broader creative application. We present twoexamples from our own studies that reveal the merits ofdesigning purpose-built devices for experimental contexts.},
address = {Oslo, Norway},
author = {Marquez-Borbon, Adnan and Gurevich, Michael and Fyans, A Cavan and Stapleton, Paul},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {DMIs,Experiment,Instrument Design,Methodology},
pages = {373--376},
title = {{Designing Digital Musical Interactions in Experimental Contexts}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}373.pdf},
year = {2011}
}
@inproceedings{Hahnel2010,
address = {Sydney, Australia},
author = {H{\"{a}}hnel, Tilo},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {articulation,expres-,historically informed performance,nime10,sive performance,synthetic performance},
number = {Nime},
pages = {72--75},
title = {{From Mozart to MIDI : A Rule System for Expressive Articulation}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}072.pdf},
year = {2010}
}
@inproceedings{Ferguson:2013,
abstract = {Feedback created by guitars and amplifiers is difficult to use in musicalsettings -- parameters such as pitch and loudness are hard to specify preciselyby fretting a string or by holding the guitar near an amplifier. This researchinvestigates methods for controlling the level and pitch of the feedbackproduced by a guitar and amplifier, which are based on incorporatingcorpus-based control into the system. Two parameters are used to define thecontrol parameter space -- a simple automatic gain control system to controlthe output level, and a band-pass filter frequency for controlling the pitch ofthe feedback. This control parameter space is mapped to a corpus of soundscreated by these parameters and recorded, and these sounds are analysed usingsoftware created for concatenative synthesis. Following this process, thedescriptors taken from the analysis can be used to select control parametersfrom the feedback system.},
address = {Daejeon, Republic of Korea},
author = {Ferguson, Sam and Martin, Aengus and Johnston, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
month = {may},
pages = {541--546},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A corpus-based method for controlling guitar feedback}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Freed2009a,
address = {Pittsburgh, PA, United States},
author = {Freed, Adrian and Schmeder, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {and,implemented in an increasing,media software development environments,nime09,number of core music,open sound control,osc,reservation protocols,time tag},
pages = {116--120},
title = {{Features and Future of Open Sound Control version 1.1 for NIME}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}116.pdf},
year = {2009}
}
@inproceedings{Poepel2007,
abstract = {Many fascinating new developments in the area bowed stringed instruments have been developed in recent years. However, the majority of these new applications are either not well known, used orconsidered in a broader context by their target users. The necessaryexchange between the world of developers and the players is ratherlimited. A group of performers, researchers, instrument developersand composers was founded in order to share expertise and experiences and to give each other feedback on the work done to developnew instruments. Instruments incorporating new interfaces, synthesis methods, sensor technology, new materials like carbon fiber andwood composites as well as composite materials and research outcome are presented and discussed in the group. This paper gives anintroduction to the group and reports about activities and outcomesin the last two years.},
address = {New York City, NY, United States},
author = {Poepel, Cornelius and Marx, G{\"{u}}nter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Interdisciplinary user group,electronic bowed string instrument,evaluation of computer based musical instruments},
pages = {281--284},
title = {{{\textgreater}hot{\_}strings SIG}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}281.pdf},
year = {2007}
}
@inproceedings{Zhang:2013,
abstract = {The Microsoft Kinect is a popular and versatile input devicefor musical interfaces. However, using the Kinect for suchinterfaces requires not only signi{\_}x000C{\_}cant programming experience,but also the use of complex geometry or machinelearning techniques to translate joint positions into higherlevel gestures. We created the Kinect Instrument Builder(KIB) to address these di{\_}x000E{\_}culties by structuring gesturalinterfaces as combinations of gestural widgets. KIB allowsthe user to design an instrument by con{\_}x000C{\_}guring gesturalprimitives, each with a set of simple but attractive visualfeedback elements. After designing an instrument on KIB'sweb interface, users can play the instrument on KIB's performanceinterface, which displays visualizations and transmitsOSC messages to other applications for sound synthesisor further remapping.},
address = {Daejeon, Republic of Korea},
author = {Zhang, Edward},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Kinect,OSC,gesture,mapping,widgets},
pages = {519--524},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{KIB: Simplifying Gestural Instrument Creation Using Widgets}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Schiesser2006,
address = {Paris, France},
author = {Schiesser, S{\'{e}}bastien and Traube, Caroline},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Schiesser, Traube - 2006 - On making and playing an electronically-augmented saxophone.pdf:pdf},
keywords = {augmented instrument,gestural control,live electronics,perfor- mance,saxophone},
pages = {308--313},
title = {{On Making and Playing an Electronically-augmented Saxophone}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}308.pdf},
year = {2006}
}
@inproceedings{nime2014:ngold,
abstract = {The Leap Motion(TM) sensor offers fine-grained gesture-recognition and hand tracking. Since its release, there have been several uses of the device for instrument design, musical interaction and expression control, documented through online video. However, there has been little formal documented investigation of the potential and challenges of the platform in this context. This paper presents lessons learned from work-in-progress on the development of musical instruments and control applications using the Leap Motion(TM) sensor. Two instruments are presented: Air-Keys and Air-Pads and the potential for augmentation of a traditional keyboard is explored. The results show that the platform is promising in this context but requires various challenges, both physical and logical, to be overcome.},
address = {London, United Kingdom},
author = {Han, Jihyun and Gold, Nicolas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {371--374},
publisher = {Goldsmiths, University of London},
title = {{Lessons Learned in Exploring the Leap Motion(TM) Sensor for Gesture-based Instrument Design}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}485.pdf},
year = {2014}
}
@inproceedings{Gimenes2007,
address = {New York City, NY, United States},
author = {Gimenes, Marcelo and Miranda, Eduardo and Johnson, Chris},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {artificial life,musical style,musicianship,nime07},
pages = {197--202},
title = {{Musicianship for Robots with Style}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}197.pdf},
year = {2007}
}
@inproceedings{Robson2001,
address = {Seattle, WA, United States},
author = {Robson, Dominic},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
keywords = {and,components,linear potentiometers etc,of these projects was,simple physical interface using,the process for each,tilt switches,to design a,very basic electronic},
pages = {51--53},
title = {{PLAY! : Sound Toys For the Non Musical}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}051.pdf},
year = {2001}
}
@inproceedings{Miller2010,
address = {Sydney, Australia},
author = {Miller, Jace and Hammond, Tracy},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {cello,figure 1,gesture recognition,human computer interaction,motion,nime10,recognition,the orientation of the,violin,virtual instrument,wii remote,wii remote determines},
number = {June},
pages = {15--18},
title = {{Wiiolin : a Virtual Instrument Using the Wii Remote}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}497.pdf},
year = {2010}
}
@inproceedings{Livingstone2005,
address = {Vancouver, BC, Canada},
author = {Livingstone, Dan and Miranda, Eduardo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Adaptive System,Conscious Subconscious Interaction.,Music Robots,Smart Interfaces,Sound Installation,Spatial Music},
pages = {65--69},
title = {{Orb3 -- Adaptive Interface Design for Real time Sound Synthesis {\&} Diffusion within Socially Mediated Spaces}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}065.pdf},
year = {2005}
}
@inproceedings{Dahl2011,
address = {Oslo, Norway},
author = {Dahl, Luke and Herrera, Jorge and Wilkerson, Carr},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {audience participation,data visual-,interaction,ization,multi-user instrument,sonification,text processing,twitter},
number = {June},
pages = {272--275},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{TweetDreams : Making Music with the Audience and the World using Real-time Twitter Data}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}272.pdf},
year = {2011}
}
@inproceedings{Flety2011,
address = {Oslo, Norway},
author = {Fl{\'{e}}ty, Emmanuel and Maestracci, C{\^{o}}me},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Fl{\'{e}}ty, Maestracci - 2011 - Latency Improvement in Sensor Wireless Transmission Using IEEE 802.15.4.pdf:pdf},
keywords = {15,4,802,embedded sensors,gesture recognition,interaction,music computing,sound and,wireless,zigbee},
number = {June},
pages = {409--412},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Latency Improvement in Sensor Wireless Transmission Using IEEE 802.15.4}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}409.pdf},
year = {2011}
}
@inproceedings{Corness2008,
abstract = {Through the developing of tools for analyzing the performerssonic and movement-based gestures, research into the systemperformer interaction has focused on the computer's ability torespond to the performer. Where as such work shows interestwithin the community in developing an interaction paradigmmodeled on the player, by focusing on the perception andreasoning of the system, this research assumes that theperformer's manner of interaction is in agreement with thiscomputational model. My study presents an alternative model ofinteraction designed for improvisatory performance centered onthe perception of the performer as understood by theories takenfrom performance practices and cognitive science.},
address = {Genoa, Italy},
author = {Corness, Greg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {HCI,Interactive performance,Perception},
pages = {265--268},
title = {{Performer Model : Towards a Framework for Interactive Performance Based on Perceived Intention}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}265.pdf},
year = {2008}
}
@inproceedings{Topper2007,
address = {New York City, NY, United States},
author = {Topper, David},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {203--204},
title = {{Extended Applications of the Wireless Sensor Array (WISEAR)}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}203.pdf},
year = {2007}
}
@inproceedings{Fiebrink2007,
address = {New York City, NY, United States},
author = {Fiebrink, Rebecca and Wang, Ge and Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {164--167},
title = {{Don {\{}$\backslash$textquoteright{\}} t Forget the Laptop : Using Native Input Capabilities for Expressive Musical Control}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}164.pdf},
year = {2007}
}
@inproceedings{Biggs2007,
address = {New York City, NY, United States},
author = {Biggs, Betsey},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {424},
title = {{The Tipping Point}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}424.pdf},
year = {2007}
}
@inproceedings{Tseng2011,
address = {Oslo, Norway},
author = {Tseng, Yu-Chung and Liu, Che-Wei and Chi, Tzu-Heng and Wang, Hui-Yu},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
number = {June},
pages = {320--321},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Sound Low Fun}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}320.pdf},
year = {2011}
}
@inproceedings{Jackie:2013,
abstract = {SoloTouch is a guitar inspired pocket sized controller system that consists ofa capacitive touch trigger and a lick-based note selector. The touch triggerallows an intuitive way to play both velocity sensitive notes and vibratoexpressively using only one finger. The lick-based note selector is an originalconcept that provides the player an easy way to play expressive melodic linesby combining pre-programmed ``licks'' without the need to learn the actualnotes. The two-part controller is primarily used as a basic MIDI controller forplaying MIDI controlled virtual instruments, normally played by keyboardcontrollers. The controller is targeted towards novice musicians, playerswithout prior musical training could play musical and expressive solos,suitable for improvised jamming along modern popular music.},
address = {Daejeon, Republic of Korea},
author = {Jackie and Chui, Yi Tang and Marafa, Mubarak and Samson and Young, Ka Fai},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Capacitive touch controller,automated note selector,novice musicians.,virtual instrument MIDI controller},
pages = {389--393},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{SoloTouch: A Capacitive Touch Controller with Lick-based Note Selector}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Freed2006a,
abstract = {Software and hardware enhancements to an electric 6-stringcello are described with a focus on a new mechanical tuningdevice, a novel rotary sensor for bow interaction and controlstrategies to leverage a suite of polyphonic soundprocessing effects.},
address = {Paris, France},
author = {Freed, Adrian and Wessel, David and Zbyszynski, Michael and Uitti, Frances M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Freed et al. - 2006 - Augmenting the Cello.pdf:pdf},
keywords = {Cello,Double Bowing,FSR,Rotary Absolute Position Encoder,chordophone,convolution.,double stops,triple stops},
pages = {409--413},
title = {{Augmenting the Cello}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}409.pdf},
year = {2006}
}
@inproceedings{Ben-Asher:2013,
abstract = {A system is presented for detecting common gestures, musical intentions andemotions of pianists in real-time using only kinesthetic data retrieved bywireless motion sensors. The algorithm can detect common Western musicalstructures such as chords, arpeggios, scales, and trills as well as musicallyintended emotions: cheerful, mournful, vigorous, dreamy, lyrical, and humorouscompletely and solely based on low-sample-rate motion sensor data. Thealgorithm can be trained per performer in real-time or can work based onprevious training sets. The system maps the emotions to a color set andpresents them as a flowing emotional spectrum on the background of a pianoroll. This acts as a feedback mechanism for emotional expression as well as aninteractive display of the music. The system was trained and tested on a numberof pianists and it classified structures and emotions with promising results ofup to 92{\%} accuracy.},
address = {Daejeon, Republic of Korea},
author = {Ben-Asher, Matan and Leider, Colby},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Computer Music,Expressive Piano Performance,IMUs,Machine Learning,Motion Sensors,Music and Emotion},
pages = {21--24},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Toward an Emotionally Intelligent Piano: Real-Time Emotion Detection and Performer Feedback via Kinesthetic Sensing in Piano Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{nime2014:slee,
abstract = {In the setting of collaborative live coding, a number of issues emerge: (1) need for communication, (2) issues of conflicts in sharing program state space, and (3) remote control of code execution. In this paper, we propose solutions to these problems. In the recent extension of UrMus, a programming environment for mobile music application development, we introduce a paradigm of shared and individual namespaces safeguard against conflicts in parallel coding activities. We also develop live variable view that communicates live changes in state among live coders, networked performers, and the audience. Lastly, we integrate collaborative aspects of programming execution into built-in live chat, which enables not only communication with others, but also distributed execution of code.},
address = {London, United Kingdom},
author = {Lee, Sang Won and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {263--268},
publisher = {Goldsmiths, University of London},
title = {{Communication, Control, and State Sharing in Collaborative Live Coding}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}554.pdf},
year = {2014}
}
@inproceedings{Mandelis:2004,
abstract = {This paper describes the use of evolutionary and artificial life techniques in sound design and the development of performance mapping to facilitate the real-time manipulation of such sounds through some input device controlled by the performer. A concrete example of such a system is described which allows musicians without detailed knowledge and experience of sound synthesis techniques to interactively develop new sounds and performance manipulation mappings according to their own aesthetic judgements. Experiences with the system are discussed. },
address = {Hamamatsu, Japan},
author = {Mandelis, James and Husbands, Phil},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {musical interaction,performance mapping,sound synthesis},
pages = {47--50},
title = {{Don't Just Play it, Grow it! : Breeding Sound Synthesis and Performance Mappings}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}047.pdf},
year = {2004}
}
@inproceedings{Young:2002,
address = {Dublin},
author = {Young, Diana},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Hyperbow,Hyperinstrument,Hyperviolin,accelerometer,bow,position sensor,strain sensor,violin},
pages = {201--206},
title = {{The Hyperbow Controller: Real-Time Dynamics Measurement of Violin Performance}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}201.pdf},
year = {2002}
}
@inproceedings{Dahl:2012,
abstract = {Composing music for ensembles of computer-based instruments, such as laptop orchestra or mobile phone orchestra, is a multi-faceted and challenging endeavor whose parameters and criteria for success are ill-defined. In the design community, tasks with these qualities are known as wicked problems. This paper frames composing for computer-based ensemble as a design task, shows how Buchanan's four domains of design are present in the task, and discusses its wicked properties. The themes of visibility, risk, and embodiment, as formulated by Klemmer, are shown to be implicitly present in this design task. Composers are encouraged to address them explicitly and to take advantage of the practices of prototyping and iteration.},
address = {Ann Arbor, Michigan},
author = {Dahl, Luke},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Design,composition,instrument design,interaction design,laptop orchestra,mobile phone orchestra},
publisher = {University of Michigan},
title = {{Wicked Problems and Design Considerations in Composing for Laptop Orchestra}},
year = {2012}
}
@inproceedings{Donaldson2007,
address = {New York City, NY, United States},
author = {Donaldson, Justin and Knopke, Ian and Raphael, Chris},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {213--219},
title = {{Chroma Palette : Chromatic Maps of Sound As Granular Synthesis Interface}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}213.pdf},
year = {2007}
}
@inproceedings{Paine:2004,
address = {Hamamatsu, Japan},
author = {Paine, Garth},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {dynamic,dynamic morphology,gesture,interaction,mapping,mind,music,orchestration,spectral morphology},
pages = {80--86},
title = {{Gesture and Musical Interaction : Interactive Engagement Through Dynamic Morphology}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}080.pdf},
year = {2004}
}
@inproceedings{Young2003,
address = {Montreal, QC, Canada},
author = {Young, Diana and Essl, Georg},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {9--14},
title = {{HyperPuja: A Tibetan Singing Bowl Controller}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}009.pdf},
year = {2003}
}
@inproceedings{Ogawa2009,
address = {Pittsburgh, PA, United States},
author = {Ogawa, Keisuke and Kuhara, Yasuo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {as,as blinkers and gliders,cell pattern,cellular automata,consequently,conway,interactive composition,moving cell patterns such,nime09,patterns in the matrix,performance,s game of life,scale,we used small cell},
pages = {50--51},
title = {{Life Game Orchestra as an Interactive Music Composition System Translating Cellular Patterns of Automata into Musical Scales}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}050.pdf},
year = {2009}
}
@inproceedings{Goto2006,
address = {Paris, France},
author = {Goto, Suguru},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {artificial,gesture controller,humanoid robot,intelligence,interaction,robot},
pages = {292--295},
title = {{The Case Study of An Application of The System, BodySuit and RoboticMusic: Its Introduction and Aesthetics}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}292.pdf},
year = {2006}
}
@inproceedings{Bianchi2009,
abstract = {It has been shown that collaborative musical interfaces encourage novice users to explore the sound space and promote their participation as music performers. Nevertheless, such interfaces are generally physically situated and can limit the possibility of movements on the stage, a critical factor in live music performance. In this paper we introduce the Drummer, a networked digital musical interface that allows multiple performers to design and play drum kits simultaneously while, at the same time, keeping their ability to freely move on the stage. The system consists of multiple Nintendo DS clients with an intuitive, user-configurable interface and a server computer which plays drum sounds. The Drummer Machine, a small piece of hardware to augment the performance of the Drummer, is also introduced. },
address = {Pittsburgh, PA, United States},
author = {Bianchi, Andrea and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {2,collaborative interface,expression,figure 1,game control,motivations and related work,multiplayer,musical,musical control,nime09,nintendo ds,on a nintendo ds,the drummer client running},
pages = {316--319},
title = {{The Drummer : a Collaborative Musical Interface with Mobility}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}316.pdf},
year = {2009}
}
@inproceedings{Marchini2011,
abstract = {This paper introduces and evaluates a novel methodologyfor the estimation of bow pressing force in violin performance, aiming at a reduced intrusiveness while maintaininghigh accuracy. The technique is based on using a simplifiedphysical model of the hair ribbon deflection, and feeding thismodel solely with position and orientation measurements ofthe bow and violin spatial coordinates. The physical modelis both calibrated and evaluated using real force data acquired by means of a load cell.},
address = {Oslo, Norway},
author = {Marchini, Marco and Papiotis, Panos and P{\'{e}}rez, Alfonso and Maestre, Esteban},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {6DOF,bow force,bow pressing force,bow simplified physical model,force,hair ribbon ends,pressing force,string ends,violin playing},
pages = {481--486},
title = {{A Hair Ribbon Deflection Model for Low-intrusiveness Measurement of Bow Force in Violin Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}481.pdf},
year = {2011}
}
@inproceedings{Serafin2004,
address = {Hamamatsu, Japan},
author = {Serafin, Stefania and Young, Diana},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
pages = {108--111},
title = {{Toward a Generalized Friction Controller: from the Bowed String to Unusual Musical Instruments}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}108.pdf},
year = {2004}
}
@inproceedings{Goto2009,
address = {Pittsburgh, PA, United States},
author = {Goto, Suguru},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {328},
title = {{BodyJack}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}328.pdf},
year = {2009}
}
@inproceedings{Tsai2010,
address = {Sydney, Australia},
author = {Tsai, Chih-Chieh and Liu, Cha-Lin and Chang, Teng-Wen},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {ambient environment,interactive performance,music performance,nime10,responsive,skin},
number = {Nime},
pages = {399--402},
title = {{An Interactive Responsive Skin for Music}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}399.pdf},
year = {2010}
}
@inproceedings{Gold2011,
address = {Oslo, Norway},
author = {Gold, Nicolas E and Dannenberg, Roger B},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {live performance,popular music,software design},
number = {June},
pages = {36--39},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{A Reference Architecture and Score Representation for Popular Music Human-Computer Music Performance Systems}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}036.pdf},
year = {2011}
}
@inproceedings{Nam:2013,
abstract = {This paper describes the Musical Poi (mPoi), a unique sensor-based musicalinstrument rooted in the ancient art of poi spinning. The trajectory ofcircular motion drawn by the performance and the momentum of the mPoiinstrument are converted to the energetic and vibrant sound, which makesspiritual and meditative soundscape that opens everyone up the aura and clearsthe thought forms away. The mPoi project and its concepts will be introducedfirst and then its interaction with a performer will be discussed.The mPoi project seeks to develop a prototype for a set of mobile musicalinstrument based on electronic motion sensors and circuit boards. Thistechnology is installed in egg-shaped structure and allows communicationbetween a performer and the mPoi instrument. The principal motivation for themPoi project has been a desire to develop an extensible interface that willsupport the Poi performance, which is a style of performance art originatedwith the Maori people of New Zealand involving swinging tethered weightsthrough a variety of rhythmical and geometric patterns. As an extension of the body and the expansion of the movement, the mPoiutilizes the creative performance of Poi to make spatial and spiritual soundand music. The aims of the mPoi project are:to create a prototype of mPoi instrument that includes circuit board thatconnects the instrument to a sensor.to develop a software, which includes programming of the circuit board and forthe sound generation.to make a new artistic expression to refine the captured sound into artisticmusical notes. The creative part of the project is to design a unique method to translate theperformer's gesture into sound. A unique algorithm was developed to extractfeatures of the swing motion and translate them into various patterns of sound.},
address = {Daejeon, Republic of Korea},
author = {Nam, Sangbong},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Jwibulnori,Musical Poi,Poi,mPoi,sensor-based musical instrument},
pages = {148--151},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Musical Poi (mPoi)}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Keating2007,
address = {New York City, NY, United States},
author = {Keating, Noah H},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {audiovisual play,immersive dance,kinetic games,movement rich game play,nime07,responsive environments,smart floor},
pages = {338--343},
title = {{The Lambent Reactive : An Audiovisual Environment for Kinesthetic Playforms}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}338.pdf},
year = {2007}
}
@inproceedings{Jorda2003,
address = {Montreal, QC, Canada},
author = {Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {70--76},
title = {{Sonigraphical Instruments: From FMOL to the reacTable*}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}070.pdf},
year = {2003}
}
@inproceedings{Kiefer2009,
address = {Pittsburgh, PA, United States},
author = {Kiefer, Chris and Collins, Nick and Fitzpatrick, Geraldine},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {246--249},
title = {{Phalanger : Controlling Music Software With Hand Movement Using A Computer Vision and Machine Learning Approach}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}246.pdf},
year = {2009}
}
@inproceedings{Hashida2004,
address = {Hamamatsu, Japan},
author = {Hashida, Tomoko and Kakehi, Yasuaki and Naemura, Takeshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {contrapuntal music,human,improvisational ensemble play,spatially augmented reality,traces,tracking},
pages = {215--216},
title = {{Ensemble System with i-trace}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}215.pdf},
year = {2004}
}
@inproceedings{nime2014:nklugel1,
abstract = {In this contribution, we will discuss a prototype that allows a group of users to design sound collaboratively in real time using a multi-touch tabletop. We make use of a machine learning method to generate a mapping from perceptual audio features to synthesis parameters. This mapping is then used for visualization and interaction. Finally, we discuss the results of a comparative evaluation study.},
address = {London, United Kingdom},
author = {Kl{\"{u}}gel, Niklas and Becker, Timo and Groh, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {327--330},
publisher = {Goldsmiths, University of London},
title = {{Designing Sound Collaboratively - Perceptually Motivated Audio Synthesis}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}339.pdf},
year = {2014}
}
@inproceedings{Camurri2007a,
abstract = {EyesWeb XMI (for eXtended Multimodal Interaction) is the new version of the well-known EyesWeb platform. It has a main focus on multimodality and the main design target of this new release has been to improve the ability to process and correlate several streams of data. It has been used extensively to build a set of interactive systems for performing arts applications for Festival della Scienza 2006, Genoa, Italy. The purpose of this paper is to describe the developed installations as well as the new EyesWeb features that helped in their development.},
address = {New York City, NY, United States},
author = {Camurri, Antonio and Coletta, Paolo and Varni, Giovanna and Ghisio, Simone},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {EyesWeb,multimodal interactive systems,performing arts.},
pages = {305--308},
title = {{Developing Multimodal Interactive Systems with EyesWeb XMI}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}305.pdf},
year = {2007}
}
@inproceedings{Diakopoulos2011,
address = {Oslo, Norway},
author = {Diakopoulos, Dimitri and Kapur, Ajay},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {arduino,controllers,hci,hid,microcontrollers,midi,usb},
number = {June},
pages = {405--408},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{HIDUINO : A firmware for building driverless USB-MIDI devices using the Arduino microcontroller}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}405.pdf},
year = {2011}
}
@inproceedings{Flety:2004,
abstract = {This paper reports our recent developments on sensor acquisition systems, taking advantage of computer network technology. We present a versatile hardware system which can be connected to wireless modules, Analog to Digital Converters, and enables Ethernet communication. We are planning to make freely available the design of this architecture. We describe also several approaches we tested for wireless communication. Such technology developments are currently used in our newly formed Performance Arts Technology Group. },
address = {Hamamatsu, Japan},
author = {Fl{\'{e}}ty, Emmanuel and Leroy, Nicolas and Ravarini, Jean-Christophe and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Fl{\'{e}}ty et al. - 2004 - Versatile Sensor Acquisition System Utilizing Network Technology.pdf:pdf},
keywords = {802.11,Computer Music.,Ethernet,Gesture,Sensors},
pages = {157--160},
title = {{Versatile Sensor Acquisition System Utilizing Network Technology}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}157.pdf},
year = {2004}
}
@inproceedings{Suzuki2008,
address = {Genoa, Italy},
author = {Suzuki, Kenji and Kyoya, Miho and Kamatani, Takahiro and Uchiyama, Toshiaki},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {embodied sound media,hyper-instrument,laser beams,nime08},
pages = {360--361},
title = {{beacon : Embodied Sound Media Environment for Socio-Musical Interaction}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}360.pdf},
year = {2008}
}
@inproceedings{Demey2008,
address = {Genoa, Italy},
author = {Demey, Michiel and Leman, Marc and Bossuyt, Frederick and Vanfleteren, Jan},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {embodied music cognition,movement,music and,nime08,social interaction,tempo perception,wireless sensors},
pages = {372--373},
title = {{The Musical Synchrotron : Using Wireless Motion Sensors to Study How Social Interaction Affects Synchronization with Musical Tempo}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}372.pdf},
year = {2008}
}
@inproceedings{Schroeder2011,
address = {Oslo, Norway},
author = {Schroeder, Benjamin and Ainger, Marc and Parent, Richard},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {a human performer,agents,agents smoothly changing the,behavioral animation,figure 1,length of,physically based sound,pro-,strings being played by},
number = {April 2005},
pages = {120--123},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{A Physically Based Sound Space for Procedural Agents}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}120.pdf},
year = {2011}
}
@inproceedings{Garcia2011a,
abstract = {We conducted three studies with contemporary music composers at IRCAM. We found that even highly computer-literate composers use an iterative process that begins with expressing musical ideas on paper, followed by active parallel exploration on paper and in software, prior to final execution of their ideas as an original score. We conducted a participatory design study that focused on the creative exploration phase, to design tools that help composers better integrate their paper-based and electronic activities. We then developed InkSplorer as a technology probe that connects users' hand-written gestures on paper to Max/MSP and OpenMusic. Composers appropriated InkSplorer according to their preferred composition styles, emphasizing its ability to help them quickly explore musical ideas on paper as they interact with the computer. We conclude with recommendations for designing interactive paper tools that support the creative process, letting users explore musical ideas both on paper and electronically. },
address = {Oslo, Norway},
author = {Garcia, J{\'{e}}r{\'{e}}mie and Tsandilas, Theophanis and Agon, Carlos and Mackay, Wendy E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Composer,Creativity,Design Exploration,InkSplorer,Interactive Paper,OpenMusic,Technology Probes.},
pages = {361--366},
title = {{InkSplorer : Exploring Musical Ideas on Paper and Computer}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}361.pdf},
year = {2011}
}
@inproceedings{Kallblad2008,
address = {Genoa, Italy},
author = {K{\"{a}}llblad, Anna and Friberg, Anders and Svensson, Karl and Edelholm, Elisabet S},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {nime08},
pages = {128--133},
title = {{Hoppsa Universum  An Interactive Dance Installation for Children}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}128.pdf},
year = {2008}
}
@inproceedings{Beyer2011,
address = {Oslo, Norway},
author = {Beyer, Gilbert and Meier, Max},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {algorithmic composition,home media,interactive music,out-of-,public displays,soft constraints,user experience},
number = {June},
pages = {507--510},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Music Interfaces for Novice Users : Composing Music on a Public Display with Hand Gestures}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}507.pdf},
year = {2011}
}
@inproceedings{DeJong2011,
address = {Oslo, Norway},
author = {de Jong, Staas},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
keywords = {and others,and today granular,barry truax,curtis roads,granular sound synthesis,instrumental control,tangible display,tangible manipulation},
number = {June},
pages = {326--328},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Making Grains Tangible: Microtouch for Microsound}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}326.pdf},
year = {2011}
}
@inproceedings{Nagashima2004,
address = {Hamamatsu, Japan},
author = {Nagashima, Yoichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
pages = {173--176},
title = {{Measurement of Latency in Interactive Multimedia Art}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}173.pdf},
year = {2004}
}
@inproceedings{Dolphin2009a,
abstract = {This paper reports on work in progress on the creativeproject MagNular, part of a wider practical study of thepotential collaborative compositional applications of gameengine technologies. MagNular is a sound toy utilizingcomputer game and physics engine technologies to createan animated interface used in conjunction with an externalsound engine developed within Max/MSP. The playercontrols virtual magnets that attract or repel numerousparticle objects, moving them freely around the virtualspace. Particle object collision data is mapped to controlsound onsets and synthesis/DSP (Digital SignalProcessing) parameters. The user "composes" bycontrolling and influencing the simulated physicalbehaviors of the particle objects within the animatedinterface.},
address = {Pittsburgh, PA, United States},
author = {Dolphin, Andy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Animated Interfaces,Game Engines,Max/MSP.,Open Work,Sound Toys},
pages = {159--160},
title = {{MagNular : Symbolic Control of an External Sound Engine Using an Animated Interface}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}159.pdf},
year = {2009}
}
@inproceedings{Skogstad2010,
address = {Sydney, Australia},
author = {Skogstad, St{\aa}le A and Jensenius, Alexander R and Nymoen, Kristian},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {nime10},
number = {Nime},
pages = {407--410},
title = {{Using IR Optical Marker Based Motion Capture for Exploring Musical Interaction}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}407.pdf},
year = {2010}
}
@inproceedings{HensonJamie2012,
abstract = {This paper introduces the concept of Kugelschwung, a dig- ital musical instrument centrally based around the use of pendulums and lasers to create unique and highly inter- active electronic ambient soundscapes. Here, we explore the underlying design and physical construction of the in- strument, as well as its implementation and feasibility as an instrument in the real world. To conclude, we outline potential expansions to the instrument, describing how its range of applications can be extended to accommodate a variety of musical styles.},
address = {Ann Arbor, Michigan},
author = {{Henson Jamie} and {Collins Benjamin} and {Giles Alexander} and {Webb Kathryn} and {Livingston Matthew} and {Mortensson Thomas}},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {{Essl Georg} and {Gillespie Brent} and {Gurevich Michael} and {O'Modhrain Sile}},
keywords = {electronic,expressive performance,instrument design,laser,pendulums,sampler,soundscape},
organization = {University of Bristol},
pages = {4},
publisher = {Division of Computer Science {\&} Engineering, University of Michigan},
series = {NIME12},
title = {{Kugelschwung - a Pendulum-based Musical Instrument}},
url = {http://www.eecs.umich.edu/nime2012/Proceedings/papers/131{\_}Final{\_}Manuscript.pdf},
year = {2012}
}
@inproceedings{Cook2005,
abstract = {A wide variety of singing synthesis models and methods exist,but there are remarkably few real-time controllers for thesemodels. This paper describes a variety of devices developedover the last few years for controlling singing synthesismodels implemented in the Synthesis Toolkit in C++ (STK),Max/MSP, and ChucK. All of the controllers share somecommon features, such as air-pressure sensing for breathingand/or loudness control, means to control pitch, and methodsfor selecting and blending phonemes, diphones, and words.However, the form factors, sensors, mappings, and algorithmsvary greatly between the different controllers.},
address = {Vancouver, BC, Canada},
author = {Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Singing synthesis,real-time singing synthesis control.},
pages = {236--237},
title = {{Real-Time Performance Controllers for Synthesized Singing}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}236.pdf},
year = {2005}
}
@inproceedings{jo:2013,
abstract = {In this paper, we present a method to produce analog records with standardvector graphics software (i.e. Adobe Illustrator) and two different types ofcutting machines: laser cutter, and paper cutter. The method enables us toengrave wave forms on a surface of diverse materials such as paper, wood,acrylic, and leather without or with prior acoustic information (i.e. digitalaudio data). The results could be played with standard record players. Wepresent the method with its technical specification and explain our initialtrials with two performances and a workshop. The work examines the role ofmusical reproduction in the age of personal fabrication. ---p.s. If it's possible, we also would like to submit the work for performanceand workshop.A video of performance {\textless} it contains information on the authorshttp://www.youtube.com/watch?v=vbCLe06P7j0},
address = {Daejeon, Republic of Korea},
author = {Kazuhiro jo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Analog Record,Media Archaeology,Personal Fabrication},
pages = {283--286},
publisher = {Graduate School of Culture Technology, KAIST},
title = {cutting record - a record without (or with) prior acoustic information},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Lee:2012c,
abstract = {This research aims to improve the correspondence between music and dance, and explores the use of human respiration pattern for musical applications with focus on the motional aspect of breathing. While respiration is frequently considered as an indicator of the metabolic state of human body that contains meaningful information for medicine or psychology, motional aspect of respiration has been relatively unnoticed in spite of its strong correlation with muscles and the brain.
This paper introduces an interactive system to control music playback for dance performances based on the respiration pattern of the dancer. A wireless wearable sensor device detects the dancer's respiration, which is then utilized to modify the dynamic of music. Two different respiration-dynamic mappings were designed and evaluated through public performances and private tests by professional choreographers. Results from this research suggest a new conceptual approach to musical applications of respiration based on the technical characteristics of music and dance.},
address = {Ann Arbor, Michigan},
author = {Lee, Jeong-seob and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Music,correspondence,dance,interactive performance,respiration,wireless interface},
month = {may},
publisher = {University of Michigan},
title = {{Real-time Modification of Music with Dancer's Respiration Pattern}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}309.pdf},
year = {2012}
}
@inproceedings{nime2014:mneupert,
abstract = {We present an instrument for audio-visual performance that allows to recombine sounds from a collection of sampled media through concatenative synthesis. A three-dimensional distribution derived from feature-analysis becomes accessible through a theremin-inspired interface, allowing the player to shift from exploration and intuitive navigation toward embodied performance on a granular level. In our example we illustrate this concept by using the audiovisual recording of an instrumental performance as a source. Our system provides an alternative interface to the musical instrument's audiovisual corpus: as the instrument's sound and behavior is accessed in ways that are not possible on the instrument itself, the resulting non-linear playback of the grains generates an instant remix in a cut-up aesthetic. The presented instrument is a human-computer interface that employs the structural outcome of machine analysis accessing audiovisual corpora in the context of a musical performance.},
address = {London, United Kingdom},
author = {Go{\ss}mann, Joachim and Neupert, Max},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {151--154},
publisher = {Goldsmiths, University of London},
title = {{Musical Interface to Audiovisual Corpora of Arbitrary Instruments}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}296.pdf},
year = {2014}
}
@inproceedings{Mills2010a,
address = {Sydney, Australia},
author = {Mills, Roger},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {improvisation,internet audio,networked collaboration,nime10,sound},
number = {Nime},
pages = {186--191},
title = {{Dislocated Sound : A Survey of Improvisation in Networked Audio Platforms}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}186.pdf},
year = {2010}
}
@inproceedings{Manousakis:2012,
abstract = {This paper presents the system and technology developed for the distributed, micro-telematic, interactive sound art installation, The Network Is A Blind Space. The piece uses sound to explore the physical yet invisible electromagnetic spaces created by Wireless Local Area Networks (WLANs). To this end, the author created a framework for indoor WiFi localization, providing a variety of control data for various types of `musical echolocation'. This data, generated mostly by visitors exploring the installation while holding WiFi-enabled devices, is used to convey the hidden properties of wireless networks as dynamic spaces through an artistic experience.},
address = {Ann Arbor, Michigan},
author = {Manousakis, Stelios},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Manousakis - 2012 - Network spaces as collaborative instruments WLAN trilateration for musical echolocation in sound art.pdf:pdf},
keywords = {Network music,Pure Data,RjDj,SuperCollider,WiFi,collaborative instrument,distributed music,echolocation,electromagnetic signals,interactivity,mapping,mobile music,site-specific,sound art installation,traceroute,trilateration},
publisher = {University of Michigan},
title = {{Network spaces as collaborative instruments: WLAN trilateration for musical echolocation in sound art}},
year = {2012}
}
@inproceedings{Tobise2013,
abstract = {In guitar performance, fingering is an important factor, and complicated. In particular, the fingering of the left hand comprises various relationshipsbetween the finger and the string, such as a finger touching the strings, afinger pressing the strings, and a finger releasing the strings. The recognition of the precise fingering of the left hand is applied to aself-learning support system, which is able to detect strings being muted by afinger, and which transcribes music automatically, including the details offingering techniques. Therefore, the goal of our study is the construction of a system forrecognizing the touch of strings for the guitar. We propose a method for recognizing the touch of strings based on theconductive characteristics of strings and frets. We develop a prototype system, and evaluate its effectiveness.Furthermore, we propose an application which utilizes our system.},
address = {Daejeon, Republic of Korea},
author = {Tobise, Hayami and Takegawa, Yoshinari and Terada, Tsutomu and Tsukamoto, Masahiko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Fingering recognition,Guitar,Touched strings},
pages = {261--266},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Construction of a System for Recognizing Touch of Strings for Guitar}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{nime2014:ahindle1,
abstract = {One problem with live computer music performance is the transport of computers to a venue and the following setup of the computers used in playing and rendering music. The more computers involved the longer the setup and tear-down of a performance. Each computer adds power and cabling requirements that the venue must accommodate. Cloud computing can change of all this by simplifying the setup of many (10s, 100s) of machines at the click of a button. But there's a catch, the cloud is not physically near you, you cannot run an audio cable to the cloud. The audio from a computer music instrument in the cloud needs to streamed back to the performer and listeners. There are many solutions for streaming audio over networks and the internet, most of them suffer from high latency, heavy buffering, or proprietary/non-portable clients. In this paper we propose a portable cloud-friendly method of streaming, almost a cloud soundcard, whereby performers can use mobile devices (Android, iOS, laptops) to stream audio from the cloud with far lower latency than technologies like icecast. This technology enables near-realtime control over power computer music networks enabling performers to travel light and perform live with more computers than ever before.},
address = {London, United Kingdom},
author = {Hindle, Abram},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {277--280},
publisher = {Goldsmiths, University of London},
title = {{CloudOrch: A Portable SoundCard in the Cloud}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}541.pdf},
year = {2014}
}
@inproceedings{Schiesser2009,
address = {Pittsburgh, PA, United States},
author = {Schiesser, S{\'{e}}bastien},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {converter,gumstix,midi,nime09,open sound control},
pages = {165--168},
title = {{midOSC : a Gumstix-Based MIDI-to-OSC Converter}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}165.pdf},
year = {2009}
}
@inproceedings{Brent:2012,
abstract = {This paper introduces the Gesturally Extended Piano---an augmented instrument controller that relies on information drawn from performer motion tracking in order to control real-time audiovisual processing and synthesis. Specifically, the positions, heights, velocities, and relative distances and angles of points on the hands and forearms are followed. Technical details and installation of the tracking system are covered, as well as strategies for interpreting and mapping the resulting data in relation to synthesis parameters. Design factors surrounding mapping choices and the interrelation between mapped parameters are also considered.},
address = {Ann Arbor, Michigan},
author = {Brent, William},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {Augmented instruments,controllers,mapping,motion tracking},
publisher = {University of Michigan},
title = {{The Gesturally Extended Piano}},
year = {2012}
}
@inproceedings{Schmeder2009,
abstract = {An on-the-fly reconfigurable low-level embedded servicearchitecture is presented as a means to improve scalability, improve conceptual comprehensibility, reduce humanerror and reduce development time when designing newsensor-based electronic musical instruments with real-timeresponsiveness. The implementation of the concept ina project called micro-OSC is described. Other sensorinterfacing products are evaluated in the context of DIYprototyping of musical instruments. The capabilities ofthe micro-OSC platform are demonstrated through a set ofexamples including resistive sensing, mixed digital-analogsystems, many-channel sensor interfaces and time-basedmeasurement methods.},
address = {Pittsburgh, PA, United States},
author = {Schmeder, Andrew and Freed, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {DIY design,em- bedded web services,rapid prototyping,real-time musical interface,reconfigurable firmware},
pages = {121--124},
title = {{A Low-level Embedded Service Architecture for Rapid DIY Design of Real-time Musical Instruments}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}121.pdf},
year = {2009}
}
@inproceedings{Christopher:2013,
abstract = {This paper describes Kontrol, a new hand interface that extends the intuitivecontrol of electronic music to traditional instrumentalist and dancers. Thegoal of the authors has been to provide users with a device that is capable ofdetecting the highly intricate and expressive gestures of the master performer,in order for that information to be interpreted and used for control ofelectronic music. This paper discusses related devices, the architecture ofKontrol, it's potential as a gesture recognition device, and severalperformance applications.},
address = {Daejeon, Republic of Korea},
author = {Christopher, Kameron and He, Jingyin and Kapur, Raakhi and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Hand controller,Wekinator,computational ethnomusicology,conducting interface,dance interface,wearable sensors},
pages = {267--270},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Kontrol: Hand Gesture Recognition for Music and Dance Interaction}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Grohauser2010,
address = {Sydney, Australia},
author = {Gro{\ss}hauser, Tobias and Gro{\ss}ekath{\"{o}}fer, Ulf and Hermann, Thomas},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {left hand,nime10,ordered means models,pressure,sensor,strings},
number = {Nime},
pages = {271--276},
title = {{New Sensors and Pattern Recognition Techniques for String Instruments}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}271.pdf},
year = {2010}
}
@inproceedings{Choi:2013,
abstract = {The advent of Web Audio API in 2011 marked a significant advance for web-basedmusic systems by enabling real-time sound synthesis on web browsers simply bywriting JavaScript code. While this powerful functionality has arrived there isa yet unaddressed need for an extension to the API to fully reveal itspotential. To meet this need, a JavaScript library dubbed WAAX was created tofacilitate music and audio programming based on Web Audio API bypassingunderlying tasks and augmenting useful features. In this paper, we describecommon issues in web audio programming, illustrate how WAAX can speed up thedevelopment, and discuss future developments.},
address = {Daejeon, Republic of Korea},
author = {Choi, Hongchan and Berger, Jonathan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Chrome,JavaScript,Web Audio API,audience participation,collaborative music making,web-based music system},
pages = {499--502},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{WAAX: Web Audio API eXtension}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Smith:2012,
abstract = {Machine learning models are useful and attractive tools forthe interactive computer musician, enabling a breadth of interfaces and instruments. With current consumer hardwareit becomes possible to run advanced machine learning algorithms in demanding performance situations, yet expertiseremains a prominent entry barrier for most would-be users.Currently available implementations predominantly employsupervised machine learning techniques, while the adaptive,self-organizing capabilities of unsupervised models are notgenerally available. We present a free, new toolbox of unsupervised machine learning algorithms implemented in Max5 to support real-time interactive music and video, aimedat the non-expert computer artist.},
address = {Ann Arbor, Michigan},
author = {Smith, Benjamin D and Garnett, Guy E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Max 5,NIME,adaptive resonance theory,self-organizing maps,unsupervised machine learning},
publisher = {University of Michigan},
title = {{Unsupervised Play: Machine Learning Toolkit for Max}},
year = {2012}
}
@inproceedings{B2007,
abstract = {In this paper we describe the design and implementation ofthe PHYSMISM: an interface for exploring the possibilitiesfor improving the creative use of physical modelling soundsynthesis.The PHYSMISM is implemented in a software and hardware version. Moreover, four different physical modellingtechniques are implemented, to explore the implications ofusing and combining different techniques.In order to evaluate the creative use of physical models,a test was performed using 11 experienced musicians as testsubjects. Results show that the capability of combining thephysical models and the use of a physical interface engagedthe musicians in creative exploration of physical models.},
address = {New York City, NY, United States},
author = {Bottcher, Niels and Gelineck, Steven and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Physical models,excitation,hybrid instruments,resonator.},
pages = {31--36},
title = {{{\{}PHY{\}}SMISM : A Control Interface for Creative Exploration of Physical Models}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}031.pdf},
year = {2007}
}
@inproceedings{nime2014:cfaubel1,
abstract = {In the paper I present a robotic device that offers new ways of interaction for producing rhythmic patterns. The apparatus is placed on an overhead projector and a visual presentation of these rhythmic patterns is delivered as a shadow play. The rhythmic patterns can be manipulated by modifying the environment of the robot, through direct physical interaction with the robot, by rewiring the internal connectivity, and by adjusting internal parameters. The theory of embodied cognition provides the theoretical basis of this device. The core postulate of embodied cognition is that biological behavior can only be understood through an understanding of the real-time interactions of an organism's nervous system, the organism's body and the environment. One the one hand the device illustrates this theory because the patterns that are created equally depend on the real-time interactions of the electronics, the physical structure of the device and the environment. On the other hand the device presents a synthesis of these ideas and it is effectively possible to play with it at all the three levels, the electronics, the physical configuration of the robot and the environment.},
address = {London, United Kingdom},
author = {Faubel, Christian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {491--494},
publisher = {Goldsmiths, University of London},
title = {{Rhythm Apparatus on Overhead}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}503.pdf},
year = {2014}
}
@inproceedings{Berthaut:2013,
abstract = {Digital musical instruments bring new possibilities for musical performance.They are also more complex for the audience to understand, due to the diversityof their components and the magical aspect of the musicians' actions whencompared to acoustic instruments. This complexity results in a loss of livenessand possibly a poor experience for the audience. Our approach, called Rouages,is based on a mixed-reality display system and a 3D visualization application.It reveals the mechanisms of digital musical instruments by amplifyingmusicians' gestures with virtual extensions of the sensors, by representingthe sound components with 3D shapes and specific behaviors and by showing theimpact ofmusicians gestures on these components. We believe that Rouages opens up newperspectives to help instrument makers and musicians improve audienceexperience with their digital musical instruments.},
address = {Daejeon, Republic of Korea},
author = {Berthaut, Florent and Marshall, Mark T and Subramanian, Sriram and Hachet, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {3D interface,digital musical instruments,mappings,mixed-reality,rouages},
pages = {164--169},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Rouages: Revealing the Mechanisms of Digital Musical Instruments to the Audience}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Trail2012a,
abstract = {Hyper-instruments extend traditional acoustic instruments with sensing technologies that capture digitally subtle and sophisticated aspects of human performance. They leverage the long training and skills of performers while simultaneously providing rich possibilities for digital control. Many existing hyper-instruments suffer from being one of a kind instruments that require invasive modifications to the underlying acoustic instrument. In this paper we focus on the pitched percussion family and describe a non-invasive sensing approach for extending them to hyper-instruments. Our primary concern is to retain the technical integrity of the acoustic instrument and sound production methods while being able to intuitively interface the computer. This is accomplished by utilizing the Kinect sensor to track the position of the mallets without any modification to the instrument which enables easy and cheap replication of the pro-posed hyper-instrument extensions. In addition we describe two approaches to higher-level gesture control that remove the need for additional control devices such as foot pedals and fader boxes that are frequently used in electro-acoustic performance. This gesture control integrates more organically with the natural flow of playing the instrument providing user selectable control over filter parameters, synthesis, sampling, sequencing, and improvisation using a commer-cially available low-cost sensing apparatus.},
address = {Ann Arbor, Michigan},
author = {Trail, Shawn and Dean, Michael and Odowichuk, Gabrielle and Tavares, Tiago Fernandes and Driessen, Peter and Schloss, W Andrew and Tzanetakis, George},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
month = {may},
publisher = {University of Michigan},
title = {{Non-invasive sensing and gesture control for pitched percussion hyper-instruments using the Kinect}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}297.pdf},
year = {2012}
}
@inproceedings{Nishino2011,
address = {Oslo, Norway},
author = {Nishino, Hiroki},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {computer music,programming,programming language,the psychology of,usability},
number = {June},
pages = {499--502},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Cognitive Issues in Computer Music Programming}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}499.pdf},
year = {2011}
}
@inproceedings{Moriwaki2007,
abstract = {In this paper the ,
,
authors present the MIDI Scrapyard Challenge (MSC) workshop, a one-day hands-on experience which asks participants to create musical controllers out of cast-off electronics, found materials and junk. The workshop experience, principles, and considerations are detailed, along with sample projects which have been created in various MSC workshops. Observations and implications as well as future developments for the workshop are discussed. },
address = {New York City, NY, United States},
author = {Moriwaki, Katherine and Brucken-Cohen, Jonah},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {168--172},
title = {{MIDI Scrapyard Challenge Workshops}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}168.pdf},
year = {2007}
}
@inproceedings{Torre2009,
address = {Pittsburgh, PA, United States},
author = {Torre, Giuseppe and Sazdov, Robert and Konczewska, Dorota},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {330},
title = {{MOLITVA  Composition for Voice, Live Electronics, Pointing-At Glove Device and 3-D Setup of Speakers}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}330.pdf},
year = {2009}
}
@inproceedings{Mandelis2002,
address = {Dublin, Ireland},
author = {Mandelis, James},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {adaptive interfaces,artificial life,expressivity,hyperinstru-,live performance,ments,motion-to-sound mapping,selective breeding,sound meta-synthesis},
pages = {116--117},
title = {{Adaptive Hyperinstruments: Applying Evolutionary Techniques to Sound Synthesis and Performance}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}116.pdf},
year = {2002}
}
@inproceedings{Chang2007,
address = {New York City, NY, United States},
author = {Chang, Angela and Ishii, Hiroshi},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {a,and pulling,b,c,figure 1,grasping,involve,musical expressivity,nime07,scrunching and,tactile design,tangible interfaces,textiles,twisting,typical interactions with fabric},
pages = {46--49},
title = {{Zstretch : A Stretchy Fabric Music Controller}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}046.pdf},
year = {2007}
}
@inproceedings{Liu:2013,
abstract = {Cloud Bridge is an immersive interactive audiovisual software interface forboth data exploration and artistic creation. It explores how information can besonified and visualized to facilitate findings, and eventually becomeinteractive musical compositions. Cloud Bridge functions as a multi-user,multimodal instrument. The data represents the history of items checked out bypatrons of the Seattle Public Library. A single user or agroup of users functioning as a performance ensemble participate in the pieceby interactively querying the database using iOS devices. Each device isassociated with aunique timbre and color for contributing to the piece, whichappears on large shared screens and a surround-sound system for allparticipants and observers. Cloud Bridge leads to a new media interactiveinterface utilizing audio synthesis, visualization and real-time interaction.},
address = {Daejeon, Republic of Korea},
author = {Liu, Qian and Han, Yoon Chung and Kuchera-Morin, JoAnn and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Data Sonification,Data Visualization,Open Sound Control,Sonic Interaction Design,Sonification,User Interface},
pages = {431--436},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Cloud Bridge: a Data-driven Immersive Audio-Visual Software Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Tanaka:2012,
abstract = {Mobile devices represent a growing research field within NIME, and a growing area for commercial music software. They present unique design challenges and opportunities, which are yet to be fully explored and exploited. In this paper, we propose using a survey method combined with qualitative analysis to investigate the way in which people use mobiles musically. We subsequently present as an area of future research our own PDplayer, which provides a completely self contained end application in the mobile device, potentially making the mobile a more viable and expressive tool for musicians.},
address = {Ann Arbor, Michigan},
author = {Tanaka, Atau and Parkinson, Adam and Settel, Zack and Tahiroglu, Koray},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Mobile Music,NIME,Pure Data},
publisher = {University of Michigan},
title = {{A Survey and Thematic Analysis Approach as Input to the Design of Mobile Music GUIs}},
year = {2012}
}
@inproceedings{nime2014:emorgan,
abstract = {New technologies have led to the design of exciting interfaces for collaborative music making. However we still have very little understanding of the underlying affective and communicative processes which occur during such interactions. To address this issue, we carried out a pilot study where we collected continuous behavioural, physiological, and performance related measures from pairs of improvising drummers. This paper presents preliminary findings, which could be useful for the evaluation and design of user-centred collaborative interfaces for musical creativity and expression.},
address = {London, United Kingdom},
author = {Morgan, Evan and Gunes, Hatice and Bryan-Kinns, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {23--28},
publisher = {Goldsmiths, University of London},
title = {{Instrumenting the Interaction: Affective and Psychophysiological Features of Live Collaborative Musical Improvisation}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}353.pdf},
year = {2014}
}
@inproceedings{Fels2009,
address = {Pittsburgh, PA, United States},
author = {Fels, Sidney S and Pritchard, Bob and Lenters, Allison},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {274--275},
title = {{ForTouch : A Wearable Digital Ventriloquized Actor}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}274.pdf},
year = {2009}
}
@inproceedings{Ferris:2002,
abstract = {The Cardboard Box Garden (CBG) originated from a dissatisfaction with current computer technology as it is presented to children. This paper shall briefly review the process involved in the creation of this installation, from motivation through to design and subsequent implementation and user experience with the CBG. Through the augmentation of an everyday artefact, namely the standard cardboard box, a simple yet powerful interactive environment was created that has achieved its goal of stirring childrens imagination judging from the experience of our users. },
address = {Dublin},
author = {Ferris, Kieran and Bannon, Liam},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Education,assembly,augmented reality,cardboard box,disappearing computer,pervasive computing,play},
pages = {56--58},
title = {{The Musical Box Garden}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}056.pdf},
year = {2002}
}
@inproceedings{Peng2009,
address = {Pittsburgh, PA, United States},
author = {Peng, Lijuan and Gerhard, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {conducting,gesture,infrared,learning,nime09,wii},
pages = {155--156},
title = {{A Wii-Based Gestural Interface for Computer Conducting Systems}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}155.pdf},
year = {2009}
}
@inproceedings{Takegawa2007,
address = {New York City, NY, United States},
author = {Takegawa, Yoshinari and Terada, Tsutomu},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {additional black keys,diapason change,nime07,portable keyboard},
pages = {82--87},
title = {{Mobile Clavier : New Music Keyboard for Flexible Key Transpose}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}082.pdf},
year = {2007}
}
@inproceedings{Kimura2006,
abstract = {This is a description of a demonstration, regarding theuse of auditory illusions and psycho-acoustic phenomenonused in the interactive work of Jean-Claude Risset, writtenfor violinist Mari Kimura.},
address = {Paris, France},
author = {Kimura, Mari and Risset, Jean-Claude},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Kimura.,Risset,Violin,auditory illusions,psycho-acoustic phenomena,sig- nal processing,subharmonics},
pages = {407--408},
title = {{Auditory Illusion and Violin: Demonstration of a Work by Jean-Claude Risset Written for Mari Kimura}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}407.pdf},
year = {2006}
}
@inproceedings{DelleMonache2008,
address = {Genoa, Italy},
author = {{Delle Monache}, Stefano and Polotti, Pietro and Papetti, Stefano and Rocchesso, Davide},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {augmented everyday objects,interactivity,nime08,physics-based sound,rag-time washboard,sonification,sounding objects,synthesis},
pages = {154--157},
title = {{Sonically Augmented Found Objects}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}154.pdf},
year = {2008}
}
@inproceedings{Giordano:2012,
abstract = {Force-feedback devices can provide haptic feedback duringinteraction with physical models for sound synthesis. However, low-end devices may not always provide high-fidelitydisplay of the acoustic characteristics of the model. This article describes an enhanced handle for the Phantom Omnicontaining a vibration actuator intended to display the highfrequency portion of the synthesized forces. Measurementsare provided to show that this approach achieves a morefaithful representation of the acoustic signal, overcominglimitations in the device control and dynamics.},
address = {Ann Arbor, Michigan},
author = {Giordano, Marcello and Sinclair, Stephen and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {Haptics,audio,bowing,force feedback,interaction},
publisher = {University of Michigan},
title = {{Bowing a vibration-enhanced force feedback device}},
year = {2012}
}
@inproceedings{Ando2011,
address = {Oslo, Norway},
author = {Ando, Daichi},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {com-,interactive evolutionary computation,user-interface},
number = {June},
pages = {76--79},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Improving User-Interface of Interactive EC for Composition-Aid by means of Shopping Basket Procedure}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}076.pdf},
year = {2011}
}
@inproceedings{Marshall:2002,
abstract = {This paper introduces a subtle interface, which evolved from the design of an alternative gestural controller in the development of a performance interface. The conceptual idea used is based on that of the traditional Bodhran instrument, an Irish frame drum. The design process was user-centered and involved professional Bodhran players and through prototyping and user testing the resulting Vodhran emerged. },
address = {Dublin},
author = {Marshall, Mark T and Rath, Matthias and Moynihan, Breege},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Virtual instrument,gesture,sound modeling,user-centered design},
pages = {118--119},
title = {{The Virtual Bodhran -- The Vodhran}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}118.pdf},
year = {2002}
}
@inproceedings{Villar2007,
abstract = {This paper describes the design and implementation of a new interface prototype for live music mixing. The ColorDex system employs a completely new operational metaphor which allows the mix DJ to prepare up to six tracks at once, and perform mixes between up to three of those at a time. The basic premises of the design are: 1) Build a performance tool that multiplies the possible choices a DJ has in respect in how and when tracks are prepared and mixed; 2) Design the system in such a way that the tool does not overload the performer with unnecessary complexity, and 3) Make use of novel technology to make the performance of live music mixing more engaging for both the performer and the audience. The core components of the system are: A software program to load, visualize and playback digitally encoded tracks; the HDDJ device (built chiefly out of a repurposed hard disk drive), which provides tactile manipulation of the playback speed and position of tracks; and the Cubic Crossfader, a wireless sensor cube that controls of the volume of individual tracks, and allows the DJ to mix these in interesting ways.},
address = {New York City, NY, United States},
author = {Villar, Nicolas and Gellersen, Hans and Jervis, Matt and Lang, Alexander},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Novel interfaces,accelerometer-based cubic control,crossfading,cube-based interfaces,live music-mixing,repurposing HDDs},
pages = {264--269},
title = {{The ColorDex DJ System : A New Interface for Live Music Mixing}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}264.pdf},
year = {2007}
}
@inproceedings{Fraietta2008,
address = {Genoa, Italy},
author = {Fraietta, Angelo},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {a,data transmission protocols,gestural controllers,has been implemented as,midi,nime08,open sound control,osc},
pages = {19--23},
title = {{Open Sound Control : Constraints and Limitations}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}019.pdf},
year = {2008}
}
@inproceedings{Brown2010,
address = {Sydney, Australia},
author = {Brown, Andrew R},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {collaborative,ensemble,generative,interaction,network,nime10},
number = {Nime},
title = {{Network Jamming : Distributed Performance using Generative Music}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}283.pdf},
year = {2010}
}
@inproceedings{Bottoni2006,
address = {Paris, France},
author = {Bottoni, Paolo and Faralli, Stefano and Labella, Anna and Pierro, Mario},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Max/MSP,agent,mapping,planning},
pages = {322--325},
title = {{Mapping with Planning Agents in the Max/MSP Environment: the GO/Max Language}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}322.pdf},
year = {2006}
}
@inproceedings{McElligott2002,
address = {Dublin, Ireland},
author = {McElligott, Lisa and Dixon, Edward and Dillon, Michelle},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {3d sensing pressure,control device,effort,expression,force,gesture,input,intent,movement,music,resolu-,sensor,sound,tion,weight distribution},
pages = {126--130},
title = {{PegLegs in Music' - Processing the Effort Generated by Levels of Expressive Gesturing in Music}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}126.pdf},
year = {2002}
}
@inproceedings{Mccaig2002,
address = {Dublin, Ireland},
author = {Mccaig, Graeme and Fels, Sidney S},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {affective computing,ambient display,biosensor,communication,heart rate,interactive music,non-verbal},
pages = {120--125},
title = {{Playing on Heart-Strings: Experiences with the 2Hearts System}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}120.pdf},
year = {2002}
}
@inproceedings{Mcgee2011,
abstract = {BioRhythm is an interactive bio-feedback installation controlled by the cardiovascular system. Data from a photoplethysmograph (PPG) sensor controls sonification and visualization parameters in real-time. Biological signals areobtained using the techniques of Resonance Theory in Hemodynamics and mapped to audiovisual cues via the Five Element Philosophy. The result is a new media interface utilizing sound synthesis and spatialization with advanced graphics rendering. BioRhythm serves as an artistic explorationof the harmonic spectra of pulse waves.},
address = {Oslo, Norway},
author = {Mcgee, Ryan and Fan, Yuan-Yi and Ali, Reza},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {bio-feedback,bio-sensing,fm synthesis,open sound control,parallel computing,sonification,spa-,spatial audio,tialization,tion,visualiza-},
pages = {80--83},
title = {{BioRhythm : a Biologically-inspired Audio-Visual Installation}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}080.pdf},
year = {2011}
}
@inproceedings{Shiraiwa2003,
address = {Montreal, QC, Canada},
author = {Shiraiwa, Hiroko and Segnini, Rodrigo and Woo, Vivian},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {83--86},
title = {{Sound Kitchen: Designing a Chemically Controlled Musical Performance}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}083.pdf},
year = {2003}
}
@inproceedings{Loviscach2008,
address = {Genoa, Italy},
author = {Loviscach, J{\"{o}}rn},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {information visualization,intelligent,mutual information,nime08},
pages = {221--224},
title = {{Programming a Music Synthesizer through Data Mining}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}221.pdf},
year = {2008}
}
@inproceedings{Gelineck:2013,
abstract = {This paper presents the continuous work towards the development of an interfacefor music mixing targeted towards expert sound technicians and producers. Themixing interface uses a stage metaphor mapping scheme where audio channels arerepresented as digital widgets on a 2D surface. These can be controlled bymulti touch or by smart tangibles, which are tangible blocks with embeddedsensors. The smart tangibles developed for this interface are able to sense howthey are grasped by the user. The paper presents the design of the mixinginterface including the smart tangible as well as a preliminary user studyinvolving a hands-on focus group session where 5 different control technologiesare contrasted and discussed. Preliminary findings suggest that smart tangibleswere preferred, but that an optimal interface would include a combination oftouch, smart tangibles and an extra function control tangible for extending thefunctionality of the smart tangibles. Finally, the interface should incorporateboth an edit and mix mode - the latter displaying very limited visual feedbackin order to force users to focus their attention to listening instead of theinterface.},
address = {Daejeon, Republic of Korea},
author = {Gelineck, Steven and Overholt, Dan and B{\"{u}}chert, Morten and Andersen, Jesper},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {control surface,graspables,multi-touch,music mixing,physical-digital interface,sketching in hardware,smart objects,tangible user interface,tangibles,wireless sensing},
pages = {180--185},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Towards an Interface for Music Mixing based on Smart Tangibles and Multitouch}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Franco2005,
address = {Vancouver, BC, Canada},
author = {Franco, Ivan},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
keywords = {computer music,infrared sensing,music controller},
pages = {248--249},
title = {{The Airstick: A Free-Gesture Controller Using Infrared Sensing}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}248.pdf},
year = {2005}
}
@inproceedings{Tahiroglu:2013,
abstract = {This paper introduces a novel collaborative environment (PESI) in whichperformers are not only free to move and interact with each other but wheretheir social interactions contribute to the sonic outcome. PESI system isdesigned for co-located collaboration and provides embodied and spatialopportunities for musical exploration. To evaluate PESI with skilled musicians,a user-test jam session was conducted. Musicians' comments indicate that thesystem facilitates group interaction finely to bring up further intentions tomusical ideas. Results from our user-test jam session indicate that, through some modificationof the 'in-space' response to the improvisation, and through more intuitiveinteractions with the 'on-body' mobile instruments, we could make thecollaborative music activity a more engaging and active experience. Despitebeing only user-tested once with musicians, the group interview has raisedfruitful discussions on the precise details of the system components.Furthermore, the paradigms of musical interaction and social actions in groupactivities need to be questioned when we seek design requirements for such acollaborative environment. We introduced a system that we believe can open upnew ways of musical exploration in group music activity with a number ofmusicians. The system brings up the affordances of accessible technologieswhile creating opportunities for novel design applications to be explored. Ourresearch proposes further development of the system, focusing on movementbehavior in long-term interaction between performers. We plan to implement thisversion and evaluate design and implementation with distinct skilled musicians.},
address = {Daejeon, Republic of Korea},
author = {Tahiro$\backslash$u glu, Koray and Correia, Nuno N and Espada, Miguel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Affordances,NIME,collaboration,extended system,mobile music,social interaction},
pages = {35--40},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{PESI Extended System: In Space, On Body, with 3 Musicians}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Essl2010a,
abstract = {Impact force is an important dimension for percussive musical instruments such as the piano. We explore three possible mechanisms how to get impact forces on mobile multi-touch devices: using built-in accelerometers, the pressure sensing capability of Android phones, and external force sensing resistors. We find that accelerometers are difficult to control for this purpose. Android's pressure sensing shows some promise, especially when combined with augmented playing technique. Force sensing resistors can offer good dynamic resolution but this technology is not currently offered in commodity devices and proper coupling of the sensor with the applied impact is difficult. },
address = {Sydney, Australia},
author = {Essl, Georg and Rohs, Michael and Kratz, Sven},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Force,impact,mobile music making.,mobile phone,multi-touch,pressure},
pages = {182--185},
title = {{Use the Force (or something) - Pressure and Pressure - Like Input for Mobile Music Performance}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}182.pdf},
year = {2010}
}
@inproceedings{Choi2003,
address = {Montreal, QC, Canada},
author = {Choi, Insook},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {201--204},
title = {{A Component Model of Gestural Primitive Throughput}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}201.pdf},
year = {2003}
}
@inproceedings{You:2013,
abstract = {Remix{\_}Dance Music 3 is a four-channel quasi-fixed media piece that can beimprovised by a single player operating the Max/MSP-based controller on atablet such as iPad. Within the fixed time limit of six minutes, the performercan freely (de)activate and displace the eighty seven precomposed audio filesthat are simultaneously running, generating a sonic structure to one's likingout of the given network of musical possibilities. The interface is designed toinvite an integral musical structuring particularly in the dimensions ofperformatively underexplored (but still sonically viable) parameters that arelargely based on MPEG-7 audio descriptors.},
address = {Daejeon, Republic of Korea},
author = {You, Jaeseong and Wierenga, Red},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {MPEG-7,Novel controllers,interface for musical expression,music cognition,music perception,musical mapping strategy},
pages = {124--127},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Remix{\_}Dance 3: Improvisatory Sound Displacing on Touch Screen-Based Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Kobayashi2005,
address = {Vancouver, BC, Canada},
author = {Kobayashi, Shigeru and Masayuki, Akamasu},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {208--211},
title = {{Spinner: A Simple Approach to Reconfigurable User Interfaces}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}208.pdf},
year = {2005}
}
@inproceedings{Bortz:2013,
abstract = {Mountains and Valleys (an anonymous name for confidentiality) is a communal,site-specific installation that takes shape as a spatially-responsiveaudio-visual field. The public participates in the creation of theinstallation, resulting in shared ownership of the work between both theartists and participants. Furthermore, the installation takes new shape in eachrealization, both to incorporate the constraints and affordances of eachspecific site, as well as to address the lessons learned from the previousiteration. This paper describes the development and execution of Mountains andValleys over its most recent version, with an eye toward the next iteration ata prestigious art museum during a national festival in Washington, D.C.},
address = {Daejeon, Republic of Korea},
author = {Bortz, Brennon and Ishida, Aki and Bukvic, Ivica Ico and Knapp, R Benjamin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Japanese lanterns,Participatory creation,communal interaction,fields,interactive installation},
pages = {73--78},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Lantern Field: Exploring Participatory Design of a Communal, Spatially Responsive Installation}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Gurevich2010,
address = {Sydney, Australia},
author = {Gurevich, Michael and Stapleton, Paul and Marquez-Borbon, Adnan},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {design,interaction,nime10,performance,persuasive technology},
number = {Nime},
pages = {106--111},
title = {{Style and Constraint in Electronic Musical Instruments}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}106.pdf},
year = {2010}
}
@inproceedings{Martin2010,
abstract = {Many musical instruments have interfaces which emphasisethe pitch of the sound produced over other perceptual characteristics, such as its timbre. This is at odds with the musical developments of the last century. In this paper, weintroduce a method for replacing the interface of musicalinstruments (both conventional and unconventional) witha more flexible interface which can present the intrument'savailable sounds according to variety of different perceptualcharacteristics, such as their brightness or roughness. Weapply this method to an instrument of our own design whichcomprises an electro-mechanically controlled electric guitarand amplifier configured to produce feedback tones.},
address = {Sydney, Australia},
author = {Martin, Aengus and Ferguson, Sam and Beilharz, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Concatenative Synthesis,Feedback,Guitar},
pages = {364--367},
title = {{Mechanisms for Controlling Complex Sound Sources : Applications to Guitar Feedback Control}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}364.pdf},
year = {2010}
}
@inproceedings{Hatanaka2003,
address = {Montreal, QC, Canada},
author = {Hatanaka, Motohide},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {77--82},
title = {{Ergonomic Design of A Portable Musical Instrument}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}077.pdf},
year = {2003}
}
@inproceedings{Camurri2008,
address = {Genoa, Italy},
author = {Camurri, Antonio and Canepa, Corrado and Coletta, Paolo and Mazzarino, Barbara and Volpe, Gualtiero},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Active listening of music,collaborative environments,expressive interfaces,full-body motion analysis and expressive gesture,multimodal interactive systems for music and perf,social interaction.},
pages = {134--139},
title = {{Mappe per Affetti Erranti : a Multimodal System for Social Active Listening and Expressive Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}134.pdf},
year = {2008}
}
@inproceedings{Ng2002,
address = {Dublin, Ireland},
author = {Ng, Kia},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {gesture,interactive,motion,music,performance},
pages = {131--132},
title = {{Interactive Gesture Music Performance Interface}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}131.pdf},
year = {2002}
}
@inproceedings{Porres2007,
abstract = {This work presents an interactive device to control an adaptive tuning and synthesis system. The gestural controller is based on the theremin concept in which only an antenna is used as a proximity sensor. This interactive process is guided by sensorial consonance curves and adaptive tuning related to psychoacoustical studies. We used an algorithm to calculate the dissonance values according to amplitudes and frequencies of a given sound spectrum. The theoretical background is presented followed by interactive composition strategies and sound results. },
address = {New York City, NY, United States},
author = {Porres, Alexandre T and Manzolli, Jonatas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Interaction,adaptive tuning,sensorial dissonance,synthesis.,theremin},
pages = {363--366},
title = {{Adaptive Tuning Using Theremin as Gestural Controller}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}363.pdf},
year = {2007}
}
@inproceedings{Hayes2011,
address = {Oslo, Norway},
author = {Hayes, Lauren},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {augmented instruments,composition,digital,human-computer interfaces,real-time performance,vibrotactile feedback},
number = {June},
pages = {72--75},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Vibrotactile Feedback-Assisted Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}072.pdf},
year = {2011}
}
@inproceedings{McGee:2013,
abstract = {VOSIS is an interactive image sonification interface that creates complexwavetables by raster scanning greyscale image pixel data. Using a multi-touchscreen to play image regions of unique frequency content rather than a linearscale of frequencies, it becomes a unique performance tool for experimental andvisual music. A number of image filters controlled by multi-touch gestures addvariation to the sound palette. On a mobile device, parameters controlled bythe accelerometer add another layer expressivity to the resulting audio-visualmontages.},
address = {Daejeon, Republic of Korea},
author = {McGee, Ryan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {image sonification,multi-touch,visual music},
pages = {460--463},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{VOSIS: a Multi-touch Image Sonification Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Rodriguez2005,
address = {Vancouver, BC, Canada},
author = {Rodr{\'{i}}guez, David and Rodr{\'{i}}guez, Iv{\'{a}}n},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {252--253},
title = {{VIFE {\_} alpha v.01 Real-time Visual Sound Installation performed by Glove-Gesture}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}252.pdf},
year = {2005}
}
@inproceedings{nime2014:ptimothy,
abstract = {This paper describes the technologies, collaborative processes, and artistic intents of the musical composition Engravings for Prepared Snare Drum, iPad, and Computer, which was composed by Timothy Polashek for percussionist Brad Meyer using a jointly created electro- acoustic and interactive musical instrument. During performance, the percussionist equally manipulates and expresses through two surfaces, an iPad displaying an interactive touch screen and a snare drum augmented with various foreign objects, including a contact microphone adhered to the drumhead's surface. A computer program created for this composition runs on a laptop computer in front of the percussionist. The software captures sound from the contact microphone and transforms this sound through audio signal processing controlled by the performer's gestures on the iPad. The computer screen displays an animated graphic score, as well as the current states of iPad controls and audio signal processing, for the performer. Many compositional and technological approaches used in this project pay tribute to composer John Cage, since the premiere performance of Engravings for Prepared Snare Drum, iPad, and Computer took place in 2012, the centennial celebration of Cage's birth year.},
address = {London, United Kingdom},
author = {Polashek, Timothy and Meyer, Brad},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {82--83},
publisher = {Goldsmiths, University of London},
title = {{Engravings for Prepared Snare Drum, iPad, and Computer}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}254.pdf},
year = {2014}
}
@inproceedings{nime2014:tbarraclough,
abstract = {This paper describes the Modulome System, a new hard- ware interface set for group-based electronic music perfor- mance and installation. Taking influence from a variety of established interfaces, the Modulome is a modular controller with application dependant use cases.},
address = {London, United Kingdom},
author = {Barraclough, Timothy J and Murphy, Jim and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {155--158},
publisher = {Goldsmiths, University of London},
title = {{New Open-Source Interfaces for Group Based Participatory Performance of Live Electronic Music}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}309.pdf},
year = {2014}
}
@inproceedings{Paine2007,
address = {New York City, NY, United States},
author = {Paine, Garth and Stevenson, Ian and Pearce, Angela},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {evaluation,mapping,musical instrument design,musicianship,nime07},
pages = {70--77},
title = {{The Thummer Mapping Project (ThuMP)}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}070.pdf},
year = {2007}
}
@inproceedings{Knapp2011,
address = {Oslo, Norway},
author = {Knapp, Benjamin and Bortz, Brennon},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {affective computing,bile music performance,mo-,physiological signal measurement},
number = {June},
pages = {203--206},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{MobileMuse: Integral Music Control Goes Mobile}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}203.pdf},
year = {2011}
}
@inproceedings{Borchers2006,
address = {Paris, France},
author = {Borchers, Jan and Hadjakos, Aristotelis and M{\"{u}}hlh{\"{a}}user, Max},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {conducting,exhibit,music stand,score display},
pages = {254--259},
title = {{MICON A Music Stand for Interactive Conducting}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}254.pdf},
year = {2006}
}
@inproceedings{DeJong2008,
address = {Genoa, Italy},
author = {de Jong, Staas},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {nime08},
pages = {370--371},
title = {{The Cyclotactor : Towards a Tactile Platform for Musical Interaction}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}370.pdf},
year = {2008}
}
@inproceedings{Lippit2004,
address = {Hamamatsu, Japan},
author = {Lippit, Takuro M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {dj,max,microchip,midi,msp,pic microcontroller,realtime sampling,turntablism},
pages = {211--212},
title = {{Realtime Sampling System for the Turntablist, Version 2: 16padjoystickcontroller}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}211.pdf},
year = {2004}
}
@inproceedings{Martin2010a,
address = {Sydney, Australia},
author = {Martin, Charles and Forster, Benjamin and Cormick, Hanna},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {cross-artform performance,networked performance,nime10,physi-},
number = {Nime},
pages = {204--207},
title = {{Cross-Artform Performance Using Networked Interfaces : Last Man to Die's Vital LMTD}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}204.pdf},
year = {2010}
}
@inproceedings{Gurevich2007,
abstract = {We describe the prevailing model of musical expression, which assumes a binary formulation of "the text" and "the act," along with its implied roles of composer and performer. We argue that this model not only excludes some contemporary aesthetic values but also limits the communicative ability of new music interfaces. As an alternative, an ecology of musical creation accounts for both a diversity of aesthetic goals and the complex interrelation of human and non-human agents. An ecological perspective on several approaches to musical creation with interactive technologies reveals an expanded, more inclusive view of artistic interaction that facilitates novel, compelling ways to use technology for music. This paper is fundamentally a call to consider the role of aesthetic values in the analysis of artistic processes and technologies. },
address = {New York City, NY, United States},
author = {Gurevich, Michael and Trevi{\~{n}}o, Jeffrey},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Expression,aesthetic goal,communication,construct,discipline,discourse,emotion,evaluation,experience,expressivity,model,non-expressive,transparency},
pages = {106--111},
title = {{Expression and Its Discontents : Toward an Ecology of Musical Creation}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}106.pdf},
year = {2007}
}
@inproceedings{Lahdeoja2009,
abstract = {In this paper we describe an approach for introducing newelectronic percussive sound possibilities for stringinstruments by "listening" to the sounds of the instrument'sbody and extracting audio and data from the wood'sacoustic vibrations. A method for capturing, localizing andanalyzing the percussive hits on the instrument's body ispresented, in connection with an audio-driven electronicpercussive sound module. The system introduces a newgesture-sound relationship in the electric string instrumentplaying environment, namely the use of percussivetechniques on the instrument's body which are null inregular circumstances due to selective and exclusivemicrophone use for the strings. Instrument bodypercussions are widely used in the acoustic instrumentalpraxis. They yield a strong potential for providing anextended soundscape via instrument augmentation, directlycontrolled by the musician through haptic manipulation ofthe instrument itself. The research work was carried out onthe electric guitar, but the method used can apply to anystring instrument with a resonating body.},
address = {Pittsburgh, PA, United States},
author = {L{\"{a}}hdeoja, Otso},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {augmented instrument,chordophone,contact microphone systems,electric,electronic percussion,even with,guitar,leaving the instrument body,nime09,there is always a,trade-off,virtually mute},
pages = {102--105},
title = {{Augmenting Chordophones with Hybrid Percussive Sound Possibilities}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}102.pdf},
year = {2009}
}
@inproceedings{dAlessandro2007,
abstract = {In this paper, we present a new bi-manual gestural controller, called HandSketch, composed of purchasable devices :pen tablet and pressure-sensing surfaces. It aims at achieving real-time manipulation of several continuous and articulated aspects of pitched sounds synthesis, with a focus onexpressive voice. Both prefered and non-prefered hand issues are discussed. Concrete playing diagrams and mappingstrategies are described. These results are integrated and acompact controller is proposed.},
address = {New York City, NY, United States},
author = {D'Alessandro, Nicolas and Dutoit, Thierry},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {FSR,Pen tablet,bi-manual gestural control.},
pages = {78--81},
title = {{HandSketch Bi-Manual Controller Investigation on Expressive Control Issues of an Augmented Tablet}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}078.pdf},
year = {2007}
}
@inproceedings{Dolphin2009,
address = {Pittsburgh, PA, United States},
author = {Dolphin, Andy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {56--57},
title = {{SpiralSet : A Sound Toy Utilizing Game Engine Technologies}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}056.pdf},
year = {2009}
}
@inproceedings{Wang2006,
address = {Paris, France},
author = {Wang, Ge and Misra, Ananya and Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Graphical interfaces,collaborative performance,computer music ensemble,education.,emergence,networking,visualization},
pages = {49--52},
title = {{Building Collaborative Graphical interFaces in the Audicle}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}049.pdf},
year = {2006}
}
@inproceedings{Carrascal2011,
abstract = {Audio mixing is the adjustment of relative volumes, panning and other parameters corresponding to different soundsources, in order to create a technically and aestheticallyadequate sound sum. To do this, audio engineers employ"panpots" and faders, the standard controls in audio mixers. The design of such devices has remained practically unchanged for decades since their introduction. At the time,no usability studies seem to have been conducted on suchdevices, so one could question if they are really optimizedfor the task they are meant for.This paper proposes a new set of controls that might beused to simplify and/or improve the performance of audiomixing tasks, taking into account the spatial characteristicsof modern mixing technologies such as surround and 3Daudio and making use of multitouch interface technologies.A preliminary usability test has shown promising results.},
address = {Oslo, Norway},
author = {Carrascal, Juan P and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {audio mixing,control surface,multitouch,touchscreen},
pages = {100--103},
title = {{Multitouch Interface for Audio Mixing}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}100.pdf},
year = {2011}
}
@inproceedings{Schacher2008,
address = {Genoa, Italy},
author = {Schacher, Jan C},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {composition,electronic music,embedded linux,gps,interaction,location-based,mapping,nime08,pure data,soundscape},
pages = {168--171},
title = {{Davos Soundscape, a Location Based Interactive Composition}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}168.pdf},
year = {2008}
}
@inproceedings{Morris2007,
address = {New York City, NY, United States},
author = {Morris, Simon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {425},
title = {{Musique Concrete : Transforming Space , Sound and the City Through Skateboarding}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}425.pdf},
year = {2007}
}
@inproceedings{Deutscher2005,
address = {Vancouver, BC, Canada},
author = {Deutscher, Meghan and Fels, Sidney S and Hoskinson, Reynald and Takahashi, Sachiyo},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
pages = {274},
title = {{Echology}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}274.pdf},
year = {2005}
}
@inproceedings{Fyfe2011,
abstract = {JunctionBox is a new software toolkit for creating multitouch interfaces for controlling sound and music. Morespecifically, the toolkit has special features which make iteasy to create TUIO-based touch interfaces for controllingsound engines via Open Sound Control. Programmers using the toolkit have a great deal of freedom to create highlycustomized interfaces that work on a variety of hardware.},
address = {Oslo, Norway},
author = {Fyfe, Lawrence and Tindale, Adam and Carpendale, Sheelagh},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Multi-touch,Open Sound Control,TUIO,Toolkit},
pages = {276--279},
title = {{JunctionBox : A Toolkit for Creating Multi-touch Sound Control Interfaces}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}276.pdf},
year = {2011}
}
@inproceedings{Hansen2006,
address = {Paris, France},
author = {Hansen, Kjetil F and Bresin, Roberto},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {controllers,dj,instrument mapping,scratching,virtual},
pages = {188--191},
title = {{Mapping Strategies in DJ Scratching}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}188.pdf},
year = {2006}
}
@inproceedings{Ciglar2008,
address = {Genoa, Italy},
author = {Ciglar, Miha},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {1,dancer,fig,from the system in,gesture recognition,haptic feedback,in,markers attached to the,motion tracking,nime08,s limbs,the dancer receives feedback,two ways},
pages = {203--206},
title = {{"3rd. Pole"  A Composition Performed via Gestural Cues}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}203.pdf},
year = {2008}
}
@inproceedings{Gillian2011a,
abstract = {This paper presents the SARC EyesWeb Catalog, (SEC),a machine learning toolbox that has been specifically developed for musician-computer interaction. The SEC features a large number of machine learning algorithms that can be used in real-time to recognise static postures, perform regression and classify multivariate temporal gestures. The algorithms within the toolbox have been designed to work with any N -dimensional signal and can be quickly trained with a small number of training examples. We also provide the motivation for the algorithms used for the recognition of musical gestures to achieve a low intra-personal generalisation error, as opposed to the inter-personal generalisation error that is more common in other areas of human-computer interaction.},
address = {Oslo, Norway},
author = {Gillian, Nicholas and Knapp, Benjamin and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Machine learning,SEC,gesture recognition,musician-computer interaction},
pages = {343--348},
title = {{A Machine Learning Toolbox For Musician Computer Interaction}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}343.pdf},
year = {2011}
}
@inproceedings{Nishibori2006,
address = {Paris, France},
author = {Nishibori, Yu and Iwai, Toshio},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
pages = {172--175},
title = {{TENORI-ON}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}172.pdf},
year = {2006}
}
@inproceedings{Tanaka2010,
address = {Sydney, Australia},
author = {Tanaka, Atau},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {2,defining the instrument,instrument definition,mapping,mobile,multimodal interaction,musical affordance,nime,nime10},
number = {Nime},
pages = {15--18},
title = {{Mapping Out Instruments, Affordances, and Mobiles}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}088.pdf},
year = {2010}
}
@inproceedings{Hansen2006,
address = {Paris, France},
author = {Hansen, Kjetil F and Bresin, Roberto},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {controllers,dj,instrument mapping,scratching,virtual},
pages = {188--191},
title = {{Mapping Strategies in DJ Scratching}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}188.pdf},
year = {2006}
}
@inproceedings{Crawford2009,
address = {Pittsburgh, PA, United States},
author = {Crawford, Langdon and Fastenow, William D},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {149--150},
title = {{The Midi-AirGuitar , A serious Musical Controller with a Funny Name Music Technology Program}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}149.pdf},
year = {2009}
}
@inproceedings{Nixdorf2006,
address = {Paris, France},
author = {Nixdorf, Joshua J and Gerhard, David},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {gem,live systems,pd,performance sys-,real-time systems,sound architecture,sound localization,sound spatialization,surround sound,tems},
pages = {318--321},
title = {{Real-time Sound Source Spatialization as Used in Challenging Bodies: Implementation and Performance}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}318.pdf},
year = {2006}
}
@inproceedings{Trappe2012,
abstract = {In this paper we present our project to make sound synthesis and music controller construction accessible to children in a technology design workshop. We present the work we have carried out to develop a graphical user interface, and give account of the workshop we conducted in collaboration with a local primary school. Our results indicate that the production of audio events by means of digital synthesis and algorithmic composition provides a rich and interesting field to be discovered for pedagogical workshops taking a Constructionist approach.},
address = {Ann Arbor, Michigan},
author = {Trappe, Christoph},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Child Computer Interaction,Constructionism,Human-Computer Interface Design,Interactive Audio Sys- tems,Mu- sic Composition and Generation,Sound and Music Computing,Technology Design Activities.},
publisher = {University of Michigan},
title = {{Making Sound Synthesis Accessible for Children}},
year = {2012}
}
@inproceedings{Mitchell2012,
abstract = {This paper presents a toolbox of gestural control mechanisms which are available when the input sensing apparatus is a pair of data gloves fitted with orientation sensors. The toolbox was developed in advance of a live music performance in which the mapping from gestural input to audio output was to be developed rapidly in collaboration with the performer. The paper begins with an introduction to the associated literature before introducing a range of continuous, discrete and combined control mechanisms, enabling a flexible range of mappings to be explored and modified easily. An application of the toolbox within a live music performance is then described with an evaluation of the system with ideas for future developments.},
address = {Ann Arbor, Michigan},
author = {Mitchell, Thomas and Madgwick, Sebastian and Heap, Imogen},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Computer Music,Data Gloves,Gestural Control},
month = {may},
publisher = {University of Michigan},
title = {{Musical Interaction with Hand Posture and Orientation: A Toolbox of Gestural Control Mechanisms}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}272.pdf},
year = {2012}
}
@inproceedings{Maruyama2010,
address = {Sydney, Australia},
author = {Maruyama, Yutaro and Takegawa, Yoshinari and Terada, Tsutomu and Tsukamoto, Masahiko},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {musical instruments,nime10,script language},
number = {Nime},
pages = {15--18},
title = {{UnitInstrument : Easy Configurable Musical Instruments}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}007.pdf},
year = {2010}
}
@inproceedings{Aylward2006,
address = {Paris, France},
author = {Aylward, Ryan and Paradiso, Joseph A},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Aylward, Paradiso - 2006 - Sensemble A Wireless, Compact, Multi-User Sensor System for Interactive Dance.pdf:pdf},
isbn = {1617452564},
keywords = {collective motion analysis,inertial gesture,interactive dance,multi-user interface,tracking,wearable sensor networks},
pages = {134--139},
title = {{Sensemble: A Wireless, Compact, Multi-User Sensor System for Interactive Dance}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}134.pdf},
year = {2006}
}
@article{Jorda2004a,
author = {Jord{\`{a}}, Sergi},
doi = {10.1080/0929821042000317886},
file = {::},
issn = {0929-8215},
journal = {Journal of New Music Research},
month = {sep},
number = {3},
pages = {321--341},
title = {{Instruments and Players: Some Thoughts on Digital Lutherie}},
url = {http://www.tandfonline.com/doi/abs/10.1080/0929821042000317886},
volume = {33},
year = {2004}
}
@inproceedings{Fraietta2008,
abstract = {Open Sound Control (OSC) is being used successfully as amessaging protocol among many computers, gesturalcontrollers and multimedia systems. Although OSC hasaddressed some of the shortcomings of MIDI, OSC cannotdeliver on its promises as a real-time communication protocolfor constrained embedded systems. This paper will examinesome of the advantages but also dispel some of the mythsconcerning OSC. The paper will also describe how some of thebest features of OSC can be used to develop a lightweightprotocol that is microcontroller friendly.},
address = {Genoa, Italy},
author = {Fraietta, Angelo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {a,data transmission protocols,gestural controllers,has been implemented as,midi,nime08,open sound control,osc},
pages = {19--23},
title = {{Open Sound Control : Constraints and Limitations}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}019.pdf},
year = {2008}
}
@inproceedings{Kim2011a,
abstract = {In this paper, we introduce a pipe interface that recognizestouch on tone holes by the resonances in the pipe instead ofa touch sensor. This work was based on the acoustic principles of woodwind instruments without complex sensors andelectronic circuits to develop a simple and durable interface.The measured signals were analyzed to show that differentfingerings generate various sounds. The audible resonancesignal in the pipe interface can be used as a sonic event formusical expression by itself and also as an input parameterfor mapping different sounds.},
address = {Oslo, Norway},
author = {Kim, Seunghun and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {mapping,pipe,resonance},
pages = {217--219},
title = {{Musical Control of a Pipe Based on Acoustic Resonance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}217.pdf},
year = {2011}
}
@inproceedings{Huott2005,
address = {Vancouver, BC, Canada},
author = {Huott, Robert},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {244--245},
title = {{Precise Control on Compound Curves}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}244.pdf},
year = {2005}
}
@inproceedings{Porres2007,
address = {New York City, NY, United States},
author = {Porres, Alexandre T and Manzolli, Jonatas},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {adaptive tuning,interaction,nime07,sensorial dissonance,theremin},
pages = {363--366},
title = {{Adaptive Tuning Using Theremin as Gestural Controller}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}363.pdf},
year = {2007}
}
@inproceedings{Taylor2010,
address = {Sydney, Australia},
author = {Taylor, Robyn and Schofield, Guy and Shearer, John and Boulanger, Pierre and Wallace, Jayne and Olivier, Patrick},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {busking,collaborative interface,creative practice,experience centered design,frustrated total internal reflection,ftir,multi-touch screen,multimedia,nime10,participatory performance},
number = {June},
pages = {15--18},
title = {{humanaquarium : A Participatory Performance System}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}440.pdf},
year = {2010}
}
@inproceedings{Everett:2013,
abstract = {This presentation-demonstration discusses the creation of FIRST LIFE, a75-minute mixed media performance for string quartet, live audio processing,live motion capture video, and audience participation utilizing stochasticmodels of chemical data provided by Martha Grover's Research Group at theSchool of Chemical and Biomolecular Engineering at Georgia Institute ofTechnology. Each section of this work is constructed from contingent outcomesdrawn from biochemical research exploring possible early Earth formations oforganic compounds. Audio-video excerpts of the composition will be played during the presentation.Max patches for sonification and for generating stochastic processes will bedemonstrated as well.},
address = {Daejeon, Republic of Korea},
author = {Everett, Steve},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Data-driven composition,live electronics-video,sonification},
pages = {277--278},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Sonifying Chemical Evolution}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Corness2007,
address = {New York City, NY, United States},
author = {Seo, Jinsil and Corness, Greg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {431},
title = {{nite{\_}aura : An Audio-Visual Interactive Immersive Installation}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}431.pdf},
year = {2007}
}
@inproceedings{Lieberman2009,
address = {Pittsburgh, PA, United States},
author = {Lieberman, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {321},
title = {{Anigraphical Etude 9}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}321.pdf},
year = {2009}
}
@inproceedings{Favilla2008,
address = {Genoa, Italy},
author = {Favilla, Stuart and Cannon, Joanne and Hicks, Tony and Chant, Dale and Favilla, Paris},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {interfaces or extend their,joined by a number,nime08,of new musicians in,order to develop new,own acoustic instruments with,sensors,sound interfaces and software,the project has been},
pages = {366--369},
title = {{Gluisax : Bent Leather Band's Augmented Saxophone Project}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}366.pdf},
year = {2008}
}
@inproceedings{Collins2010a,
abstract = {Gaining access to a prototype motion capture suit designedby the Animazoo company, the Interactive Systems groupat the University of Sussex have been investigating application areas. This paper describes our initial experimentsin mapping the suit control data to sonic attributes for musical purposes. Given the lab conditions under which weworked, an agile design cycle methodology was employed,with live coding of audio software incorporating fast feedback, and more reflective preparations between sessions, exploiting both individual and pair programming. As the suitprovides up to 66 channels of information, we confront achallenging mapping problem, and techniques are describedfor automatic calibration, and the use of echo state networksfor dimensionality reduction.},
address = {Sydney, Australia},
author = {Collins, Nick and Kiefer, Chris and Patoli, Zeeshan and White, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Agile De- sign,Mapping,Motion Capture,Musical Controller},
pages = {455--458},
title = {{Musical Exoskeletons : Experiments with a Motion Capture Suit}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}455.pdf},
year = {2010}
}
@inproceedings{nime2014:ihattwick1,
abstract = {This paper presents results from the development of a digital musical instrument which uses audio feature extraction for the control of sound synthesis. Our implementation utilizes multi-band audio analysis to generate control signals. This technique is well-suited to instruments for which the gestural interface is intentionally weakly defined. We present a percussion instrument utilizing this technique in which the timbral characteristics of found objects are the primary source of audio for analysis.},
address = {London, United Kingdom},
author = {Hattwick, Ian and Beebe, Preston and Hale, Zachary and Wanderley, Marcelo and Leroux, Philippe and Marandola, Fabrice},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {597--600},
publisher = {Goldsmiths, University of London},
title = {{Unsounding Objects: Audio Feature Extraction for the Control of Sound Synthesis}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}369.pdf},
year = {2014}
}
@inproceedings{Lock2006,
abstract = {The Orbophone is a new interface that radiates rather thanprojects sound and image. It provides a cohesive platformfor audio and visual presentation in situations where bothmedia are transmitted from the same location andlocalization in both media is perceptually correlated. Thispaper discusses the advantages of radiation overconventional sound and image projection for certain kindsof interactive public multimedia exhibits and describes theartistic motivation for its development against a historicalbackdrop of sound systems used in public spaces. Oneexhibit using the Orbophone is described in detail togetherwith description and critique of the prototype, discussingaspects of its design and construction. The paper concludeswith an outline of the Orbophone version 2.},
address = {Paris, France},
author = {Lock, Damien and Schiemer, Greg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Immersive Sound; Multi-channel Sound; Loud-speaker},
pages = {89--92},
title = {{Orbophone: a New Interface for Radiating Sound and Image}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}089.pdf},
year = {2006}
}
@inproceedings{Whalley2010,
address = {Sydney, Australia},
author = {Whalley, Ian},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {flocking,genetic algorithm,gesture,improvisation,interaction,nime10},
number = {Nime},
pages = {255--258},
title = {{Generative Improv . {\&} Interactive Music Project (GIIMP)}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}255.pdf},
year = {2010}
}
@inproceedings{Park:2013a,
abstract = {In this paper, we designed a sound effect device, which was applicable forspray paint art process. For the applicability research of the device, wedesigned a prototype which had a form not far off the traditional spray cans,using Arduino and various sensors. Through the test process of the prototype,we verified the elements that would be necessary to apply our newly designeddevice to real spray paint art activities. Thus we checked the possibility ofvarious musical expressions by expanding the functions of the designed device.},
address = {Daejeon, Republic of Korea},
author = {Park, Gibeom and Lee, Kyogu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Arduino,Pure Data,Sound effect device,Spray paint art},
pages = {65--68},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Sound Spray - can-shaped sound effect device}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Kiefer2008,
abstract = {There is small but useful body of research concerning theevaluation of musical interfaces with HCI techniques. Inthis paper, we present a case study in implementing thesetechniques; we describe a usability experiment which evaluated the Nintendo Wiimote as a musical controller, andreflect on the effectiveness of our choice of HCI methodologies in this context. The study offered some valuable results,but our picture of the Wiimote was incomplete as we lackeddata concerning the participants' instantaneous musical experience. Recent trends in HCI are leading researchers totackle this problem of evaluating user experience; we reviewsome of their work and suggest that with some adaptation itcould provide useful new tools and methodologies for computer musicians.},
address = {Genoa, Italy},
author = {Kiefer, Chris and Collins, Nick and Fitzpatrick, Geraldine},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Kiefer, Collins, Fitzpatrick - 2008 - HCI Methodology For Evaluating Musical Controllers A Case Study.pdf:pdf},
keywords = {Evaluating Musical Interac- tion,HCI Methodology,Wiimote},
pages = {87--90},
title = {{HCI Methodology For Evaluating Musical Controllers : A Case Study}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}087.pdf},
year = {2008}
}
@inproceedings{Goto2009a,
address = {Pittsburgh, PA, United States},
author = {Goto, Suguru and Powell, Rob},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {artificial intelligence,gesture controller,humanoid robot,interaction,internet,nime09,robot},
pages = {48--49},
title = {{netBody - "Augmented Body and Virtual Body II" with the System, BodySuit, Powered Suit and Second Life - Its Introduction of an Application of the System}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}048.pdf},
year = {2009}
}
@inproceedings{nime2014:jharriman,
abstract = {What do new possibilities for music and art making look like in a world in which the biological and mechanical are increasingly entangled? Can a contrived environment envelope the senses to the point that one feel fully immersed in it? It was with these questions in mind that the interactive mechanical sound art installation endo/exo came into being. Through the use of networked technology the system becomes more like a self-aware organism, passing messages from node to node as cells communicate through chemical signals with their neighbors. In an artistic context, the communication network resembles, but differs from, other mechanical systems. Issues such as latency are often considered negative factors, yet they can contribute a touch of personality in this context. This paper is a reflection on these and other considerations gained from the experience of designing and constructing endo/exo as well as future implications for the Honeycomb platform as a tool for creating musical interactions within a new paradigm which allows for emergent behavior across vast physical spaces. The use of swarming and self-organization, as well as playful interaction, creates an ``aliveness" in the mechanism, and renders its exploration pleasurable, intriguing and uncanny.},
address = {London, United Kingdom},
author = {Harriman, Jiffer and Theodore, Michael and Correll, Nikolaus and Ewen, Hunter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {383--386},
publisher = {Goldsmiths, University of London},
title = {{endo/exo - Making Art and Music with Distributed Computing}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}523.pdf},
year = {2014}
}
@inproceedings{Gelineck:2013,
abstract = {This paper presents the continuous work towards the development of an interfacefor music mixing targeted towards expert sound technicians and producers. Themixing interface uses a stage metaphor mapping scheme where audio channels arerepresented as digital widgets on a 2D surface. These can be controlled bymulti touch or by smart tangibles, which are tangible blocks with embeddedsensors. The smart tangibles developed for this interface are able to sense howthey are grasped by the user. The paper presents the design of the mixinginterface including the smart tangible as well as a preliminary user studyinvolving a hands-on focus group session where 5 different control technologiesare contrasted and discussed. Preliminary findings suggest that smart tangibleswere preferred, but that an optimal interface would include a combination oftouch, smart tangibles and an extra function control tangible for extending thefunctionality of the smart tangibles. Finally, the interface should incorporateboth an edit and mix mode - the latter displaying very limited visual feedbackin order to force users to focus their attention to listening instead of theinterface.},
address = {Daejeon, Republic of Korea},
author = {Gelineck, Steven and Overholt, Dan and B{\"{u}}chert, Morten and Andersen, Jesper},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {control surface,graspables,multi-touch,music mixing,physical-digital interface,sketching in hardware,smart objects,tangible user interface,tangibles,wireless sensing},
month = {may},
pages = {180--185},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Towards an Interface for Music Mixing based on Smart Tangibles and Multitouch}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{nime2014:osarier,
abstract = {In the recent years many touch screen interfaces have been designed and used for musical control. When compared with their physical counterparts, current control paradigms employed in touch screen musical interfaces do not require the same level of physical labor and this negatively affects the user experience in terms of expressivity, engagement and enjoyment. This lack of physicality can be remedied by using interaction elements, which are designed for the exertion of the user. Employing intentionally difficult and inefficient interaction design can enhance the user experience by allowing greater bodily expression, kinesthetic feedback, more apparent skill acquisition, and performer satisfaction. Rub Synth is a touch screen musical instrument with an exertion interface. It was made for creating and testing exertion strategies that are possible by only using 2d touch coordinates as input and evaluating the outcomes of implementing intentional difficulty. This paper discusses the strategies that can be employed to model effort on touch screens, the benefits of having physical difficulty, Rub Synth's interaction design, and user experience results of using such an interface.},
address = {London, United Kingdom},
author = {Sarier, Ozan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {179--182},
publisher = {Goldsmiths, University of London},
title = {{Rub Synth : A Study of Implementing Intentional Physical Difficulty Into Touch Screen Music Controllers}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}367.pdf},
year = {2014}
}
@inproceedings{Vigliensoni:2012,
abstract = {This paper presents a comparison of three-dimensional (3D) position tracking systems in terms of some of their performance parameters such as static accuracy and precision, update rate, and shape of the space they sense. The underlying concepts and characteristics of position tracking tech- nologies are reviewed, and four position tracking systems (Vicon, Polhemus, Kinect, and Gametrak), based on dif- ferent technologies, are empirically compared according to their performance parameters and technical specifications. Our results show that, overall, the Vicon was the position tracker with the best performance.},
address = {Ann Arbor, Michigan},
author = {Vigliensoni, Gabriel and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Position tracker,comparison,gestural control,touch-less},
publisher = {University of Michigan},
title = {{A Quantitative Comparison of Position Trackers for the Development of a Touch-less Musical Interface}},
year = {2012}
}
@inproceedings{nime2014:fmorreale,
abstract = {This paper presents MINUET, a framework for musical interface design grounded in the experience of the player. MINUET aims to provide new perspectives on the design of musical interfaces, referred to as a general term that comprises digital musical instruments and interactive installations. The ultimate purpose is to reduce the complexity of the design space emphasizing the experience of the player. MINUET is structured as a design process consisting of two stages: goal and specifications. The reliability of MINUET is tested through a systematic comparison with the related work and through a case study. To this end, we present the design and prototyping of Hexagon, a new musical interface with learning purposes.},
address = {London, United Kingdom},
author = {Morreale, Fabio and Angeli, Antonella De and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Morreale, Angeli, O'Modhrain - 2014 - Musical Interface Design An Experience-oriented Framework.pdf:pdf},
keywords = {DMI,evaluation},
mendeley-tags = {DMI,evaluation},
pages = {467--472},
publisher = {Goldsmiths, University of London},
title = {{Musical Interface Design: An Experience-oriented Framework}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}437.pdf},
year = {2014}
}
@inproceedings{Essl2009,
address = {Pittsburgh, PA, United States},
author = {Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {mobile phone instruments,nime,nime09,on-the-fly},
pages = {270--273},
title = {{SpeedDial : Rapid and On-The-Fly Mapping of Mobile Phone Instruments}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}270.pdf},
year = {2009}
}
@inproceedings{Stewart2006,
address = {Paris, France},
author = {Stewart, D Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {composition,controller,cross- modal interaction,gesture,materials,process},
pages = {103--105},
title = {{SonicJumper Composer}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}103.pdf},
year = {2006}
}
@inproceedings{Martin2011,
address = {Oslo, Norway},
author = {Martin, Charles and Lai, Chi-Hsia},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {computer vision,media performance,percussion},
number = {June},
pages = {142--143},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Strike on Stage : a Percussion and Media Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}142.pdf},
year = {2011}
}
@inproceedings{MakiPatola2006,
address = {Paris, France},
author = {Maki-Patola, Teemu and H{\"{a}}m{\"{a}}l{\"{a}}inen, Perttu and Kanerva, Aki},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {1,2,2 9,3897,39,425,43,7,8,9},
pages = {364--369},
title = {{The Augmented Djembe Drum - Sculpting Rhythms}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}364.pdf},
year = {2006}
}
@inproceedings{Kobayashi2006,
address = {Paris, France},
author = {Kobayashi, Shigeru and Endo, Takanori and Harada, Katsuhiko and Oishi, Shosei},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {learning,rapid prototyping,reconfigurable,sensor interface},
pages = {346--351},
title = {{GAINER: A Reconfigurable {\{}I/O{\}} Module and Software Libraries for Education}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}346.pdf},
year = {2006}
}
@inproceedings{Hankins:2002,
abstract = {The Circular Optical Object Locator is a collaborative and cooperative music-making device. It uses an inexpensive digital video camera to observe a rotating platter. Opaque objects placed on the platter are detected by the camera during rotation. The locations of the objects passing under the camera are used to generate music. },
address = {Dublin},
author = {Hankins, Tim and Merrill, David and Robert, Jocelyn},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Input devices,collaborative,music controllers,real-time score manipulation.},
pages = {80--81},
title = {{Circular Optical Object Locator}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}080.pdf},
year = {2002}
}
@inproceedings{Lew:2004,
abstract = {This paper describes the design of an expressive tangible interface for cinema editing as a live performance. A short survey of live video practices is provided. The Live Cinema instrument is a cross between a musical instrument and a film editing tool, tailored for improvisational control as well as performance presence. Design specifications for the instrument evolved based on several types of observations including: our own performances in which we used a prototype based on available tools; an analysis of performative aspects of contemporary DJ equipment; and an evaluation of organizational aspects of several generations of film editing tools. Our instrument presents the performer with a large canvas where projected images can be grabbed and moved around with both hands simultaneously; the performer also has access to two video drums featuring haptic display to manipulate the shots and cut between streams. The paper ends with a discussion of issues related to the tensions between narrative structure and hands-on control, live and recorded arts and the scoring of improvised films. },
address = {Hamamatsu, Japan},
author = {Lew, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {DJ,VJ,film editing,improvisation,live cinema,narrative structure.,performance,tactile interface,two-hand interaction,video controller,visual music},
pages = {144--149},
title = {{Live Cinema: Designing an Instrument for Cinema Editing as a Live Performance}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}144.pdf},
year = {2004}
}
@inproceedings{Dahl2007,
address = {New York City, NY, United States},
author = {Dahl, Luke and Whetsell, Nathan and {Van Stoecker}, John},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Musical controller,Puredata,flex sensors.,scanned synthesis},
pages = {270--272},
title = {{The WaveSaw : A Flexible Instrument for Direct Timbral Manipulation}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}270.pdf},
year = {2007}
}
@inproceedings{Richards2006,
abstract = {Why is a seemingly mundane issue such as airline baggageallowance of great significance in regards to the performancepractice of electronic music? This paper discusses how aperformance practice has evolved that seeks to question thebinary and corporate digital world. New 'instruments' andapproaches have emerged that explore 'dirty electronics' and'punktronics': DIY electronic instruments made from junk.These instruments are not instruments in the traditionalsense, defined by physical dimensions or by a set number ofparameters, but modular systems, constantly evolving, nevercomplete, infinitely variable and designed to be portable. Acombination of lo- and hi-fi, analogue and digital,synchronous and asynchronous devices offer new modes ofexpression. The development of these new interfaces formusical expression run side-by-side with an emerging postdigital aesthetic.},
address = {Paris, France},
author = {Richards, John},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {bastardisation,dirty electronics,diy,ebay,live,modular,performance,portability,post-digital,punktronics},
pages = {283--287},
title = {{32kg: Performance Systems for a Post-Digital Age}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}283.pdf},
year = {2006}
}
@inproceedings{Zoran2008,
address = {Genoa},
author = {Zoran, Amit and Maes, Pattie},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {nime08},
pages = {67--70},
title = {{Considering Virtual {\&} Physical Aspects in Acoustic Guitar Design}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}067.pdf},
year = {2008}
}
@inproceedings{Gelineck:2013,
abstract = {This paper presents the continuous work towards the development of an interfacefor music mixing targeted towards expert sound technicians and producers. Themixing interface uses a stage metaphor mapping scheme where audio channels arerepresented as digital widgets on a 2D surface. These can be controlled bymulti touch or by smart tangibles, which are tangible blocks with embeddedsensors. The smart tangibles developed for this interface are able to sense howthey are grasped by the user. The paper presents the design of the mixinginterface including the smart tangible as well as a preliminary user studyinvolving a hands-on focus group session where 5 different control technologiesare contrasted and discussed. Preliminary findings suggest that smart tangibleswere preferred, but that an optimal interface would include a combination oftouch, smart tangibles and an extra function control tangible for extending thefunctionality of the smart tangibles. Finally, the interface should incorporateboth an edit and mix mode - the latter displaying very limited visual feedbackin order to force users to focus their attention to listening instead of theinterface.},
address = {Daejeon, Republic of Korea},
author = {Gelineck, Steven and Overholt, Dan and B{\"{u}}chert, Morten and Andersen, Jesper},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {control surface,graspables,multi-touch,music mixing,physical-digital interface,sketching in hardware,smart objects,tangible user interface,tangibles,wireless sensing},
pages = {180--185},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Towards an Interface for Music Mixing based on Smart Tangibles and Multitouch}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Vanegas2007,
abstract = {The guitar pick has traditionally been used to strike or rakethe strings of a guitar or bass, and in rarer instances, ashamisen, lute, or other stringed instrument. The pressure exerted on it, however, has until now been ignored.The MIDI Pick, an enhanced guitar pick, embraces this dimension, acting as a trigger for serial data, audio samples,MIDI messages 1, Max/MSP patches, and on/off messages.This added scope expands greatly the stringed instrumentplayer's musical dynamic in the studio or on stage.},
address = {New York City, NY, United States},
author = {Vanegas, Roy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Arduino,ITP,MIDI,NIME,ZigBee,bluetooth,guitar,pick,plectrum,wireless},
pages = {330--333},
title = {{The MIDI Pick : Trigger Serial Data , Samples, and MIDI from a Guitar Pick}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}330.pdf},
year = {2007}
}
@inproceedings{Wang2011,
address = {Oslo, Norway},
author = {Wang, Ge and Oh, Jieun and Lieber, Tom},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {design,experiential,ipad,magic fiddle,music education,physical interaction design},
number = {June},
pages = {197--202},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Designing for the iPad : Magic Fiddle}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}197.pdf},
year = {2011}
}
@inproceedings{Fraietta2005a,
abstract = {The Smart Controller is a portable hardware device that responds to input control voltage, OSC, and MIDI messages; producing output control voltage, OSC, and MIDI messages (depending upon the loaded custom patch). The Smart Controller is a stand alone device; a powerful, reliable, and compact instrument capable of reducing the number of electronic modules required in a live performance or installation, particularly the requirement of a laptop computer. More powerful, however, is the Smart Controller Workbench, a complete interactive development environment. In addition to enabling the composer to create and debug their patches, the Smart Controller Workbench accurately simulates the behaviour of the hardware, and functions as an incircuit debugger that enables the performer to remotely monitor, modify, and tune patches running in an installation without the requirement of stopping or interrupting the live performance. },
address = {Vancouver, BC, Canada},
author = {Fraietta, Angelo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Actuators,Algorithmic Composition,Control Voltage,Interaction.,MIDI,Open Sound Control,Sensors,Sound Installations,electronic music,programmable logic control,synthesizers},
pages = {46--49},
title = {{The Smart Controller Workbench}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}046.pdf},
year = {2005}
}
@inproceedings{Han:2013a,
abstract = {This paper presents a framework that transforms fingerprint patterns intoaudio. We describe Digiti Sonus, an interactive installation performingfingerprint sonification and visualization, including novel techniques forrepresenting user-intended fingerprint expression as audio parameters. In orderto enable personalized sonification and broaden timbre of sound, theinstallation employs sound synthesis based on various visual feature analysissuch as minutiae extraction, area, angle, and push pressure of fingerprints.The sonification results are discussed and the diverse timbres of soundretrieved from different fingerprints are compared.},
address = {Daejeon, Republic of Korea},
author = {Han, Yoon Chung and Han, Byeong-jun and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Fingerprint,Fingerprint sonification,biometric data,interactive sonification,sound synthesis},
pages = {136--141},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Digiti Sonus: Advanced Interactive Fingerprint Sonification Using Visual Feature Analysis}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Burlet:2013,
abstract = {Sensor-based gesture recognition is investigated as a possible solution to theproblem of managing an overwhelming number of audio effects in live guitarperformances. A realtime gesture recognition system, which automaticallytoggles digital audio effects according to gestural information captured by anaccelerometer attached to the body of a guitar, is presented. To supplement theseveral predefined gestures provided by the recognition system, personalizedgestures may be trained by the user. Upon successful recognition of a gesture,the corresponding audio effects are applied to the guitar signal and visualfeedback is provided to the user. An evaluation of the system yielded 86{\%}accuracy for user-independent recognition and 99{\%} accuracy for user-dependentrecognition, on average.},
address = {Daejeon, Republic of Korea},
author = {Burlet, Gregory and Fujinaga, Ichiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Augmented instrument,accelerometer,gesture recognition,pattern recognition,performance practice},
pages = {41--44},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Stompboxes: Kicking the Habit}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Tarakajian:2013,
abstract = {Mira is an iPad app for controlling Max patchers in real time with minimalconfiguration. This submission includes a paper describing Mira's design andimplementation, as well as a demo showing how Mira works with Max.The Mira iPad app discovers open Max patchers automatically using the Bonjourprotocol, connects to them over WiFi and negotiates a description of the Maxpatcher. As objects change position and appearance, Mira makes sure that theinterface on the iPad is kept up to date. Mira eliminates the need for anexplicit mapping step between the interface and the system being controlled.The user is never asked to input an IP address, nor to configure the mappingbetween interface objects on the iPad and those in the Max patcher. So theprototyping composer is free to rapidly configure and reconfigure theinterface.},
address = {Daejeon, Republic of Korea},
author = {Tarakajian, Sam and Zicarelli, David and Clayton, Joshua},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Max/MSP/Jitter,Mira,NIME,bonjour,ipad,osc,zeroconf},
pages = {421--426},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Mira: Liveness in iPad Controllers for Max/MSP}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Takahata2004,
address = {Hamamatsu, Japan},
author = {Takahata, Masami and Shiraki, Kensuke and Sakane, Yutaka and Takebayashi, Yoichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {karate,learning environment,sound feedback,wearable},
pages = {13--18},
title = {{Sound Feedback for Powerful Karate Training}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}013.pdf},
year = {2004}
}
@inproceedings{Oliver2007,
address = {New York City, NY, United States},
author = {Oliver, Julian and Pickles, Steven},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {430},
title = {{Fijuu2 : A Game-Based Audio-Visual Performance and Composition Engine}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}430.pdf},
year = {2007}
}
@inproceedings{Pardue2011,
address = {Oslo, Norway},
author = {Pardue, Laurel S and Boch, Andrew and Boch, Matt and Southworth, Christine and Rigopulos, Alex},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {bali,figure 1,galak tika,gamelan,gamelan at the rear,including,instruments from the beta,midi ensemble,musical instrument design,s gangsa and reongs},
number = {June},
pages = {18--23},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Gamelan Elektrika: An Electronic Balinese Gamelan}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}018.pdf},
year = {2011}
}
@inproceedings{Crevoisier2006,
address = {Paris, France},
author = {Crevoisier, Alain and Bornand, C{\'{e}}dric and Guichard, Arnaud and Matsumura, Seiichiro and Arakawa, Chuichi},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
pages = {212--215},
title = {{Sound Rose: Creating Music and Images with a Touch Table}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}212.pdf},
volume = {1},
year = {2006}
}
@inproceedings{Marshall2002,
address = {Dublin, Ireland},
author = {Marshall, Mark T and Rath, Matthias and Moynihan, Breege},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {gesture,sound modeling,user-,virtual instrument},
pages = {118--119},
title = {{The Virtual Bodhran  The Vodhran}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}118.pdf},
year = {2002}
}
@inproceedings{Nelson2004,
address = {Hamamatsu, Japan},
author = {Nelson, Mark and Thom, Belinda},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
pages = {35--38},
title = {{A Survey of Real-Time MIDI Performance}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}035.pdf},
year = {2004}
}
@inproceedings{Leeuw2009,
address = {Pittsburgh, PA, United States},
author = {Leeuw, Hans},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {3,and bradley factor have,and helped,bluetooth,controller,lcd,low latency,made the trumpet midi,max,msp,multiple arduinos,nime09,osc,sukandar kartadinata has made,trumpet},
pages = {193--198},
title = {{The Electrumpet , a Hybrid Electro-Acoustic Instrument}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}193.pdf},
year = {2009}
}
@article{Taube2009,
author = {Taube, Heinrich},
doi = {10.1080/07494460802664080},
file = {::},
isbn = {0749446080},
issn = {0749-4467},
journal = {Contemporary Music Review},
month = {feb},
number = {1},
pages = {115--128},
title = {{Artists' Statements}},
url = {http://www.tandfonline.com/doi/abs/10.1080/07494460802664080},
volume = {28},
year = {2009}
}
@inproceedings{Feugere:2013,
abstract = {Digitartic, a system for bi-manual gestural control of Vowel-Consonant-Vowelperformative singing synthesis is presented. This system is an extension of areal-time gesture-controlled vowel singing instrument developed in the Max/MSPlanguage. In addition to pitch, vowels and voice strength control, Digitarticis designed for gestural control of articulation parameters for a wide set onconsonant, including various places and manners of articulation. The phases ofarticulation between two phonemes are continuously controlled and can bedriven in real time without noticeable delay, at any stage of the syntheticphoneme production. Thus, as in natural singing, very accurate rhythmicpatterns are produced and adapted while playing with other musicians. Theinstrument features two (augmented) pen tablets for controlling voiceproduction: one is dealing with the glottal source and vowels, the second oneis dealing with consonant/vowel articulation. The results show very naturalconsonant and vowel synthesis. Virtual choral practice confirms theeffectiveness of Digitartic as an expressive musical instrument.},
address = {Daejeon, Republic of Korea},
author = {Feug{\`{e}}re, Lionel and D'Alessandro, Christophe},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {articulation,formants synthesis,gestural control,singing voice synthesis,syllabic synthesis},
pages = {331--336},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Digitartic: bi-manual gestural control of articulation in performative singing synthesis}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Bencina2008,
address = {Genoa, Italy},
author = {Bencina, Ross and Wilde, Danielle and Langley, Somaya},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {gestural control,mapping,nime08,prototyping,three-axis accelerometers,vocal,wii remote},
pages = {197--202},
title = {{Gesture  Sound Experiments : Process and Mappings}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}197.pdf},
year = {2008}
}
@inproceedings{nime2014:jlarsen,
abstract = {People with a physical handicap are often not able to engage and embrace the world of music on the same terms as normal functioning people. Musical instruments have been refined the last centuries which makes them highly specialized instruments that nearly all requires at least two functioning hands. In this study we try to enable people with hemiplegia to play a real electrical guitar by modifying it in a way that make people with hemiplegia able to actually play the guitar. We developed the guitar platform to utilize sensors to capture the rhythmic motion of alternative fully functioning limbs, such as a foot, knee or the head to activate a motorized fader moving a pick back and forth across the strings. The approach employs the flexibility of a programmable digital system which allows us to scale and map different ranges of data from various sensors to the motion of the actuator and thereby making it easier adapt to individual users. To validate and test the instrument platform we collaborated with the Helena Elsass Center during their 2013 Summer Camp to see if we actually succeeded in creating an electrical guitar that children with hemiplegia could actually play. The initial user studies showed that children with hemiplegia were able to play the actuated guitar by producing rhythmical movement across the strings that enables them to enter a world of music they so often see as closed.},
address = {London, United Kingdom},
author = {Larsen, Jeppe and Overholt, Dan and Moeslund, Thomas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {60--65},
publisher = {Goldsmiths, University of London},
title = {{The Actuated guitar: Implementation and user test on children with Hemiplegia}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}486.pdf},
year = {2014}
}
@inproceedings{Melo:2012,
abstract = {This paper describes the conceptualization and development of an open source tool for controlling the sound of a saxophone via the gestures of its performer. The motivation behind this work is the need for easy access tools to explore, compose and perform electroacoustic music in Colombian music schools and conservatories. This work led to the adaptation of common hardware to be used as a sensor attached to an acoustic instrument and the development of software applications to record, visualize and map performers gesture data into signal processing parameters. The scope of this work suggested that focus was to be made on a specific instrument so the saxophone was chosen. Gestures were selected in an iterative process with the performer, although a more ambitious strategy to figure out main gestures of an instruments performance was first defined. Detailed gesture-to-sound processing mappings are exposed in the text. An electroacoustic musical piece was successfully rehearsed and recorded using the Gest-O system.},
address = {Ann Arbor, Michigan},
author = {Melo, Jonh and G{\'{o}}mez, Daniel and Vargas, Miguel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {Electroacoustic music,expanded instrument,gesture.,saxophone},
publisher = {University of Michigan},
title = {{Gest-O: Performer gestures used to expand the sounds of the saxophone}},
year = {2012}
}
@inproceedings{Gerhard:2004,
abstract = {This paper describes an approach to match visual and acoustic parameters to produce an animated musical expression.Music may be generated to correspond to animation, asdescribed here; imagery may be created to correspond tomusic; or both may be developed simultaneously. This approach is intended to provide new tools to facilitate bothcollaboration between visual artists and musicians and examination of perceptual issues between visual and acousticmedia. As a proof-of-concept, a complete example is developed with linear fractals as a basis for the animation, andarranged rhythmic loops for the music. Since both visualand acoustic elements in the example are generated fromconcise specifications, the potential of this approach to create new works through parameter space exploration is accentuated, however, there are opportunities for applicationto a wide variety of source material. These additional applications are also discussed, along with issues encounteredin development of the example.},
address = {Hamamatsu, Japan},
author = {Gerhard, David and Hepting, Daryl and Mckague, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {Multimedia creation and interaction,parameter space,sonification.,visu- alization},
pages = {96--99},
title = {{Exploration of the Correspondence between Visual and Acoustic Parameter Spaces}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}096.pdf},
year = {2004}
}
@inproceedings{Paine2004,
address = {Hamamatsu, Japan},
author = {Paine, Garth},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {dynamic,dynamic morphology,gesture,interaction,mapping,mind,music,orchestration,spectral morphology},
pages = {80--86},
title = {{Gesture and Musical Interaction : Interactive Engagement Through Dynamic Morphology}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}080.pdf},
year = {2004}
}
@inproceedings{Dimitrov2011,
abstract = {A contemporary PC user, typically expects a sound cardto be a piece of hardware, that: can be manipulated by'audio' software (most typically exemplified by 'media players'); and allows interfacing of the PC to audio reproduction and/or recording equipment. As such, a 'sound card'can be considered to be a system, that encompasses designdecisions on both hardware and software levels -- that also demand a certain understanding of the architecture of thetarget PC operating system.This project outlines how an Arduino Duemillanoveboard (containing a USB interface chip, manufactured byFuture Technology Devices International Ltd [FTDI]company) can be demonstrated to behave as a full-duplex,mono, 8-bit 44.1 kHz soundcard, through an implementation of: a PC audio driver for ALSA (Advanced LinuxSound Architecture); a matching program for theArduino'sATmega microcontroller -- and nothing more than headphones (and a couple of capacitors). The main contributionof this paper is to bring a holistic aspect to the discussionon the topic of implementation of soundcards -- also by referring to open-source driver, microcontroller code and testmethods; and outline a complete implementation of an open -- yet functional -- soundcard system.},
address = {Oslo, Norway},
author = {Dimitrov, Smilen and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {alsa,arduino,audio,driver,linux,sound card},
pages = {211--216},
title = {{Audio Arduino -- an ALSA (Advanced Linux Sound Architecture) Audio Driver for FTDI-based Arduinos}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}211.pdf},
year = {2011}
}
@inproceedings{Chun2010,
abstract = {The field of mixed-reality interface design is relatively young and in regards to music, has not been explored in great depth. Using computer vision and collision detection techniques, Freepad further explores the development of mixed-reality interfaces for music. The result is an accessible user-definable MIDI interface for anyone with a webcam, pen and paper, which outputs MIDI notes with velocity values based on the speed of the strikes on drawn pads. },
address = {Sydney, Australia},
author = {Chun, Sungkuk and Hawryshkewich, Andrew and Jung, Keechul and Pasquier, Philippe},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Computer vision,MIDI,collision detection,custom interface,form recognition,mixed- reality},
pages = {31--36},
title = {{Freepad : A Custom Paper-based MIDI Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}031.pdf},
year = {2010}
}
@inproceedings{Dobrian2006,
address = {Paris, France},
author = {Dobrian, Christopher and Koppelman, Daniel},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
keywords = {expression,instrument design,performance,virtuosity},
pages = {277--282},
title = {{The E in NIME: Musical Expression with New Computer Interfaces}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}277.pdf},
year = {2006}
}
@inproceedings{Murray-Browne2011,
abstract = {Many performers of novel musical instruments find it difficult to engage audiences beyond those in the field. Previousresearch points to a failure to balance complexity with usability, and a loss of transparency due to the detachmentof the controller and sound generator. The issue is oftenexacerbated by an audience's lack of prior exposure to theinstrument and its workings.However, we argue that there is a conflict underlyingmany novel musical instruments in that they are intendedto be both a tool for creative expression and a creative workof art in themselves, resulting in incompatible requirements.By considering the instrument, the composition and theperformance together as a whole with careful considerationof the rate of learning demanded of the audience, we propose that a lack of transparency can become an asset ratherthan a hindrance. Our approach calls for not only controllerand sound generator to be designed in sympathy with eachother, but composition, performance and physical form too.Identifying three design principles, we illustrate this approach with the Serendiptichord, a wearable instrument fordancers created by the ,
,
authors.},
address = {Oslo, Norway},
author = {Murray-Browne, Tim and Mainstone, Di and Bryan-Kinns, Nick and Plumbley, Mark D},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Performance,composed instrument,constraint.,transparency},
pages = {56--59},
title = {{The Medium is the Message: Composing Instruments and Performing Mappings}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}056.pdf},
year = {2011}
}
@inproceedings{Brandtsegg2011,
address = {Oslo, Norway},
author = {Brandtsegg, {\O}yvind and Saue, Sigurd and Johansen, Thom},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {granular synthesis,live performance,mapping,modulation},
number = {7491},
pages = {316--319},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{A Modulation Matrix for Complex Parameter Sets}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}316.pdf},
year = {2011}
}
@inproceedings{Gaye2006,
address = {Paris, France},
author = {Gaye, Lalya and Holmquist, Lars E and Behrendt, Frauke and Tanaka, Atau},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
pages = {22--25},
title = {{Mobile Music Technology: Report on an Emerging Community}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}022.pdf},
volume = {1},
year = {2006}
}
@inproceedings{Booth:2012,
abstract = {In this paper, we argue that the design of New Interfaces for Musical Expression has much to gain from the study of interaction in ensemble laptop performance contexts using ethnographic techniques. Inspired by recent third-stream research in the field of human computer interaction, we describe a recent ethnomethodologically-informed study of the Birmingham Laptop Ensemble (BiLE), and detail our approach to thick description of the group's working practices. Initial formal analysis of this material sheds light on the fluidity of composer, performer and designer roles within the ensemble and shows how confluences of these roles constitute member's differing viewpoints. We go on to draw out a number of strands of interaction that highlight the essentially complex, socially constructed and value driven nature of the group's practice and conclude by reviewing the implications of these factors on the design of software tools for laptop ensembles.},
address = {Ann Arbor, Michigan},
author = {Booth, Graham and Gurevich, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Ethnography,Ethnomethodology,Human Computer Interaction.,Laptop Performance},
publisher = {University of Michigan},
title = {{Collaborative composition and socially constituted instruments: Ensemble laptop performance through the lens of ethnography}},
year = {2012}
}
@inproceedings{Goina2008,
abstract = {In this paper, we investigate the relationships between gesture and sound by means of an elementary gesture sonification. This work takes inspiration from Bauhaus' ideals and Paul Klee's investigation into forms and pictorial representation. In line with these ideas, the main aim of this work is to reduce gesture to a combination of a small number of elementary components (gestalts) used to control a corresponding small set of sounds. By means of a demonstrative tool, we introduce here a line of research that is at its initial stage. The envisaged goal of future developments is a novel system that could be a composing/improvising tool as well as an interface for interactive dance and performance. },
address = {Genoa, Italy},
author = {Goina, Maurizio and Polotti, Pietro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Bauhaus,Klee,gesture analysis,sonification.},
pages = {150--153},
title = {{Elementary Gestalts for Gesture Sonification}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}150.pdf},
year = {2008}
}
@inproceedings{Wang2005a,
address = {Vancouver, BC, Canada},
author = {Wang, Ge and Cook, Perry R},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {196--199},
title = {{Yeah, ChucK It! = {\textgreater} Dynamic , Controllable Interface Mapping}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}196.pdf},
year = {2005}
}
@inproceedings{Resch:2013,
abstract = {"note{\~{}} for Max - An extension for Max/MSP for Media Arts {\&} musicShort Paper/Poster {\&} Workshop proposal for the NIME 2013At the Electronic Studio/Research Department of the University of Music Baselwe have been developing the software note{\~{}}, which consists of 4 in theC-Programming language developed objects for Max/MSP. The essential concept ofnote{\~{}} is that of a classic MIDI sequencer: Recording, editing and playing backcontrol data. It provides a GUI, a scripting interface and feedback for everyinteraction so its functionality can be extended by common Max/MSP objects. Instead of using the limited MIDI format , note{\~{}} allows storing floating pointlists up to 1024 elements plus text within one event.In order to get a quick overview about note{\~{}}'s capabilities I recommendwatching the video ""noteForMax{\_}IntroductionToNote{\~{}}"".Instead of doing a demonstration for the short paper I would like to propose a3 hours workshop for note{\~{}}:The first half of the workshop would be an introduction to note{\~{}}, its basicfeatures and how to integrate it into the Max/MSP environment as it is shown inthe videos""noteForMax{\_}Webcast1"" and ""noteForMax{\_}Webcast2"".In the second half, some of the examples shown in the videos""noteForMax{\_}IntroductionToNote{\~{}}"", ""noteForMax{\_}theMakingOfWings{\&}Halos"" and""noteForMax{\_}WhatsNewInTheUpcomingRelease""would be explained in detail:- Extending note{\~{}}'s functionality with Max Programming- Synchronizing several note{\~{}} objects in order to create complex polyrhythmicstructures- Synchronizing audio-, event- and video- playback with note{\~{}}- Using the note.score object for live score generation- Importing data from SDIF- and Excel-Files and work with them algorithmically"},
address = {Daejeon, Republic of Korea},
author = {Resch, Thomas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {GUI,Max/MSP,composing,notation.,score,sequencing,timeline},
pages = {210--212},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{note{\~{}} for Max - An extension for Max/MSP for Media Arts {\&} music}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Ng:2002,
abstract = {This paper briefly describes a number of performance interfaces under the broad theme of Interactive Gesture Music (IGM). With a short introduction, this paper discusses the main components of a Trans-Domain Mapping (TDM) framework, and presents various prototypes developed under this framework, to translate meaningful activities from one creative domain onto another, to provide real-time control of musical events with physical movements. },
address = {Dublin},
author = {Ng, Kia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Gesture,Interactive,Motion,Music.,Performance},
pages = {131--132},
title = {{Interactive Gesture Music Performance Interface}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}131.pdf},
year = {2002}
}
@inproceedings{Jenkins:2013,
abstract = {This paper presents a minimally-invasive, wireless optical sensorsystem for use with any conventional piston valve acoustic trumpet. Itis designed to be easy to install and remove by any trumpeter. Ourgoal is to offer the extended control afforded by hyperinstrumentswithout the hard to reverse or irreversible invasive modificationsthat are typically used for adding digital sensing capabilities. Weutilize optical sensors to track the continuous position displacementvalues of the three trumpet valves. These values are trasmittedwirelessly and can be used by an external controller. The hardware hasbeen designed to be reconfigurable by having the housing 3D printed sothat the dimensions can be adjusted for any particular trumpetmodel. The result is a low cost, low power, easily replicable sensorsolution that offers any trumpeter the ability to augment their ownexisting trumpet without compromising the instrument's structure orplaying technique. The extended digital control afforded by our systemis interweaved with the natural playing gestures of an acoustictrumpet. We believe that this seemless integration is critical forenabling effective and musical human computer interaction.Keywords: hyperinstrument, trumpet, minimally-invasive, gesture sensing,wireless, I2C},
address = {Daejeon, Republic of Korea},
author = {Jenkins, Leonardo and Trail, Shawn and Tzanetakis, George and Driessen, Peter and Page, Wyatt},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Jenkins et al. - 2013 - An Easily Removable, wireless Optical Sensing System (EROSS) for the Trumpet.pdf:pdf},
keywords = {I2C,gesture sensing,hyperinstrument,minimally-invasive,trumpet,wireless},
pages = {352--357},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{An Easily Removable, wireless Optical Sensing System (EROSS) for the Trumpet}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Ogawa2009,
abstract = {We developed a system called Life Game Orchestra that generates music by translating cellular patterns of Conway's Game of Life into musical scales. A performer can compose music by controlling varying cell patterns and sounds with visual and auditory fun. A performer assigns the elements of tone to two-dimensional cell patterns in the matrix of the Game of Life. Our system searches defined cell patterns in the varying matrix dynamically. If the patterns are matched, corresponding tones are generated. A performer can make cells in the matrix by moving in front of a camera and interactively influencing the generation of music. The progress of the Game of Life is controlled with a clock defined by the performer to configure the groove of the music. By running multiple matrices with different pattern mapping, clock timing, and instruments, we can perform an ensemble. The Life Game Orchestra is a fusion system of the design of a performer and the emergence of cellular automata as a complex system. },
address = {Pittsburgh, PA, United States},
author = {Ogawa, Keisuke and Kuhara, Yasuo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Cell pattern,Cellular automata,Conway's Game of Life,Interactive composition,performance.,scale},
pages = {50--51},
title = {{Life Game Orchestra as an Interactive Music Composition System Translating Cellular Patterns of Automata into Musical Scales}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}050.pdf},
year = {2009}
}
@inproceedings{Gimenes2007,
abstract = {In this paper we introduce a System conceived to serve as the "musical brain" of autonomous musical robots or agent-based software simulations of robotic systems. Our research goal is to provide robots with the ability to integrate with the musical culture of their surroundings. In a multi-agent configuration, the System can simulate an environment in which autonomous agents interact with each other as well as with external agents (e.g., robots, human beings or other systems). The main outcome of these interactions is the transformation and development of their musical styles as well as the musical style of the environment in which they live. },
address = {New York City, NY, United States},
author = {Gimenes, Marcelo and Miranda, Eduardo and Johnson, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {artificial life,musical style,musicianship,nime07},
pages = {197--202},
title = {{Musicianship for Robots with Style}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}197.pdf},
year = {2007}
}
@inproceedings{Favilla2008,
abstract = {This demonstration presents three new augmented and metasaxophone interface/instruments, built by the Bent LeatherBand. The instruments are designed for virtuosic liveperformance and make use of Sukandar Kartadinata's Gluion[OSC] interfaces. The project rationale and research outcomesfor the first twelve months is discussed. Instruments/interfacesdescribed include the Gluisop, Gluialto and Leathersop.},
address = {Genoa, Italy},
author = {Favilla, Stuart and Cannon, Joanne and Hicks, Tony and Chant, Dale and Favilla, Paris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Augmented saxophone,Gluion,OSC,virtuosic performance systems},
pages = {366--369},
title = {{Gluisax : Bent Leather Band{\{}$\backslash$textquoteright{\}}s Augmented Saxophone Project}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}366.pdf},
year = {2008}
}
@inproceedings{Liebman2010,
address = {Sydney, Australia},
author = {Liebman, Noah and Nagara, Michael and Spiewla, Jacek and Zolkosky, Erin},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {audio,control surfaces,mixing board,multitouch,nime10,screen,sound,theatre,touch-,user-centered design},
number = {Nime},
pages = {51--56},
title = {{Cuebert : A New Mixing Board Concept for Musical Theatre}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}051.pdf},
year = {2010}
}
@inproceedings{Nort:2012,
abstract = {In this paper we discuss aspects of our work in develop- ing performance systems that are geared towards human-machine co-performance with a particular emphasis on improvisation. We present one particular system, FILTER, which was created in the context of a larger project related to artificial intelligence and performance, and has been tested in the context of our electro-acoustic performance trio. We discuss how this timbrally rich and highly non-idiomatic musical context has challenged the design of the system, with particular emphasis on the mapping of machine listening parameters to higher-level behaviors of the system in such a way that spontaneity and creativity are encouraged while maintaining a sense of novel dialogue.},
address = {Ann Arbor, Michigan},
author = {Nort, Doug Van and Braasch, Jonas and Oliveros, Pauline},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Electroacoustic Improvisation,Machine Learning,Mapping,Sonic Gestures,Spatialization},
publisher = {University of Michigan},
title = {{Mapping to musical actions in the FILTER system}},
year = {2012}
}
@inproceedings{Jakovich2007,
address = {New York City, NY, United States},
author = {Jakovich, Joanne and Beilharz, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Architecture,adaptation,engagement.,granular synthesis,installation,interaction},
pages = {185--190},
title = {{ParticleTecture : Interactive Granular Soundspaces for Architectural Design}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}185.pdf},
year = {2007}
}
@inproceedings{Bowers2006,
address = {Paris, France},
author = {Bowers, John and Villar, Nicolas},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {ad hoc instruments,expression,music,new interfaces for musical,performance,physical interfaces,pin,play},
pages = {234--239},
title = {{Creating Ad Hoc Instruments with Pin{\&}Play{\&}Perform}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}234.pdf},
year = {2006}
}
@inproceedings{Dubois2009,
address = {Pittsburgh, PA, United States},
author = {Dubois, R Luke and Flanigan, Lesley},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {336},
title = {{Bioluminescence}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}336.pdf},
year = {2009}
}
@inproceedings{Rogers2009,
address = {Pittsburgh, PA, United States},
author = {Rogers, Troy and Kemper, Steven and Barton, Scott},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
number = {1},
pages = {323},
title = {{Study no. 1 for PAM and MADI}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}323.pdf},
year = {2009}
}
@inproceedings{Hockman2009,
abstract = {This paper presents a method for using a runner's pacefor real-time control of the time-scaling facility of a phasevocoder, resulting in the automated synchronization of anaudio track tempo to the generated control signal. The increase in usage of portable music players during exercisehas given rise to the development of new personal exerciseaids, most notably the Nike+iPod system, which relies onembedded sensor technologies to provide kinematic workout statistics. There are also systems that select songs basedon the measured step frequency of a runner. The proposedsystem also uses the pace of a runner, but this information isused to change the tempo of the music.},
address = {Pittsburgh, PA, United States},
author = {Hockman, Jason A and Wanderley, Marcelo M and Fujinaga, Ichiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {NIME,exercise,synchronization,time-scaling.},
pages = {90--93},
title = {{Real-Time Phase Vocoder Manipulation by Runner{\{}$\backslash$textquoteright{\}}s Pace}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}090.pdf},
year = {2009}
}
@inproceedings{Butler2008,
address = {Genoa, Italy},
author = {Butler, Jennifer},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {composition,etudes,max,msp,musical controllers,musical expression,nime08,pedagogy,repertoire},
pages = {77--80},
title = {{Creating Pedagogical Etudes for Interactive Instruments}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}077.pdf},
year = {2008}
}
@inproceedings{McPherson2010,
abstract = {This paper presents the magnetic resonator piano, an augmented instrument enhancing the capabilities of the acoustic grand piano. Electromagnetic actuators induce the stringsto vibration, allowing each note to be continuously controlled in amplitude, frequency, and timbre without external loudspeakers. Feedback from a single pickup on thepiano soundboard allows the actuator waveforms to remainlocked in phase with the natural motion of each string. Wealso present an augmented piano keyboard which reportsthe continuous position of every key. Time and spatial resolution are sufficient to capture detailed data about keypress, release, pretouch, aftertouch, and other extended gestures. The system, which is designed with cost and setupconstraints in mind, seeks to give pianists continuous control over the musical sound of their instrument. The instrument has been used in concert performances, with theelectronically-actuated sounds blending with acoustic instruments naturally and without amplification.},
address = {Sydney, Australia},
author = {McPherson, Andrew and Kim, Youngmoo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Augmented instruments,electromagnetic actuation,gesture measurement,interfaces,piano},
pages = {217--222},
title = {{Augmenting the Acoustic Piano with Electromagnetic String Actuation and Continuous Key Position Sensing}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}217.pdf},
year = {2010}
}
@inproceedings{Jensenius2010,
address = {Sydney, Australia},
author = {Jensenius, Alexander R and Innervik, Kjell Tore and Frounberg, Ivar},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {glass instruments,microphone placement,nime10,sound analysis},
number = {Nime},
pages = {208--211},
title = {{Evaluating the Subjective Effects of Microphone Placement on Glass Instruments}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}208.pdf},
year = {2010}
}
@inproceedings{Fels:2002,
abstract = {In this paper we describe three new music controllers, each designed to be played by two players. As the intimacy between two people increases so does their ability to anticipate and predict the other's actions. We hypothesize that this intimacy between two people can be used as a basis for new controllers for musical expression. Looking at ways people communicate non-verbally, we are developing three new instruments based on different communication channels. The Tooka is a hollow tube with a pressure sensor and buttons for each player. Players place opposite ends in their mouths and modulate the pressure in the tube with their tongues and lungs, controlling sound. Coordinated button presses control the music as well. The Pushka, yet to be built, is a semirigid rod with strain gauges and position sensors to track the rod's position. Each player holds opposite ends of the rod and manipulates it together. Bend, end point position, velocity and acceleration and torque are mapped to musical parameters. The Pullka, yet to be built, is simply a string attached at both ends with two bridges. Tension is measured with strain gauges. Players manipulate the string tension at each end together to modulate sound. We are looking at different musical mappings appropriate for two players.},
address = {Dublin},
author = {Fels, Sidney S and Vogt, Florian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Two person musical instruments,cooperative music,human-human communication,intimacy,passive haptic interface},
pages = {50--55},
title = {{Tooka: Explorations of Two Person Instruments}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}050.pdf},
year = {2002}
}
@inproceedings{nime2014:tmagnusson,
abstract = {Live coding emphasises improvisation. It is an art practice that merges the act of musical composition and performance into a public act of projected writing. This paper introduces the Threnoscope system, which includes a live coding micro-language for drone-based microtonal composition. The paper discusses the aims and objectives of the system, elucidates the design decisions, and introduces in particular the code score feature present in the Threnoscope. The code score is a novel element in the design of live coding systems allowing for improvisation through a graphic score, rendering a visual representation of past and future events in a real-time performance. The paper demonstrates how the system's methods can be mapped ad hoc to GUI- or hardware-based control.},
address = {London, United Kingdom},
author = {Magnusson, Thor},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {19--22},
publisher = {Goldsmiths, University of London},
title = {{Improvising with the Threnoscope: Integrating Code, Hardware, GUI, Network, and Graphic Scores}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}276.pdf},
year = {2014}
}
@inproceedings{Cartwright2007,
abstract = {This report presents the design and construct ion of Rage in Conjunction with the Machine, a simple but novel pairing of musical interface and sound sculpture. The ,
,
authors discuss the design and creation of this instrument , focusing on the unique aspects of it, including the use of physical systems, large gestural input, scale, and the electronic coupling of a physical input to a physical output.},
address = {New York City, NY, United States},
author = {Cartwright, Mark and Jones, Matt and Terasawa, Hiroko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {audience participation,inflatable,instrume nt design,instrume nt size,mapping,musical,new musical instrument,nime07,physical systems,sound scultpure},
pages = {224--227},
title = {{Rage in Conjunction with the Machine}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}224.pdf},
year = {2007}
}
@inproceedings{nime2014:rcollecchia,
abstract = {Sirens evoke images of alarm, public service, war, and forthcoming air raid. Outside of the music of Edgard Varese, sirens have rarely been framed as musical instruments. By connecting air hoses to spinning disks with evenly-spaced perforations, the siren timbre is translated musically. Polyphony gives our instrument an organ-like personality: keys are mapped to different frequencies and the pressure applied to them determines volume. The siren organ can produce a large range of sounds both timbrally and dynamically. In addition to a siren timbre, the instrument produces similar sounds to a harmonica. Portability, robustness, and electronic stability are all areas for improvement.},
address = {London, United Kingdom},
author = {Collecchia, Regina and Somen, Dan and McElroy, Kevin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {391--394},
publisher = {Goldsmiths, University of London},
title = {{The Siren Organ}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}558.pdf},
year = {2014}
}
@inproceedings{Bokowiec2011,
abstract = {V'OCT(Ritual) is a work for solo vocalist/performer and Bodycoder System, composed in residency at Dartington College of Arts (UK) Easter 2010. This paper looks at the technical and compositional methodologies used in the realization of the work, in particular, the choices made with regard to the mapping of sensor elements to various spatialization functions. Kinaesonics will be discussed in relation to the coding of real-time one-to-one mapping of sound to gesture and its expression in terms of hardware and software design. Four forms of expressivity arising out of interactive work with the Bodycoder system will be identified. How sonic (electro-acoustic), programmed, gestural (kinaesonic) and in terms of the V'Oct(Ritual) vocal expressivities are constructed as pragmatic and tangible elements within the compositional practice will be discussed and the subsequent importance of collaboration with a performer will be exposed. },
address = {Oslo, Norway},
author = {Bokowiec, Mark A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Bodycoder,Collaboration.,Expressivity,Gestural Control,Interactive Performance Mechanisms,Kinaesonics},
pages = {40--43},
title = {{V'OCT (Ritual): An Interactive Vocal Work for Bodycoder System and 8{\~{}}{\{}C{\}}hannel Spatialization}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}040.pdf},
year = {2011}
}
@inproceedings{nime2014:mgurevich1,
abstract = {Drawing on concepts from systemics, cybernetics, and musical automata, this paper proposes a mechatronic, electroacoustic instrument that allows for shared control between programmed, mechanized motion and a human interactor. We suggest that such an instrument, situated somewhere between a robotic musical instrument and a passive controller, will foster the emergence of new, complex, and meaningful modes of musical interaction. In line with the methodological principles of practice as research, we describe the development and design of one such instrument-Stringtrees. The design process also reflects the notion of ambiguity as a resource in design: The instrument was endowed with a collection of sensors, controls, and actuators without a highly specific or prescriptive model for how a musician would interact with it.},
address = {London, United Kingdom},
author = {Gurevich, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {487--490},
publisher = {Goldsmiths, University of London},
title = {{Distributed Control in a Mechatronic Musical Instrument}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}449.pdf},
year = {2014}
}
@inproceedings{Lippit2006,
address = {Paris, France},
author = {Lippit, Takuro M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {2,aspects of turntable music,dj,improvisation,max,microcontroller,msp,physical computing,pic,turntable music,turntablist},
pages = {71--74},
title = {{Turntable Music in the Digital Era: Designing Alternative Tools for New Turntable Expression}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}071.pdf},
year = {2006}
}
@inproceedings{Freed2009a,
abstract = {The history and future of Open Sound Control (OSC) is discussed and the next iteration of the OSC specification is introduced with discussion of new features to support NIME community activities. The roadmap to a major revision of OSC is developed. },
address = {Pittsburgh, PA, United States},
author = {Freed, Adrian and Schmeder, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Freed, Schmeder - 2009 - Features and Future of Open Sound Control version 1.1 for NIME.pdf:pdf},
keywords = {OSC,Open Sound Control,Reservation Protocols.,Time Tag},
pages = {116--120},
title = {{Features and Future of Open Sound Control version 1.1 for NIME}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}116.pdf},
year = {2009}
}
@inproceedings{Chadabe:2002,
abstract = {Mapping, which describes the way a performer's controls are connected to sound variables, is a useful concept when applied to the structure of electronic instruments modelled after traditional acoustic instruments. But mapping is a less useful concept when applied to the structure of complex and interactive instruments in which algorithms generate control information. This paper relates the functioning and benefits of different types of electronic instruments to the structural principles on which they are based. Structural models of various instruments will be discussed and musical examples played. },
address = {Dublin},
author = {Chadabe, Joel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {mapping fly-by-wire algorithmic network interactiv},
pages = {38--42},
title = {{The Limitations of Mapping as a Structural Descriptive in Electronic Instruments}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}038.pdf},
year = {2002}
}
@inproceedings{Miyama2009,
address = {Pittsburgh, PA, United States},
author = {Miyama, Chikashi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {326},
title = {{Angry Sparrow}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}326.pdf},
year = {2009}
}
@inproceedings{Skogstad:2013,
abstract = {In this paper we present some custom designed filters for real-time motioncapture applications. Our target application is so-called motion controllers,i.e. systems that interpret hand motion for musical interaction. In earlierresearch we found effective methods to design nearly optimal filters forreal-time applications. However, to be able to design suitable filters for ourtarget application, it is necessary to establish the typical frequency contentof the motion capture data we want to filter. This will again allow us todetermine a reasonable cutoff frequency for the filters. We have thereforeconducted an experiment in which we recorded the hand motion of 20 subjects.The frequency spectra of these data together with a method similar to theresidual analysis method were then used to determine reasonable cutofffrequencies. Based on this experiment, we propose three cutoff frequencies fordifferent scenarios and filtering needs: 5, 10 and 15 Hz, which corresponds toheavy, medium and light filtering respectively. Finally, we propose a range ofreal-time filters applicable to motion controllers. In particular, low-passfilters and low-pass differentiators of degrees one and two, which in ourexperience are the most useful filters for our target application.},
address = {Daejeon, Republic of Korea},
author = {Skogstad, St{\aa}le A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
month = {may},
pages = {142--147},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Filtering Motion Capture Data for Real-Time Applications}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{BryanKinns2010,
address = {Sydney, Australia},
author = {Bryan-Kinns, Nick and Fencott, Robin and Metatla, Oussama and Nabavian, Shahin and Sheridan, Jennifer G},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {awareness,collaboration,composition,improvisation,interactional,music,mutual engagement,nime10,sound},
number = {Nime},
pages = {403--406},
title = {{Interactional Sound and Music : Listening to CSCW, Sonification, and Sound Art}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}403.pdf},
year = {2010}
}
@inproceedings{Derbinsky2011,
address = {Oslo, Norway},
annote = {Application of machine learning and congnitive architectures techniques to design mobile music interactions.


Mobile smart devices are the present computational platform.


Interactions are not only created by sensors, but from copuling sensor and phyiscal aspect of the object (see acceleromenters in laptop and mobile).},
author = {Derbinsky, Nate and Essl, Georg},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {cognitive architecture,machine learning,mobile music},
number = {June},
pages = {104--107},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Cognitive Architecture in Mobile Music Interactions}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}104.pdf},
year = {2011}
}
@inproceedings{Lai:2012,
abstract = {This paper addresses the issue of engaging the audience with new musical instruments in live performance context. We introduce design concerns that we consider influential to enhance the communication flow between the audience and the performer. We also propose and put in practice a design approach that considers the use of performance space as a way to engage with the audience. A collaborative project, Sound Gloves, presented here exemplifies such a concept by dissolving the space between performers and audience. Our approach resulted in a continuous interaction between audience and performers, in which the social dynamics was changed in a positive way in a live performance context of NIMEs. Such an approach, we argue, may be considered as one way to further engage and interact with the audience.},
address = {Ann Arbor, Michigan},
author = {Lai, Chi-Hsia and Tahiroglu, Koray},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {NIME,design approach,performance,wearable electronics},
month = {may},
publisher = {University of Michigan},
title = {{A Design Approach to Engage with Audience with Wearable Musical Instruments: Sound Gloves}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}197.pdf},
year = {2012}
}
@inproceedings{Fyans:2012,
abstract = {A study is presented examining the participatory design of digital musical interactions. The study takes into consideration the entire ecology of digital musical interactions including the designer, performer and spectator. A new instrument is developed through iterative participatory design involving a group of performers. Across the study the evolution of creative practice and skill development in an emerging community of practice is examined and a spectator study addresses the cognition of performance and the perception of skill with the instrument. Observations are presented regarding the cognition of a novel interaction and evolving notions of skill. The design process of digital musical interactions is reflected on focusing on involvement of the spectator in design contexts.},
address = {Ann Arbor, Michigan},
author = {Fyans, A Cavan and Marquez-Borbon, Adnan and Stapleton, Paul and Gurevich, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {DMIs,cognition,participatory design,skill,spectator},
publisher = {University of Michigan},
title = {{Ecological considerations for participatory design of DMIs}},
year = {2012}
}
@inproceedings{Xiao:2013,
abstract = {The body channels rich layers of information when playing music, from intricatemanipulations of the instrument to vivid personifications of expression. Butwhen music is captured and replayed across distance and time, the performer'sbody is too often trapped behind a small screen or absent entirely.This paper introduces an interface to conjure the recorded performer bycombining the moving keys of a player piano with life-sized projection of thepianist's hands and upper body. Inspired by reflections on a lacquered grandpiano, our interface evokes the sense that the virtual pianist is playing thephysically moving keys.Through our interface, we explore the question of how to viscerally simulate aperformer's presence to create immersive experiences. We discuss designchoices, outline a space of usage scenarios and report reactions from users.},
address = {Daejeon, Republic of Korea},
author = {Xiao, Xiao and Pereira, Anna and Ishii, Hiroshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {augmented reality,body language,embodiment,musical expressivity,piano performance,player piano,recorded music},
pages = {7--12},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Conjuring the Recorded Pianist: A New Medium to Experience Musical Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Verplank2002,
address = {Dublin, Ireland},
author = {Verplank, Bill and Gurevich, Michael and Mathews, Max},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {haptics,music controllers,scanned synthesis},
pages = {177--180},
title = {{THE PLANK: Designing a Simple Haptic Controller.}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}177.pdf},
year = {2002}
}
@inproceedings{Schlei2010,
abstract = {Multi-point devices are rapidly becoming a practical interface choice for electronic musicians. Interfaces that generate multiple simultaneous streams of point data present a unique mapping challenge. This paper describes an analysis system for point relationships that acts as a bridge between raw streams of multi-point data and the instruments they control, using a multipoint trackpad to test various configurations. The aim is to provide a practical approach for instrument programmers working with multi-point tools, while highlighting the difference between mapping systems based on point coordinate streams, grid evaluations, or object interaction and mapping systems based on multi-point data relationships. },
address = {Sydney, Australia},
author = {Schlei, Kevin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Multi-point,instrument mapping,multi- point data analysis,multi-touch interface,trackpad instrument},
pages = {136--139},
title = {{Relationship-Based Instrument Mapping of Multi-Point Data Streams Using a Trackpad Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}136.pdf},
year = {2010}
}
@inproceedings{Gallin2009,
address = {Pittsburgh, PA, United States},
author = {Gallin, Emmanuelle and Sirguy, Marc},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {computer,controller,midi,music,nime09,ribbon cello,ribbon controllers,sensor,usb},
pages = {199--202},
title = {{Sensor Technology and the Remaking of Instruments from the Past}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}199.pdf},
year = {2009}
}
@inproceedings{Bongers2002,
address = {Dublin, Ireland},
author = {Bongers, Bert and Harris, Yolande},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
pages = {18--23},
title = {{A Structured Instrument Design Approach: The Video-Organ}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}018.pdf},
year = {2002}
}
@inproceedings{Carter2005,
abstract = {In this paper, we describe a course of research investigating thepotential for new types of music made possible by locationtracking and wireless technologies. Listeners walk arounddowntown Culver City, California and explore a new type ofmusical album by mixing together songs and stories based ontheir movement. By using mobile devices as an interface, wecan create new types of musical experiences that allowlisteners to take a more interactive approach to an album.},
address = {Vancouver, BC, Canada},
author = {Carter, William and Liu, Leslie S},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Augmented Reality,Digital Soundscape,Interactive Music,Location-Based Entertainment,Mobile Music,Mobility},
pages = {176--179},
title = {{Location33: A Mobile Musical}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}176.pdf},
year = {2005}
}
@inproceedings{Mcgee2011,
address = {Oslo, Norway},
author = {Mcgee, Ryan and Fan, Yuan-Yi and Ali, Reza},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {bio-feedback,bio-sensing,fm synthesis,open sound control,parallel computing,sonification,spa-,spatial audio,tialization,tion,visualiza-},
number = {June},
pages = {80--83},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{BioRhythm : a Biologically-inspired Audio-Visual Installation}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}080.pdf},
year = {2011}
}
@inproceedings{Kimura:2012,
abstract = {As a 2010 Artist in Residence in Musical Research at IRCAM, Mari Kimura used the Augmented Violin to develop new compositional approaches, and new ways of creating interactive performances [1]. She contributed her empirical and historical knowledge of violin bowing technique, working with the Real Time Musical Interactions Team at IRCAM. Thanks to this residency, her ongoing long-distance collaboration with the team since 2007 dramatically accelerated, and led to solving several compositional and calibration issues of the Gesture Follower (GF) [2]. Kimura was also the first artist to develop projects between the two teams at IRCAM, using OMAX (Musical Representation Team) with GF. In the past year, the performance with Augmented Violin has been expanded in larger scale interactive audio/visual projects as well. In this paper, we report on the various techniques developed for the Augmented Violin and compositions by Kimura using them, offering specific examples and scores.},
address = {Ann Arbor, Michigan},
author = {Kimura, Mari and Rasamimanana, Nicolas and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Schnell, Norbert and Zamborlin, Bruno and Fl{\'{e}}ty, Emmanuel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {Augmented Violin,Gesture Follower,Interactive Performance},
publisher = {University of Michigan},
title = {{Extracting Human Expression For Interactive Composition with the Augmented Violin}},
year = {2012}
}
@inproceedings{Flanigan2008,
abstract = {Plink Jet is a robotic musical instrument made from scavenged inkjet printers and guitar parts. We investigate the expressive capabilities of everyday machine technology by recontextualizing the relatively high-tech mechanisms of typical office debris into an electro-acoustic musical instrument. We also explore the performative relationship between human and machine.},
address = {Genoa, Italy},
author = {Flanigan, Lesley and Doro, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Automation,DIY,Infra-Instrument,Interaction Design,Performing Technology,Repurposing of Consumer Technology,Robotics},
pages = {349--351},
title = {{Plink Jet}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}349.pdf},
year = {2008}
}
@inproceedings{Baba2010,
address = {Sydney, Australia},
author = {Baba, Takashi and Hashida, Mitsuyo and Katayose, Haruhiro},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {conducting system,heuristics,nime10,sensor,template},
number = {Nime},
pages = {263--270},
title = {{"VirtualPhilharmony" : A Conducting System with Heuristics of Conducting an Orchestra}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}263.pdf},
year = {2010}
}
@inproceedings{nime2014:dwilliams,
abstract = {This paper presents an implementation of a near real-time timbre morphing signal processing system, designed to facilitate an element of `liveness' and unpredictability in a musical installation. The timbre morpher is a hybrid analysis and synthesis technique based on Spectral Modeling Synthesis (an additive and subtractive modeling technique). The musical installation forms an interactive soundtrack in response to the series of Rosso Luana marble sculptures Shapes in the Clouds, I, II, III, IV {\&} V by artist Peter Randall-Page, exhibited at the Peninsula Arts Gallery in Devon, UK, from 1 February to 29 March 2014. The timbre morphing system is used to transform live input captured at each sculpture with a discrete microphone array, by morphing towards noisy source signals that have been associated with each sculpture as part of a pre-determined musical structure. The resulting morphed audio is then fed-back to the gallery via a five-channel speaker array. Visitors are encouraged to walk freely through the installation and interact with the sound world, creating unique audio morphs based on their own movements, voices, and incidental sounds.},
address = {London, United Kingdom},
author = {Williams, Duncan and Randall-Page, Peter and Miranda, Eduardo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {435--438},
publisher = {Goldsmiths, University of London},
title = {{Timbre morphing: near real-time hybrid synthesis in a musical installation}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}440.pdf},
year = {2014}
}
@inproceedings{Berdahl2011a,
address = {Oslo, Norway},
author = {Berdahl, Edgar and Ju, Wendy},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {arduino,beagle board,instruments omap,linux,microcontrollers,music controllers,nime,pd,pedagogy,texas},
number = {June},
pages = {173--178},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Satellite CCRMA: A Musical Interaction and Sound Synthesis Platform}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}173.pdf},
year = {2011}
}
@inproceedings{Kirk2007,
address = {New York City, NY, United States},
author = {Kirk, Turner and Leider, Colby},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::;::},
keywords = {alternate controller,bagpipe,frankenpipe,midi,nime07,photoresistor},
pages = {301--304},
title = {{The FrankenPipe : A Novel Bagpipe Controller}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}301.pdf},
year = {2007}
}
@inproceedings{DArcangelo2004,
address = {Hamamatsu, Japan},
author = {D'Arcangelo, Gideon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {call and response,dj tools,oral culture,turntablism},
pages = {55--58},
title = {{Recycling Music, Answering Back: Toward an Oral Tradition of Electronic Music}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}055.pdf},
year = {2004}
}
@inproceedings{Christopher:2013,
abstract = {This paper describes Kontrol, a new hand interface that extends the intuitivecontrol of electronic music to traditional instrumentalist and dancers. Thegoal of the authors has been to provide users with a device that is capable ofdetecting the highly intricate and expressive gestures of the master performer,in order for that information to be interpreted and used for control ofelectronic music. This paper discusses related devices, the architecture ofKontrol, it's potential as a gesture recognition device, and severalperformance applications.},
address = {Daejeon, Republic of Korea},
author = {Christopher, Kameron and He, Jingyin and Kapur, Raakhi and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Hand controller,Wekinator,computational ethnomusicology,conducting interface,dance interface,wearable sensors},
pages = {267--270},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Kontrol: Hand Gesture Recognition for Music and Dance Interaction}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{croberts:2014,
abstract = {We describe research enabling the rapid creation of digital musical instruments and their publication to the Internet. This research comprises both high-level abstractions for making continuous mappings between audio, interactive, and graphical elements, as well as a centralized database for storing and accessing instruments. Published instruments run in most devices capable of running a modern web browser. Notation of instrument design is optimized for readability and expressivity.},
address = {London, United Kingdom},
author = {Roberts, Charlie and Wright, Matthew and Kuchera-Morin, JoAnn and H{\"{o}}llerer, Tobias},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {239--242},
publisher = {Goldsmiths, University of London},
title = {{Rapid Creation and Publication of Digital Musical Instruments}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}373.pdf},
year = {2014}
}
@inproceedings{Wozniewski2006,
abstract = {Traditional uses of virtual audio environments tend to focus onperceptually accurate acoustic representations. Though spatialization of sound sources is important, it is necessary to leveragecontrol of the sonic representation when considering musical applications. The proposed framework allows for the creation ofperceptually immersive scenes that function as musical instruments. Loudspeakers and microphones are modeled within thescene along with the listener/performer, creating a navigable 3Dsonic space where sound sources and sinks process audio according to user-defined spatial mappings.},
address = {Paris},
author = {Wozniewski, Mike and Settel, Zack and Cooperstock, Jeremy R},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {3D audio,Control paradigms,acoustic modeling,auditory display,immersive audio environments,spatial inter- faces,spatialization,virtual instrument design},
pages = {144--149},
title = {{A Framework for Immersive Spatial Audio Performance}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}144.pdf},
year = {2006}
}
@inproceedings{Chun2010,
address = {Sydney, Australia},
author = {Chun, Sungkuk and Hawryshkewich, Andrew and Jung, Keechul and Pasquier, Philippe},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {collision detection,computer vision,custom interface,form recognition,midi,mixed-,nime10,reality},
number = {Nime},
pages = {31--36},
title = {{Freepad : A Custom Paper-based MIDI Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}031.pdf},
year = {2010}
}
@inproceedings{Bencina2008,
address = {Genoa, Italy},
author = {Bencina, Ross and Wilde, Danielle and Langley, Somaya},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {gestural control,mapping,nime08,prototyping,three-axis accelerometers,vocal,wii remote},
pages = {197--202},
title = {{Gesture $\backslash$approx Sound Experiments : Process and Mappings}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}197.pdf},
year = {2008}
}
@inproceedings{Wright2003,
address = {Montreal, QC, Canada},
author = {Wright, Matthew and Freed, Adrian and Momeni, Ali},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {153--159},
title = {{OpenSound Control: State of the Art 2003}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}153.pdf},
year = {2003}
}
@inproceedings{Magnusson2006,
address = {Paris, France},
author = {Magnusson, Thor},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {actors,creative tools,hci,interaction design,interaction models,interfaces,mapping,osc,semiotics},
pages = {162--167},
title = {{Screen-Based Musical Interfaces as Semiotic Machines}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}162.pdf},
year = {2006}
}
@inproceedings{Kim2011,
address = {Oslo, Norway},
author = {Kim, Seunghun and Kim, Luke K and Jeong, Songhee and Yeo, Woon Seung},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {clothesline installation,metaphor,musical interface},
number = {June},
pages = {60--63},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Clothesline as a Metaphor for a Musical Interface}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}060.pdf},
year = {2011}
}
@inproceedings{Pashenkov2004,
address = {Hamamatsu, Japan},
author = {Pashenkov, Nikita},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {controllers,interaction,optical,performance,tangible interface,turntable,visualization},
pages = {64--67},
title = {{A New Mix of Forgotten Technology: Sound Generation, Sequencing and Performance Using an Optical Turntable}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}064.pdf},
year = {2004}
}
@inproceedings{Miyashita:2004,
abstract = {In this paper, we propose Thermoscore, a musical score form-that dynamically alters the temperature of the instrument/player interface. We developed the first version of theThermoscore display by lining Peltier devices on piano keys.The system is controlled by MIDI notes-on messages from anMIDI sequencer, so that a composer can design songs that aresequences of temperature for each piano key. We also discussmethodologies for composing with this system, and suggesttwo approaches. The first is to make desirable keys (or otherkeys) hot. The second one uses chroma-profile, that is, a radarchart representation of the frequency of pitch notations in the-piece. By making keys of the same chroma hot in reverse proportion to the value of the chroma-profile, it is possible to-constrain the performer's improvisation and to bring the tonality space close to a certain piece.},
address = {Hamamatsu, Japan},
author = {Miyashita, Homei and Nishimoto, Kazushi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {chroma profile,improvisation,musical score,peltier device},
pages = {104--107},
title = {{Thermoscore: A New-type Musical Score with Temperature Sensation}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}104.pdf},
year = {2004}
}
@inproceedings{nime2014:gtorre,
abstract = {In this paper we describe the application of a movement-based design process for digital musical instruments which led to the development of a prototype DMI named the Twister. The development is described in two parts. Firstly, we consider the design of the interface or physical controller. Following this we describe the development of a specific sonic character, mapping approach and performance. In both these parts an explicit consideration of the type of movement we would like the device to engender in performance drove the design choices. By considering these two parts separately we draw attention to two different levels at which movement might be considered in the design of DMIs; at a general level of ranges of movement in the creation of the controller and a more specific, but still quite open, level in the creation of the final instrument and a particular performance. In light of the results of this process the limitations of existing representations of movement within the DMI design discourse is discussed. Further, the utility of a movement focused design approach is discussed.},
address = {London, United Kingdom},
author = {Ward, Nicholas and Torre, Giuseppe},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
file = {::},
pages = {449--454},
publisher = {Goldsmiths, University of London},
title = {{Constraining Movement as a Basis for DMI Design and Performance.}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}404.pdf},
year = {2014}
}
@inproceedings{Silva2005,
address = {Vancouver, BC, Canada},
author = {Silva, Andrey R and Wanderley, Marcelo M and Scavone, Gary},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Silva, Wanderley, Scavone - 2005 - On the Use of Flute Air Jet as A Musical Control Variable.pdf:pdf},
pages = {105--108},
title = {{On the Use of Flute Air Jet as A Musical Control Variable}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}105.pdf},
year = {2005}
}
@inproceedings{Frisson2010,
address = {Sydney, Australia},
author = {Frisson, Christian and Macq, Beno{\^{i}}t and Dupont, St{\'{e}}phane and Siebert, Xavier and Tardieu, Damien and Dutoit, Thierry},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {audio database,browsing by similarity,gestural interfaces,human-computer interaction,nime10,rapid pro-,totyping},
number = {Nime},
pages = {473--476},
title = {{DeviceCycle : Rapid and Reusable Prototyping of Gestural Interfaces, Applied to Audio Browsing by Similarity}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}473.pdf},
year = {2010}
}
@inproceedings{Hahnel2010,
address = {Sydney, Australia},
author = {H{\"{a}}hnel, Tilo},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {articulation,expres-,historically informed performance,nime10,sive performance,synthetic performance},
number = {Nime},
pages = {72--75},
title = {{From Mozart to MIDI : A Rule System for Expressive Articulation}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}072.pdf},
year = {2010}
}
@inproceedings{Han:2012,
abstract = {Virtual Pottery is an interactive audiovisual piece that uses hand gesture to create 3D pottery objects and sound shape. Using the OptiTrack motion capture (Rigid Body) system at TransLab in UCSB, performers can take a glove with attached trackers, move the hand in x, y, and z axis and create their own sound pieces. Performers can also manipulate their pottery pieces in real time and change arrangement on the musical score interface in order to create a continuous musical composition. In this paper we address the relationship between body, sound and 3D shapes. We also describe the origin of Virtual Pottery, its design process, discuss its aesthetic value and musical sound synthesis system, and evaluate the overall experience.},
address = {Ann Arbor, Michigan},
author = {Han, Yoon Chung and Han, Byeong-jun},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Virtual Pottery,interactive sound installation.,motion and gesture,motion perception,pottery,sound synthesis,virtual musical instrument},
publisher = {University of Michigan},
title = {{Virtual Pottery: An Interactive Audio-Visual Installation}},
year = {2012}
}
@inproceedings{Frisson2010,
abstract = {This paper presents the development of rapid and reusablegestural interface prototypes for navigation by similarity inan audio database and for sound manipulation, using theAudioCycle application. For this purpose, we propose andfollow guidelines for rapid prototyping that we apply usingthe PureData visual programming environment. We havemainly developed three prototypes of manual control: onecombining a 3D mouse and a jog wheel, a second featuring a force-feedback 3D mouse, and a third taking advantage of the multitouch trackpad. We discuss benefits andshortcomings we experienced while prototyping using thisapproach.},
address = {Sydney, Australia},
author = {Frisson, Christian and Macq, Beno{\^{i}}t and Dupont, St{\'{e}}phane and Siebert, Xavier and Tardieu, Damien and Dutoit, Thierry},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Human-computer interaction,audio database,browsing by similarity,gestural interfaces,rapid pro- totyping},
pages = {473--476},
title = {{DeviceCycle : Rapid and Reusable Prototyping of Gestural Interfaces, Applied to Audio Browsing by Similarity}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}473.pdf},
year = {2010}
}
@inproceedings{Crevoisier2008,
abstract = {In this paper, we describe a set of hardware and software tools for creating musical controllers with any flat surface or simple object, such as tables, walls, metallic plates, wood boards, etc. The system makes possible to transform such physical objects and surfaces into virtual control interfaces, by using computer vision technologies to track the interaction made by the musician, either with the hands, mallets or sticks. These new musical interfaces, freely reconfigurable, can be used to control standard sound modules or effect processors, by defining zones on their surface and assigning them musical commands, such as the triggering of notes or the modulation of parameters.},
address = {Genoa, Italy},
author = {Crevoisier, Alain and Kellum, Greg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Computer Vision,Multi-touch Interaction,Musical Interfaces.},
pages = {113--116},
title = {{Transforming Ordinary Surfaces into Multi-touch Controllers}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}113.pdf},
year = {2008}
}
@inproceedings{Jones2007,
abstract = {Physical modeling has proven to be a successful method ofsynthesizing highly expressive sounds. However, providingdeep methods of real time musical control remains a majorchallenge. In this paper we describe our work towards aninstrument for percussion synthesis, in which a waveguidemesh is both excited and damped by a 2D matrix of forcesfrom a sensor. By emulating a drum skin both as controllerand sound generator, our instrument has reproduced someof the expressive qualities of hand drumming. Details of ourimplementation are discussed, as well as qualitative resultsand experience gleaned from live performances.},
address = {New York City, NY, United States},
author = {Jones, Randy and Schloss, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Physical modeling,expressive control,instrument design,multi-touch,performance},
pages = {27--30},
title = {{Controlling a Physical Model with a {\{}2D{\}} Force Matrix}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}027.pdf},
year = {2007}
}
@inproceedings{Hadjakos2008,
abstract = {The Elbow Piano distinguishes two types of piano touch: a touchwith movement in the elbow joint and a touch without. A playednote is first mapped to the left or right hand by visual tracking.Custom-built goniometers attached to the player's arms are usedto detect the type of touch. The two different types of touchesare sonified by different instrument sounds. This gives theplayer an increased awareness of his elbow movements, which isconsidered valuable for piano education. We have implementedthe system and evaluated it with a group of music students.},
address = {Genoa, Italy},
author = {Hadjakos, Aristotelis and Aitenbichler, Erwin and M{\"{u}}hlh{\"{a}}user, Max},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Piano,education,feedback,gesture.,sonification},
pages = {285--288},
title = {{The Elbow Piano : Sonification of Piano Playing Movements}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}285.pdf},
year = {2008}
}
@inproceedings{Verplank2005,
address = {Vancouver, BC, Canada},
author = {Verplank, William},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {256--257},
title = {{Haptic Music Exercises}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}256.pdf},
year = {2005}
}
@inproceedings{Baumann2002,
address = {Dublin, Ireland},
author = {Baumann, Alain and S{\'{a}}nchez, Rosa},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {interdisciplinary applications of,mixed media instruments},
pages = {5--9},
title = {{Interdisciplinary Applications of New Instruments}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}005.pdf},
year = {2002}
}
@inproceedings{Dobda:2013,
abstract = {Breaking musical and creative expression into elements, layers, and formulas, we explore how live listeners create unique sonic experiences from a palette of these elements and their interactions. Bringing us to present-day creative applications, a social and historical overview of silent disco is presented. The advantages of this active listening interface are outlined by the author's expressions requiring discrete elements, such as binaural beats, 3D audio effects, and multiple live music acts in the same space. Events and prototypes as well as hardware and software proposals for live multi-listener manipulation of multielemental sound and music are presented. Examples in audio production, sound healing, music composition, tempo phasing, and spatial audio illustrate the applications.},
address = {Daejeon, Republic of Korea},
author = {Dobda, Russell Eric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {active music listening,binaural beats,headphone concert,mobile clubbing,multi-track audio,music production,silent disco,smart-phone apps,sound healing,wireless headphones},
pages = {69--72},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Applied and Proposed Installations with Silent Disco Headphones for Multi-Elemental Creative Expression}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Marshall2006,
address = {Paris, France},
author = {Marshall, Mark T and Wanderley, Marcelo M},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {digital musical instruments,tactile feedback,vibro-tactile},
pages = {226--229},
title = {{Vibrotactile Feedback in Digital Musical Instruments}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}226.pdf},
year = {2006}
}
@inproceedings{Berthaut:2013,
abstract = {Digital musical instruments bring new possibilities for musical performance.They are also more complex for the audience to understand, due to the diversityof their components and the magical aspect of the musicians' actions whencompared to acoustic instruments. This complexity results in a loss of livenessand possibly a poor experience for the audience. Our approach, called Rouages,is based on a mixed-reality display system and a 3D visualization application.It reveals the mechanisms of digital musical instruments by amplifyingmusicians' gestures with virtual extensions of the sensors, by representingthe sound components with 3D shapes and specific behaviors and by showing theimpact ofmusicians gestures on these components. We believe that Rouages opens up newperspectives to help instrument makers and musicians improve audienceexperience with their digital musical instruments.},
address = {Daejeon, Republic of Korea},
author = {Berthaut, Florent and Marshall, Mark T and Subramanian, Sriram and Hachet, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {3D interface,digital musical instruments,mappings,mixed-reality,rouages},
pages = {164--169},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Rouages: Revealing the Mechanisms of Digital Musical Instruments to the Audience}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Crevoisier2005,
abstract = {Tangible Acoustic Interfaces (TAI) rely on various acousticsensing technologies, such as sound source location and acoustic imaging, to detect the position of contact of users interacting with the surface of solid materials. With their ability to transform almost any physical objects, flat or curved surfaces and walls into interactive interfaces, acoustic sensing technologies show a promising way to bring the sense of touch into the realm of computer interaction. Because music making has been closely related to this sense during centuries, an application of particular interest is the use of TAI's for the design of new musical instruments that matches the physicality and expressiveness of classical instruments. This paper gives an overview of the various acoustic-sensing technologies involved in the realisation of TAI's and develops on the motivation underlying their use for the design of new musical instruments. },
address = {Vancouver, BC, Canada},
author = {Crevoisier, Alain and Polotti, Pietro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Tangible interfaces,new musical instruments design.},
pages = {97--100},
title = {{Tangible Acoustic Interfaces and their Applications for the Design of New Musical Instruments}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}097.pdf},
year = {2005}
}
@inproceedings{Castet:2012,
abstract = {This paper presents ongoing work on methods dedicated torelations between composers and performers in the contextof experimental music. The computer music community hasover the last decade paid a strong interest on various kindsof gestural interfaces to control sound synthesis processes.The mapping between gesture and sound parameters hasspecially been investigated in order to design the most relevant schemes of sonic interaction. In fact, this relevanceresults in an aesthetic choice that encroaches on the process of composition. This work proposes to examine therelations between composers and performers in the contextof the new interfaces for musical expression. It aims to define a theoretical and methodological framework clarifyingthese relations. In this project, this paper is the first experimental study about the use of physical models as gesturalmaps for the production of textural sounds.},
address = {Ann Arbor, Michigan},
author = {Castet, Julien},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Interaction,Simulation,Sonic textures},
publisher = {University of Michigan},
title = {{Performing experimental music by physical simulation}},
year = {2012}
}
@inproceedings{Schacher:2013,
abstract = {This article documents a class that teaches gestural interaction and juxtaposestraditional instrumental skills with digital musical instrument concepts. Inorder to show the principles and reflections that informed the choices made indeveloping this syllabus, fundamental elements of an instrument-bodyrelationship and the perceptual import of sensori-motor integration areinvestigated. The methods used to let participants learn in practicalexperimental settings are discussed, showing a way to conceptualise andexperience the entire workflow from instrumental sound to electronictransformations by blending gestural interaction with digital musicalinstrument techniques and traditional instrumental playing skills. Thetechnical interfaces and software that were deployed are explained, focussingof the interactive potential offered by each solution. In an attempt tosummarise and evaluate the impact of this course, a number of insights relatingto this specific pedagogical situation are put forward. Finally, concreteexamples of interactive situations that were developed by the participants areshown in order to demonstrate the validity of this approach.},
address = {Daejeon, Republic of Korea},
author = {Schacher, Jan C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Schacher - 2013 - Hybrid Musicianship - Teaching Gestural Interaction with Traditional and Digital Instruments.pdf:pdf},
keywords = {digital musical instruments,enactive approach,gestural interaction,mapping,pedagogy},
pages = {55--60},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Hybrid Musicianship - Teaching Gestural Interaction with Traditional and Digital Instruments}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Hadjakos2008,
address = {Genoa, Italy},
author = {Hadjakos, Aristotelis and Aitenbichler, Erwin and M{\"{u}}hlh{\"{a}}user, Max},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {education,feedback,gesture,nime08,piano,sonification},
pages = {285--288},
title = {{The Elbow Piano : Sonification of Piano Playing Movements}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}285.pdf},
year = {2008}
}
@inproceedings{Lee2006a,
address = {Paris, France},
author = {Lee, Jason},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
pages = {77--78},
title = {{The Chopping Board: Real-time Sample Editor}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}077.pdf},
volume = {6},
year = {2006}
}
@inproceedings{Mann2007,
address = {New York City, NY, United States},
author = {Mann, Steve},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {all or part of,ethnomusicology,hydraulophone,is granted without fee,nime07,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,tangible user interface,this work for},
pages = {118--123},
title = {{Natural Interfaces for Musical Expression : Physiphones and a Physics-Based Organology}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}118.pdf},
year = {2007}
}
@inproceedings{Hashida2007a,
address = {New York City, NY, United States},
author = {Hashida, Tomoko and Naemura, Takeshi and Sato, Takao},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {a sense of tempo,figure 1,improvisation,input devices,interactive music,nime07},
pages = {407--408},
title = {{A System for Improvisational Musical Expression Based on Player ' s Sense of Tempo}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}407.pdf},
year = {2007}
}
@inproceedings{Freed2011,
address = {Oslo, Norway},
author = {Freed, Adrian and MacCallum, John and Schmeder, Andrew},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {composability,delegation,functional programming,gesture signal,max,msp,object,object-,open sound control,oriented programming,processing},
number = {June},
pages = {308--311},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Composability for Musical Gesture Signal Processing using new OSC-based Object and Functional Programming Extensions to Max/MSP}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}308.pdf},
year = {2011}
}
@inproceedings{Hoffman2007,
address = {New York City, NY, United States},
author = {Hoffman, Matt and Cook, Perry R},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {analysis,feature,mapping,nime07,real-time,synthesis},
pages = {309--312},
title = {{Real-Time Feature-Based Synthesis for Live Musical Performance}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}309.pdf},
year = {2007}
}
@inproceedings{Hahnel2011,
address = {Oslo, Norway},
author = {H{\"{a}}hnel, Tilo and Berndt, Axel},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {articula-,duration,dynamics,egales,loudness,notes in,synthetic performance,timing,tion},
number = {June},
pages = {48--51},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Studying Interdependencies in Music Performance : An Interactive Tool}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}048.pdf},
year = {2011}
}
@inproceedings{Nash:2012,
abstract = {This paper presents concepts, models, and empirical findings relating to liveness and flow in the user experience of systems mediated by notation. Results from an extensive two-year field study of over 1,000 sequencer and tracker users, combining interaction logging, user surveys, and a video study, are used to illustrate the properties of notations and interfaces that facilitate greater immersion in musical activities and domains, borrowing concepts from programming to illustrate the role of visual and musical feedback, from the notation and domain respectively. The Cognitive Dimensions of Notations framework and Csikszentmihalyi's flow theory are combined to demonstrate how non-realtime, notation-mediated interaction can support focused, immersive, energetic, and intrinsically- rewarding musical experiences, and to what extent they are supported in the interfaces of music production software. Users are shown to maintain liveness through a rapid, iterative edit- audition cycle that integrates audio and visual feedback.},
address = {Ann Arbor, Michigan},
author = {Nash, Chris and Blackwell, Alan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {DAWs,composition,feedback,flow,liveness,notation,performance,programming,sequencers,soundtracking,user studies},
publisher = {University of Michigan},
title = {{Liveness and Flow in Notation Use}},
year = {2012}
}
@inproceedings{Hartman2008,
address = {Genoa, Italy},
author = {Hartman, Ethan and Cooper, Jeff and Spratt, Kyle},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {nime08},
pages = {356--357},
title = {{Swing Set : Musical Controllers with Inherent Physical Dynamics}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}356.pdf},
year = {2008}
}
@inproceedings{Coduys:2004,
abstract = {High capacity of transmission lines (Ethernet in particular) is much higher than what imposed by MIDI today. So it is possible to use capturing interfaces with high-speed and high-resolution, thanks to the OSC protocol, for musical synthesis (either in realtime or non real-time). These new interfaces offer many advantages, not only in the area of musical composition with use of sensors but also in live and interactive performances. In this manner, the processes of calibration and signal processing are delocalized on a personal computer and augments possibilities of processing. In this demo, we present two hardware interfaces developed in La kitchen with corresponding processing to achieve a high-resolution, high-speed sensor processing for musical applications. },
address = {Hamamatsu, Japan},
author = {Coduys, Thierry and Henry, Cyrille and Cont, Arshia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
keywords = {Calibration,Interface,Max/MSP.,OSC,Precision,Pure Data,Sensors},
pages = {205--206},
title = {{TOASTER and KROONDE: High-Resolution and High- Speed Real-time Sensor Interfaces}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}205.pdf},
year = {2004}
}
@inproceedings{Hoffman2007,
abstract = {A crucial set of decisions in digital musical instrument design deals with choosing mappings between parameters controlled by the performer and the synthesis algorithms that actually generate sound. Feature-based synthesis offers a way to parameterize audio synthesis in terms of the quantifiable perceptual characteristics, or features, the performer wishes the sound to take on. Techniques for accomplishing such mappings and enabling feature-based synthesis to be performed in real time are discussed. An example is given of how a real-time performance system might be designed to take advantage of feature-based synthesis's ability to provide perceptually meaningful control over a large number of synthesis parameters. },
address = {New York City, NY, United States},
author = {Hoffman, Matt and Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {Analysis,Feature,Mapping,Real-time.,Synthesis},
pages = {309--312},
title = {{Real-Time Feature-Based Synthesis for Live Musical Performance}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}309.pdf},
year = {2007}
}
@inproceedings{Mann2009,
address = {Pittsburgh, PA, United States},
author = {Mann, Yotam and Lubow, Jeff and Freed, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {86--89},
title = {{The Tactus : a Tangible , Rhythmic Grid Interface Using Found-Objects}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}086.pdf},
year = {2009}
}
@inproceedings{Lee:2012c,
abstract = {This research aims to improve the correspondence between music and dance, and explores the use of human respiration pattern for musical applications with focus on the motional aspect of breathing. While respiration is frequently considered as an indicator of the metabolic state of human body that contains meaningful information for medicine or psychology, motional aspect of respiration has been relatively unnoticed in spite of its strong correlation with muscles and the brain.
This paper introduces an interactive system to control music playback for dance performances based on the respiration pattern of the dancer. A wireless wearable sensor device detects the dancer's respiration, which is then utilized to modify the dynamic of music. Two different respiration- dynamic mappings were designed and evaluated through public performances and private tests by professional choreographers. Results from this research suggest a new conceptual approach to musical applications of respiration based on the technical characteristics of music and dance.},
address = {Ann Arbor, Michigan},
author = {Lee, Jeong-seob and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {Music,correspondence,dance,interactive performance,respiration,wireless interface},
publisher = {University of Michigan},
title = {{Real-time Modification of Music with Dancer's Respiration Pattern}},
year = {2012}
}
@inproceedings{Peiper2003,
address = {Montreal, QC, Canada},
author = {Peiper, Chad and Warden, David and Garnett, Guy},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {192--196},
title = {{An Interface for Real-time Classification of Articulations Produced by Violin Bowing}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}192.pdf},
year = {2003}
}
@inproceedings{Paine2009,
address = {Pittsburgh, PA, United States},
author = {Paine, Garth and Atherton, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {324},
title = {{Fue Sho -- Electrofusion}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}324.pdf},
year = {2009}
}
@inproceedings{Hasan:2002,
abstract = {We have created a new electronic musical instrument, referred to as the Termenova (Russian for "daughter of Theremin") that combines a free-gesture capacitive sensing device with an optical sensing system that detects the reflection of a hand when it intersects a beam of an array of red lasers. The laser beams, which are made visible by a thin layer of theatrical mist, provide visual feedback and guidance to the performer to alleviate the difficulties of using a non-contact interface as well as adding an interesting component for the audience to observe. The system uses capacitive sensing to detect the proximity of the player's hands; this distance is mapped to pitch, volume, or other continuous effect. The laser guide positions are calibrated before play with position controlled servo motors interfaced to a main controller board; the location of each beam corresponds to the position where the performer should move his or her hand to achieve a pre-specified pitch and/or effect. The optical system senses the distance of the player's hands from the source of each laser beam, providing an additional dimension of musical control. },
address = {Dublin},
author = {Hasan, Leila and Yu, Nicholas and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Theremin,capacitive sensing,gesture interface,laser harp,musical controller,optical proximity sensing,servo control},
pages = {82--87},
title = {{The Termenova : A Hybrid Free-Gesture Interface}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}082.pdf},
year = {2002}
}
@inproceedings{Nichols2002,
address = {Dublin, Ireland},
author = {Nichols, Charles},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {bow,controller,haptic,hci,interface,violin},
pages = {133--136},
title = {{The vBow: Development of a Virtual Violin Bow Haptic Human-Computer Interface}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}133.pdf},
year = {2002}
}
@inproceedings{Dahl2010,
abstract = {The use of metaphor has a prominent role in HCI, both as a device to help users understand unfamiliar technologies, and as a tool to guide the design process. Creators of new computerbased instruments face similar design challenges as those in HCI. In the course of creating a new piece for Mobile Phone Orchestra we propose the metaphor of a sound as a ball and explore the interactions and sound mappings it suggests. These lead to the design of a gesture-controlled instrument that allows players to "bounce" sounds, "throw" them to other players, and compete in a game to "knock out" others' sounds. We composed the piece SoundBounce based on these interactions, and note that audiences seem to find performances of the piece accessible and engaging, perhaps due to the visibility of the metaphor. },
address = {Sydney, Australia},
author = {Dahl, Luke and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Mobile music,design,gameplay.,metaphor,performance},
pages = {178--181},
title = {{Sound Bounce : Physical Metaphors in Designing Mobile Music Performance}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}178.pdf},
year = {2010}
}
@inproceedings{Gillian2009a,
abstract = {This paper presents "Scratch-Off", a new musical multiplayer DJ game that has been designed for a mobile phone. We describe how the game is used as a test platform for experimenting with various types of multimodal feedback. The game uses movement gestures made by the players to scratch a record and control crossfades between tracks, with the objective of the game to make the correct scratch at the correct time in relation to the music. Gestures are detected using the devices built-in tri-axis accelerometer and multi-touch screen display. The players receive visual, audio and various types of vibrotactile feedback to help them make the correct scratch on the beat of the music track. We also discuss the results of a pilot study using this interface. },
address = {Pittsburgh, PA, United States},
author = {Gillian, Nicholas and O'Modhrain, Sile and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Mobile devices,audio games.,gesture},
pages = {308--311},
title = {{Scratch-Off : A Gesture Based Mobile Music Game with Tactile Feedback}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}308.pdf},
year = {2009}
}
@inproceedings{Grierson:2013,
abstract = {NoiseBear is a wireless malleable controller designed for, and in participationwith, physically and cognitively disabled children. The aim of the project wasto produce a musical controller that was robust, and flexible enough to be usedin a wide range of interactive scenarios in participatory design workshops. NoiseBear demonstrates an open ended system for designing wireless malleablecontrollers in different shapes. It uses pressure sensitive material made fromconductive thread and polyester cushion stuffing, to give the feel of a softtoy. The sensor networks with other devices using the Bluetooth Low Energyprotocol, running on a BlueGiga BLE112 chip. This contains an embedded 8051processor which manages the sensor. NoiseBear has undergone an initialformative evaluation in a workshop session with four autistic children, andcontinues to evolve in series of participatory design workshops. The evaluationshowed that controller could be engaging for the children to use, andhighlighted some technical limitations of the design. Solutions to theselimitations are discussed, along with plans for future design iterations.},
address = {Daejeon, Republic of Korea},
author = {Grierson, Mick and Kiefer, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {assistive technology,malleable controllers,multiparametric mapping},
pages = {413--416},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{NoiseBear: A Malleable Wireless Controller Designed In Participation with Disabled Children}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Cook2007,
address = {New York City, NY, United States},
author = {Cook, Andrew A and Pullin, Graham},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {1,affordance,background and problem space,cultural probes,design research,improvisation,interaction design,nime07,performance},
pages = {285--288},
title = {{Tactophonics : Your Favourite Thing Wants to Sing}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}285.pdf},
year = {2007}
}
@inproceedings{nime2014:rgupfinger,
abstract = {Street art opens a new, broad research field in the context of urban communication and sound aesthetics in public space. The primary focus of this article is the relevance and effects of using sound technologies and audio devices to shape urban landscape and soundscape. This paper examines the process of developing an alternative type of street art that uses sound as its medium. It represents multiple audio device prototypes, which encourage new chances for street artists and activists to contribute their messages and signs in public spaces. Furthermore, it documents different approaches to establishing this alternative urban practice within the street art and new media art field. The findings also expose a research space for sound and technical interventions in the context of street art.},
address = {London, United Kingdom},
author = {Gupfinger, Reinhard and Kaltenbrunner, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {577--580},
publisher = {Goldsmiths, University of London},
title = {{SOUND TOSSING Audio Devices in the Context of Street Art}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}385.pdf},
year = {2014}
}
@inproceedings{Alt2010,
address = {Sydney, Australia},
author = {Alt, Florian and Shirazi, Alireza S and Legien, Stefan and Schmidt, Albrecht and Mennen{\"{o}}h, Julian},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {instant messaging,nime10,sms,sonority,text sonification},
number = {Nime},
pages = {63--68},
title = {{Creating Meaningful Melodies from Text Messages}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}063.pdf},
year = {2010}
}
@inproceedings{OSullivan:2012,
abstract = {Development of new musical interfaces often requires experimentation with the mapping of available controller inputs to output parameters. Useful mappings for a particular application may be complex in nature, with one or more inputs being linked to one or more outputs. Existing development environments are commonly used to program such mappings, while code libraries provide powerful data-stream manipulation. However, room exists for a standalone application with a simpler graphical user interface for dynamically patching between inputs and outputs. This paper presents an early prototype version of a software tool that allows the user to route control signals in real time, using various messaging formats. It is cross-platform and runs as a standalone application in desktop and Android OS versions. The latter allows the users of mobile devices to experiment with mapping signals to and from physical computing components using the inbuilt multi-touch screen. Potential uses therefore include real-time mapping during performance in a more expressive manner than facilitated by existing tools.},
address = {Ann Arbor, Michigan},
author = {O'Sullivan, Liam and Furlong, Dermot and Boland, Frank},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Android.,Mapping,Software Tools},
publisher = {University of Michigan},
title = {{Introducing CrossMapper: Another Tool for Mapping Musical Control Parameters}},
year = {2012}
}
@inproceedings{nime2014:axambo,
abstract = {Co-located tabletop tangible user interfaces (TUIs) for music performance are known for promoting multi-player collaboration with a shared interface, yet it is still unclear how to best support the awareness of the workspace in terms of understanding individual actions and the other group members actions, in parallel. In this paper, we investigate the effects of providing auditory feedback using ambisonics spatialisation, aimed at informing users about the location of the tangibles on the tabletop surface, with groups of mixed musical backgrounds. Participants were asked to improvise music on "SoundXY4: The Art of Noise", a tabletop system that includes sound samples inspired by Russolo's taxonomy of noises. We compared spatialisation vs. no-spatialisation conditions, and findings suggest that, when using spatialisation, there was a clearer workspace awareness, and a greater engagement in the musical activity as an immersive experience.},
address = {London, United Kingdom},
author = {Xamb{\'{o}}, Anna and Roma, Gerard and Laney, Robin and Dobbyn, Chris and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {40--45},
publisher = {Goldsmiths, University of London},
title = {{SoundXY4: Supporting Tabletop Collaboration and Awareness with Ambisonics Spatialisation}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}471.pdf},
year = {2014}
}
@inproceedings{Tindale2007,
address = {New York City, NY, United States},
author = {Tindale, Adam R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {electronic percussion,nime07,physical modeling,timbre recogni-},
pages = {392--393},
title = {{A Hybrid Method for Extended Percussive Gesture}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}392.pdf},
year = {2007}
}
@inproceedings{nime2014:smealla,
abstract = {The maturation process of the NIME field has brought a growing interest in teaching the design and implementa- tion of Digital Music Instruments (DMI) as well as in find- ing objective evaluation methods to assess the suitability of these outcomes. In this paper we propose a methodology for teaching NIME design and a set of tools meant to inform the design process. This approach has been applied in a master course focused on the exploration of expressiveness and on the role of the mapping component in the NIME cre- ation chain, through hands-on and self-reflective approach based on a restrictive setup consisting of smart-phones and the Pd programming language. Working Groups were formed, and a 2-step DMI design process was applied, including 2 performance stages. The evaluation tools assessed both System and Performance as- pects of each project, according to Listeners' impressions after each performance. Listeners' previous music knowl- edge was also considered. Through this methodology, stu- dents with different backgrounds were able to effectively engage in the NIME design processes, developing working DMI prototypes according to the demanded requirements; the assessment tools proved to be consistent for evaluating NIMEs systems and performances, and the fact of inform- ing the design processes with the outcome of the evaluation, showed a traceable progress in the students outcomes.},
address = {London, United Kingdom},
author = {Jord{\`{a}}, Sergi and Mealla, Sebastian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
keywords = {evaluation},
mendeley-tags = {evaluation},
pages = {233--238},
publisher = {Goldsmiths, University of London},
title = {{A Methodological Framework for Teaching, Evaluating and Informing NIME Design with a Focus on Mapping and Expressiveness}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}472.pdf},
year = {2014}
}
@inproceedings{Klugel:2013,
abstract = {Controlling the timbre generated by an audio synthesizerin a goal-oriented way requires a profound understandingof the synthesizer's manifold structural parameters. Especially shapingtimbre expressively to communicate emotional affect requires expertise.Therefore, novices in particular may not be able to adequately control timbrein viewof articulating the wealth of affects musically. In this context, the focus ofthis paper is the development of a model that can represent a relationshipbetween timbre and an expected emotional affect . The results of the evaluationof the presented model are encouraging which supports its use in steering oraugmenting the control of the audio synthesis. We explicitly envision thispaper as a contribution to the field of Synthesis by Analysis in the broadersense, albeit being potentially suitable to other related domains.},
address = {Daejeon, Republic of Korea},
author = {Kl{\"{u}}gel, Niklas and Groh, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {Analysis by Synthesis,Deep Belief Networks,Emotional affect,Machine Learning,Timbre},
pages = {525--530},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Towards Mapping Timbre to Emotional Affect}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Griffith2002,
address = {Dublin, Ireland},
author = {Griffith, Niall J and O'Leary, Sean and O'Shea, Donagh and Hammond, Ed and O'Modhrain, Sile},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {collaboration,gesture,metaphor,performance},
pages = {71--72},
title = {{Circles and Seeds: Adapting Kpelle Ideas about Music Performance for Collaborative Digital Music performance}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}071.pdf},
year = {2002}
}
@inproceedings{Dobda:2013,
abstract = {Breaking musical and creative expression into elements, layers, and formulas, we explore how live listeners create unique sonic experiences from a palette of these elements and their interactions. Bringing us to present-day creative applications, a social and historical overview of silent disco is presented. The advantages of this active listening interface are outlined by the author's expressions requiring discrete elements, such as binaural beats, 3D audio effects, and multiple live music acts in the same space. Events and prototypes as well as hardware and software proposals for live multi-listener manipulation of multielemental sound and music are presented. Examples in audio production, sound healing, music composition, tempo phasing, and spatial audio illustrate the applications.},
address = {Daejeon, Republic of Korea},
author = {Dobda, Russell Eric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {active music listening,binaural beats,headphone concert,mobile clubbing,multi-track audio,music production,silent disco,smart-phone apps,sound healing,wireless headphones},
pages = {69--72},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Applied and Proposed Installations with Silent Disco Headphones for Multi-Elemental Creative Expression}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Stahl2010,
address = {Sydney, Australia},
author = {Stahl, Alex and Clemens, Patricia},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {magnetostrictive flextensional transducer,nime10,paralinguistics,sound reinforcement,spatialization,speech enhancement,transformation,voice,wearable systems},
number = {Nime},
pages = {15--18},
title = {{Auditory Masquing : Wearable Sound Systems for Diegetic Character Voices}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}427.pdf},
year = {2010}
}
@inproceedings{Astrinaki:2013,
abstract = {This paper describes the recent progress in our approach to generateperformative and controllable speech. The goal of the performative HMM-basedspeech and singing syn- thesis library, called Mage, is to have the ability togenerate natural sounding speech with arbitrary speaker's voicecharacteristics, speaking styles and expressions and at the same time to haveaccurate reactive user control over all the available production levels. Mageallows to arbitrarily change between voices, control speaking style or vocalidentity, manipulate voice characteristics or alter the targeted contexton-the-fly and also maintain the naturalness and intelligibility of the output.To achieve these controls, it was essential to redesign and improve the initiallibrary. This paper focuses on the improvements of the architectural design,the additional user controls and provides an overview of a prototype, where aguitar is used to reactively control the generation of a synthetic voice invarious levels.},
address = {Daejeon, Republic of Korea},
author = {Astrinaki, Maria and D'Alessandro, Nicolas and Reboursi{\`{e}}re, Lo{\"{i}}c and Moinet, Alexis and Dutoit, Thierry},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {augmented guitar,hexaphonic guitar,speech synthesis},
pages = {547--550},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{MAGE 2.0: New Features and its Application in the Development of a Talking Guitar}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Ciglar2010,
abstract = {This paper, describes the second phase of an ongoing research project dealing with the implementation of an interactive interface. It is a "hands free" instrument, utilizing a non-contact tactile feedback method based on airborne ultrasound. The three main elements/components of the interface that will be discussed in this paper are: 1. Generation of audible sound by self-demodulation of an ultrasound signal during its propagation through air; 2. The condensation of the ultrasound energy in one spatial point generating a precise tactile reproduction of the audible sound; and 3. The feed-forward method enabling a real-time intervention of the musician, by shaping the tactile (ultra)sound directly with his hands.},
address = {Sydney, Australia},
author = {Ciglar, Miha},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {feedback,hands-free interface,haptics,nonlinear acoustics,parametric array.,ultrasound,vibro-tactility},
pages = {19--22},
title = {{An Ultrasound Based Instrument Generating Audible and Tactile Sound}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}019.pdf},
year = {2010}
}
@inproceedings{Zappi2010,
address = {Sydney, Australia},
author = {Zappi, Victor and Brogni, Andrea and Caldwell, Darwin},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {con-,glove device,music controller,nime10,osc,virtual reality},
number = {Nime},
pages = {297--302},
title = {{OSC Virtual Controller}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}297.pdf},
year = {2010}
}
@inproceedings{Breinbjerg2006,
abstract = {In this paper we describe the intentions, the design and functionality of an Acousmatic Composition Environment that allows children or musical novices to educate their auditory curiosity by recording, manipulating and mixing sounds of everyday life. The environment consists of three stands: A stand for sound recording with a soundproof box that ensure good recording facilities in a noisy environment; a stand for sound manipulation with five simple, tangible interfaces; a stand for sound mixing with a graphical computer interface presented on two touch screens. },
address = {Paris, France},
author = {Breinbjerg, Morten and Caprani, Ole and Lunding, Rasmus and Kramhoft, Line},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {Acousmatic listening,aesthetics,tangible interfaces.},
pages = {334--337},
title = {{An Acousmatic Composition Environment}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}334.pdf},
year = {2006}
}
@inproceedings{Dahl2007,
address = {New York City, NY, United States},
author = {Dahl, Luke and Whetsell, Nathan and {Van Stoecker}, John},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {flex sensors,musical controller,nime07,puredata,scanned synthesis},
pages = {270--272},
title = {{The WaveSaw : A Flexible Instrument for Direct Timbral Manipulation}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}270.pdf},
year = {2007}
}
@inproceedings{Flety2002,
address = {Dublin, Ireland},
author = {Fl{\'{e}}ty, Emmanuel},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {basically used for connecting,gestural controller,has been created for,it was,master keyboards to sound,midi,music,sensor,that purpose,the midi standard 1},
pages = {59--64},
title = {{AtoMIC Pro: a Multiple Sensor Acquisition Device}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}059.pdf},
year = {2002}
}
@inproceedings{Freed2006a,
address = {Paris, France},
author = {Freed, Adrian and Wessel, David and Zbyszynski, Michael and Uitti, Frances M},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Freed et al. - 2006 - Augmenting the Cello.pdf:pdf},
keywords = {cello,chordophone,double bowing,double stops,encoder,fsr,rotary absolute position,triple stops},
pages = {409--413},
title = {{Augmenting the Cello}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}409.pdf},
year = {2006}
}
@inproceedings{Kruge2011,
address = {Oslo, Norway},
author = {Kruge, Nick and Wang, Ge},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {audiovisual,crowdsourcing,ipad,iphone,mobile music,networked music,sample,sampling,social music,station,user-generated content},
number = {June},
pages = {185--190},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{MadPad : A Crowdsourcing System for Audiovisual Sampling}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}185.pdf},
year = {2011}
}
@inproceedings{Lyons2003,
abstract = {The role of the face and mouth in speech production as well asnon-verbal communication suggests the use of facial action tocontrol musical sound. Here we document work on theMouthesizer, a system which uses a headworn miniaturecamera and computer vision algorithm to extract shapeparameters from the mouth opening and output these as MIDIcontrol changes. We report our experience with variousgesture-to-sound mappings and musical applications, anddescribe a live performance which used the Mouthesizerinterface.},
address = {Montreal},
author = {Lyons, Michael J and Haehnel, Michael and Tetsutani, Nobuji},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
keywords = {Video-based interface; mouth controller; alternati},
pages = {116--121},
title = {{Designing, Playing, and Performing with a Vision-based Mouth Interface}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}116.pdf},
year = {2003}
}
@inproceedings{Hong2009,
address = {Pittsburgh, PA, United States},
author = {Hong, Min Eui},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {322},
title = {{Cosmic Strings II}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}322.pdf},
year = {2009}
}
@inproceedings{Rosenbaum2011,
address = {Oslo, Norway},
author = {Rosenbaum, Eric},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {improvisation,ipad,melody,multi-touch,representation},
number = {June},
pages = {445--447},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{MelodyMorph: A Reconfigurable Musical Instrument}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}445.pdf},
year = {2011}
}
@inproceedings{Jackie:2013,
abstract = {SoloTouch is a guitar inspired pocket sized controller system that consists ofa capacitive touch trigger and a lick-based note selector. The touch triggerallows an intuitive way to play both velocity sensitive notes and vibratoexpressively using only one finger. The lick-based note selector is an originalconcept that provides the player an easy way to play expressive melodic linesby combining pre-programmed ``licks'' without the need to learn the actualnotes. The two-part controller is primarily used as a basic MIDI controller forplaying MIDI controlled virtual instruments, normally played by keyboardcontrollers. The controller is targeted towards novice musicians, playerswithout prior musical training could play musical and expressive solos,suitable for improvised jamming along modern popular music.},
address = {Daejeon, Republic of Korea},
author = {Jackie and Chui, Yi Tang and Marafa, Mubarak and Samson and Young, Ka Fai},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Capacitive touch controller,automated note selector,novice musicians.,virtual instrument MIDI controller},
pages = {389--393},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{SoloTouch: A Capacitive Touch Controller with Lick-based Note Selector}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Wynnychuk2002,
address = {Dublin, Ireland},
author = {Wynnychuk, Jordan and Porcher, Richard and Brajovic, Lucas and Brajovic, Marko and Platas, Nacho},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {3d music interface,3d sound,analogue input controllers,audio,audio visualization,digital architecture,hybrid environments,localization,video game navigation},
pages = {154--155},
title = {{sutoolz 1.0 alpha : 3D Software Music Interface}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}154.pdf},
year = {2002}
}
@inproceedings{B2007,
address = {New York City, NY, United States},
author = {Bottcher, Niels and Gelineck, Steven and Serafin, Stefania},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {excitation,feel of the physmism,figure 1,hybrid instruments,inspired by old analogue,nime07,physical models,resonator,the final look and,was among other things},
pages = {31--36},
title = {{PHYSMISM : A Control Interface for Creative Exploration of Physical Models}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}031.pdf},
year = {2007}
}
@inproceedings{Guisan2005,
address = {Vancouver, BC, Canada},
author = {Guisan, Alain C},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {270},
title = {{Interactive Sound Installation: INTRIUM}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}270.pdf},
year = {2005}
}
@inproceedings{nime2014:hdiao,
abstract = {Sketching is a natural way for one person to convey their thoughts and intentions to another. With the recent rise of tablet-based computing, the use of sketching as a control and interaction paradigm is one that deserves exploration. In this paper we present an interactive sketch-based music composition and performance system called Drawchestra. The aim of the system is to give users an intuitive way to convey their musical ideas to a computer system with the minimum of technical training thus enabling them to focus on the creative tasks of composition and performance. The system provides the user with a canvas upon which they may create their own instruments by sketching shapes on the tablet screen. The system recognises a certain set of shapes which it treats as virtual instruments or effects. Once recognised, these virtual instruments can then be play- ed by the user in real time. The size of a sketched in- strument shape is used to control certain parameters of the sound so the user can build complex orchestras contain- ing many different shapes of different sizes. The sketched shapes may also be moved and resized as desired making it possible to customise and edit the virtual orchestra as the user goes along. The system has been implemented in Python and user tests conducted using an iPad as the control surface. We report the results of the user study at the end of the paper before briefly discussing the outcome and outlining the next steps for the system design.},
address = {London, United Kingdom},
author = {Diao, Haojing and Zhou, Yanchao and Harte, Christopher Andrew and Bryan-Kinns, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {569--572},
publisher = {Goldsmiths, University of London},
title = {{Sketch-Based Musical Composition and Performance}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}517.pdf},
year = {2014}
}
@inproceedings{Hindle:2013,
abstract = {Audience participation in computer music has long been limited byresources such as sensor technology or the material goods necessary toshare such an instrument. A recent paradigm is to take advantageof the incredible popularity of the smart-phone, a pocket sizedcomputer, and other mobile devices, to provide the audience aninterface into a computer music instrument. In this paper we discuss amethod of sharing a computer music instrument's interface with anaudience to allow them to interact via their smartphone. We propose amethod that is relatively cross-platform and device-agnostic, yetstill allows for a rich user-interactive experience. By emulating acaptive-portal or hotspot we reduce the adoptability issues and configurationproblems facing performers and their audience. We share ourexperiences with this system, as well as an implementation of thesystem itself.},
address = {Daejeon, Republic of Korea},
author = {Hindle, Abram},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Adoption,Audience Interaction,Captive Portal,Hotspot,Multi-User,Smartphone,Wifi},
pages = {174--179},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{SWARMED: Captive Portals, Mobile Devices, and Audience Participation in Multi-User Music Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Allison2005,
address = {Vancouver, BC, Canada},
author = {Allison, Jesse T and Place, Timothy},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {56--59},
title = {{Teabox: A Sensor Data Interface System}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}056.pdf},
year = {2005}
}
@inproceedings{Freed2006,
address = {Paris, France},
author = {Freed, Adrian and Avizienis, Rimas and Wright, Matthew},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
pages = {97--100},
title = {{Beyond 0-5V: Expanding Sensor Integration Architectures}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}097.pdf},
year = {2006}
}
@inproceedings{Garcia2011,
abstract = {This paper presents a study of blowing pressure profilesacquired from recorder playing. Blowing pressure signalsare captured from real performance by means of a a lowintrusiveness acquisition system constructed around commercial pressure sensors based on piezoelectric transducers.An alto recorder was mechanically modified by a luthierto allow the measurement and connection of sensors whilerespecting playability and intrusiveness. A multi-modaldatabase including aligned blowing pressure and sound signals is constructed from real practice, covering the performance space by considering different fundamental frequencies, dynamics, articulations and note durations. Once signals were pre-processed and segmented, a set of temporalenvelope features were defined as a basis for studying andconstructing a simplified model of blowing pressure profilesin different performance contexts.},
address = {Oslo, Norway},
author = {Garc{\'{i}}a, Francisco and Vinceslas, Leny and Tubau, Josep and Maestre, Esteban},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {blowing,instrumental gesture,multi-modal data,pressure,recorder,wind instrument},
pages = {124--127},
title = {{Acquisition and Study of Blowing Pressure Profiles in Recorder Playing}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}124.pdf},
year = {2011}
}
@inproceedings{Lanzalone2008,
address = {Genoa, Italy},
author = {Lanzalone, Silvia},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {nime08},
pages = {273--276},
title = {{The 'Suspended Clarinet' with the 'Uncaused Sound' : Description of a Renewed Musical Instrument}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}273.pdf},
year = {2008}
}
@inproceedings{Talmudi2004,
address = {Hamamatsu, Japan},
author = {Talmudi, Assaf K},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {1,acting as a soul,and its pneumatic,decentralized processes,emergence,genetic algorithms,instrument dualism,score,with its paper rolls},
pages = {43--46},
title = {{The Decentralized Pianola: Evolving Mechanical Music Instruments using a Genetic Algorithm}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}043.pdf},
year = {2004}
}
@inproceedings{Yonezawa2005,
address = {Vancouver, BC, Canada},
author = {Yonezawa, Tomoko and Suzuki, Takahiko and Mase, Kenji and Kogure, Kiyoshi},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {121--126},
title = {{HandySinger : Expressive Singing Voice Morphing using Personified Hand-puppet Interface}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}121.pdf},
year = {2005}
}
@inproceedings{Overholt:2012,
abstract = {The Create USB Interface is an open source microcontroller board that can be programmed in C, BASIC, or Arduino languages. The latest version is called the CUI32Stem, and it is designed to work `hand-in-hand' with the GROVE prototyping system that includes a wide range of sensors and actuators. It utilizes a high-performance Microchip{\{}$\backslash$textregistered{\}} PIC32 microcontroller unit to allow programmable user interfaces. Its development and typical uses are described, focusing on musical interaction design scenarios. Several options for wireless connectivity are described as well, enabling the CUI32Stem to pair with a smartphone and/or a normal computer. Finally, SeeedStudio's GROVE system is explained, which provides a prototyping system comprised of various elements that incorporate simple plugs, allowing the CUI32Stem to easily connect to the growing collection of open source GROVE transducers.},
address = {Ann Arbor, Michigan},
author = {Overholt, Dan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Overholt - 2012 - Musical Interaction Design with the CUI32Stem Wireless Options and the GROVE system for prototyping new interfaces.pdf:pdf},
keywords = {802.11g,Arduino language,Bluetooth,CUI32,CUI32Stem,Microchip PIC32,Microcontroller,Musical Interaction Design,NIME education,Open Sound Control,StickOS BASIC,Wifi,Wireless,Zigflea},
publisher = {University of Michigan},
title = {{Musical Interaction Design with the CUI32Stem: Wireless Options and the GROVE system for prototyping new interfaces}},
year = {2012}
}
@inproceedings{Gaye2003,
abstract = {In the project Sonic City, we have developed a system thatenables users to create electronic music in real time by walkingthrough and interacting with the urban environment. Weexplore the use of public space and everyday behaviours forcreative purposes, in particular the city as an interface andmobility as an interaction model for electronic music making.A multi-disciplinary design process resulted in theimplementation of a wearable, context-aware prototype. Thesystem produces music by retrieving information aboutcontext and user action and mapping it to real-time processingof urban sounds. Potentials, constraints, and implications ofthis type of music creation are discussed.},
address = {Montreal},
author = {Gaye, Lalya and Maz{\'{e}}, Ramia and Holmquist, Lars E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
pages = {109--115},
title = {{Sonic City: The Urban Environment as a Musical Interface}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}109.pdf},
year = {2003}
}
@inproceedings{Dezfouli:2013,
abstract = {Notesaaz is both a new physical interface meant for musical performance and aproposal for a three-stage process where the controller is used to navigatewithin a graphical score that on its turn controls the sound generation. It canbe seen as a dynamic and understandable way of using dynamic mapping betweenthe sensor input and the sound generation. Furthermore by presenting thegraphical score to both the performer and the audience a new engagement of theaudience can be established.},
address = {Daejeon, Republic of Korea},
author = {Dezfouli, Erfan Abdi and van der Heide, Edwin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {custom controller,dynamic score,gestural input,musical instrument},
pages = {115--117},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Notesaaz: a new controller and performance idiom}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Harriman:2012,
abstract = {This paper provides an overview of a new method for approaching beat sequencing. As we have come to know them drum machines provide means to loop rhythmic patterns over a certain interval. Usually with the option to specify different beat divisions. What I developed and propose for consideration is a rethinking of the traditional drum machine confines. The Sinkapater is an untethered beat sequencer in that the beat division, and the loop length can be arbitrarily modified for each track. The result is the capability to create complex syncopated patterns which evolve over time as different tracks follow their own loop rate. To keep cohesion all channels can be locked to a master channel forcing a loop to be an integer number of "Master Beats". Further a visualization mode enables exploring the patterns in another new way. Using synchronized OpenGL a 3-Dimensional environment visualizes the beats as droplets falling from faucets of varying heights determined by the loop length. Waves form in the bottom as beats splash into the virtual "sink". By combining compelling visuals and a new approach to sequencing a new way of exploring beats and experiencing music has been created.},
address = {Ann Arbor, Michigan},
author = {Harriman, Jiffer},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {NIME,drum machine,proceedings,sequencer,visualization},
publisher = {University of Michigan},
title = {{Sinkapater - An Untethered Beat Sequencer}},
year = {2012}
}
@inproceedings{Lee2004,
address = {Hamamatsu, Japan},
author = {Lee, Eric and Nakra, Teresa M and Borchers, Jan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {conducting systems,design patterns,gesture recogni-,interactive exhibits,real-time audio stretching,tion},
pages = {68--73},
title = {{You're The Conductor: A Realistic Interactive Conducting System for Children}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}068.pdf},
year = {2004}
}
@inproceedings{Tahiroglu:2013,
abstract = {This paper introduces a novel collaborative environment (PESI) in whichperformers are not only free to move and interact with each other but wheretheir social interactions contribute to the sonic outcome. PESI system isdesigned for co-located collaboration and provides embodied and spatialopportunities for musical exploration. To evaluate PESI with skilled musicians,a user-test jam session was conducted. Musicians' comments indicate that thesystem facilitates group interaction finely to bring up further intentions tomusical ideas. Results from our user-test jam session indicate that, through some modificationof the 'in-space' response to the improvisation, and through more intuitiveinteractions with the 'on-body' mobile instruments, we could make thecollaborative music activity a more engaging and active experience. Despitebeing only user-tested once with musicians, the group interview has raisedfruitful discussions on the precise details of the system components.Furthermore, the paradigms of musical interaction and social actions in groupactivities need to be questioned when we seek design requirements for such acollaborative environment. We introduced a system that we believe can open upnew ways of musical exploration in group music activity with a number ofmusicians. The system brings up the affordances of accessible technologieswhile creating opportunities for novel design applications to be explored. Ourresearch proposes further development of the system, focusing on movementbehavior in long-term interaction between performers. We plan to implement thisversion and evaluate design and implementation with distinct skilled musicians.},
address = {Daejeon, Republic of Korea},
author = {Tahiro$\backslash$u glu, Koray and Correia, Nuno N and Espada, Miguel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Affordances,NIME,collaboration,extended system,mobile music,social interaction},
pages = {35--40},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{PESI Extended System: In Space, On Body, with 3 Musicians}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Lai:2013,
abstract = {This paper presents observations from investigating audience experience of apractice-based research in live sound performance with electronics. In seekingto understand the communication flow and the engagement between performer andaudience in this particular performance context, we designed an experiment thatinvolved the following steps: (a) performing WOSAWIP at a new media festival,(b) conducting a qualitative research study with audience members and (c)analyzing the data for new insights.},
address = {Daejeon, Republic of Korea},
author = {Lai, Chi-Hsia and Bovermann, Till},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Audience Experience Study,Evaluation,Live Performance,Research Methods},
pages = {170--173},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Audience Experience in Sound Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{arenaud:2014,
abstract = {This paper provides an overview of a proposed demonstration of 3DinMotion, a system using real time motion capture of one or several subjects, which can be used in interactive audiovisual pieces and network performances. The skeleton of a subject is analyzed in real time and displayed as an abstract avatar as well as sonified based on mappings and rules to make the interplay experience lively and rewarding. A series of musical pieces have been composed for the interface following cueing strategies. In addition a second display, ``the prompter" guides the users through the piece. 3DinMotion has been developed from scratch and natively, leading to a system with a very low latency, making it suitable for real time music interactions. In addition, 3DinMotion is fully compatible with the OpenSoundControl (OSC) protocol, allowing expansion to commonly used musical and sound design applications.},
address = {London, United Kingdom},
author = {Renaud, Alain and Charbonnier, Caecilia and Chagu{\'{e}}, Sylvain},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {495--496},
publisher = {Goldsmiths, University of London},
title = {{{\{}3D{\}}inMotion A Mocap Based Interface for Real Time Visualisation and Sonification of Multi-User Interactions}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}423.pdf},
year = {2014}
}
@inproceedings{Gurevich2001,
address = {Seattle, WA, United States},
author = {Gurevich, Michael and von Muehlen, Stephan},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
file = {::},
keywords = {computer music,electronic musical instruments,human computer interface,interactive music,midi controllers,musical instrument design},
pages = {27--29},
title = {{The Accordiatron : A MIDI Controller For Interactive Music}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}027.pdf},
year = {2001}
}
@inproceedings{Loewenstein2007,
address = {New York City, NY, United States},
author = {Loewenstein, Stefan},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {405--406},
title = {{"Acoustic Map"  An Interactive Cityportrait}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}405.pdf},
year = {2007}
}
@inproceedings{Knorig2007,
abstract = {In this paper we present the concept and prototype of a new musical interface that utilizes the close relationship between gestural expression in the act of painting and that of playing a musical instrument in order to provide non-musicians the opportunity to create musical expression. A physical brush on a canvas acts as the instrument. The characteristics of its stroke are intuitively mapped to a conductor program, defining expressive parameters of the tone in real-time. Two different interaction modes highlight the importance of bodily expression in making music as well as the value of a metaphorical visual representation.},
address = {New York City, NY, United States},
author = {Kn{\"{o}}rig, Andr{\'{e}} and M{\"{u}}ller, Boris and Wettach, Reto},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {expressive gesture,musical education,musical expression,musical interface,natural interface},
pages = {384--385},
title = {{Articulated Paint : Musical Expression for Non-Musicians}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}384.pdf},
year = {2007}
}
@inproceedings{Canazza2008,
abstract = {Musical open works can be often thought like sequences of musical structures, which can be arranged by anyone who had access to them and who wished to realize the work. This paper proposes an innovative agent-based system to model the information and organize it in structured knowledge; to create effective, graph-centric browsing perspectives and views for the user; to use ,
,
authoring tools for the performance of open work of electro-acoustic music. },
address = {Genoa, Italy},
author = {Canazza, Sergio and Dattolo, Antonina},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Multimedia Information Systems,Musical Open Work,Software Agents,zz-structures.},
pages = {140--143},
title = {{New Data Structure for Old Musical Open Works}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}140.pdf},
year = {2008}
}
@inproceedings{Burtner2003,
address = {Montreal, QC, Canada},
author = {Burtner, Matthew},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Burtner - 2003 - Composing for the (dis)Embodied Ensemble Notational Systems in (dis)Appearances.pdf:pdf},
pages = {63--69},
title = {{Composing for the (dis)Embodied Ensemble : Notational Systems in (dis)Appearances}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}063.pdf},
year = {2003}
}
@inproceedings{Ferguson2006,
address = {Paris, France},
author = {Ferguson, Sam},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Lyons, Michael and Tanaka, Atau},
keywords = {interactive sonification,music,sonification,sound visualization},
pages = {384--389},
title = {{Learning Musical Instrument Skills Through Interactive Sonification}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}384.pdf},
year = {2006}
}
@inproceedings{Weinberg2009,
address = {Pittsburgh, PA, United States},
author = {Weinberg, Gil and Beck, Andrew and Godfrey, Mark},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {gestural control,mobile music,nime09},
pages = {312--315},
title = {{ZooZBeat : a Gesture-based Mobile Music Studio}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}312.pdf},
year = {2009}
}
@inproceedings{Hadjakos:2013,
abstract = {Music ensembles have to synchronize themselves with a very high precision inorder to achieve the desired musical results. For that purpose the musicians donot only rely on their auditory perception but also perceive and interpret themovements and gestures of their ensemble colleges. In this paper we present aKinect-based method to analyze ensemble play based on head tracking. We discussfirst experimental results with a violin duo performance.},
address = {Daejeon, Republic of Korea},
author = {Hadjakos, Aristotelis and Grosshauser, Tobias},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Cross-Correlogram,Ensemble,Functional Data Analysis,Kinect,Strings,Synchronization},
pages = {106--110},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Motion and Synchronization Analysis of Musical Ensembles with the Kinect}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Dahl2011,
abstract = {TweetDreams is an instrument and musical compositionwhich creates real-time sonification and visualization oftweets. Tweet data containing specified search terms is retrieved from Twitter and used to build networks of associated tweets. These networks govern the creation of melodiesassociated with each tweet and are displayed graphically.Audience members participate in the piece by tweeting,and their tweets are given special musical and visual prominence.},
address = {Oslo, Norway},
author = {Dahl, Luke and Herrera, Jorge and Wilkerson, Carr},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Twitter,audience participation,data visualization,interaction,multi-user instrument.,sonification,text processing},
pages = {272--275},
title = {{TweetDreams : Making Music with the Audience and the World using Real-time Twitter Data}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}272.pdf},
year = {2011}
}
@inproceedings{Min2009,
abstract = {It is surely not difficult for anyone with experience in thesubject known as Music Theory to realize that there is avery definite and precise relationship between music andmathematics. This paper describes the SoriSu, a newelectronic musical instrument based on Sudoku puzzles,which probe the expressive possibilities of mathematicalconcepts in music. The concept proposes a new way ofmapping numbers to sound. This interface was designed toprovide easy and pleasing access to music for users whoare unfamiliar or uncomfortable with current musicaldevices. The motivation behind the project is presented, aswell as hardware and software design.},
address = {Pittsburgh, PA, United States},
author = {Min, Hye Ki},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Game Interfaces,Mathematics and Sound,Mathematics in Music,Numbers,Puzzles,Tangible User Interfaces.},
pages = {82--85},
title = {{SORISU : Sound with Numbers}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}082.pdf},
year = {2009}
}
@inproceedings{Bokowiec2011,
address = {Oslo, Norway},
author = {Bokowiec, Mark A},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {bodycoder,collaboration,expressivity,gestural control,interactive performance mechanisms,kinaesonics},
number = {June},
pages = {40--43},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{V'OCT (Ritual): An Interactive Vocal Work for Bodycoder System and 8 Channel Spatialization}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}040.pdf},
year = {2011}
}
@inproceedings{nime2014:tmudd,
abstract = {This paper examines electronic instruments that are based on dynamical systems, where the behaviour of the instru- ment depends not only upon the immediate input to the in- strument, but also on the past input. Five instruments are presented as case studies: Michel Waisvisz' Cracklebox, Dy- lan Menzies' Spiro, no-input mixing desk, the author's Feed- back Joypad, and microphone-loudspeaker feedback. Links are suggested between the sonic affordances of each instru- ment and the dynamical mechanisms embedded in them. This is discussed in the context of contemporary, material- oriented approaches to composition and particularly to free improvisation where elements such as unpredictability and instability are often of interest, and the process of explo- ration and discovery is an important part of the practice. Links are also made with the use of dynamical interactions in computer games to produce situations in which slight variations in the timing and ordering of inputs can lead to very different outcomes, encouraging similarly explorative approaches.},
address = {London, United Kingdom},
author = {Mudd, Tom and Holland, Simon and Mulholland, Paul and Dalton, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {126--129},
publisher = {Goldsmiths, University of London},
title = {{Dynamical Interactions with Electronic Instruments}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}302.pdf},
year = {2014}
}
@inproceedings{Bouillot2008,
address = {Genoa, Italy},
author = {Bouillot, Nicolas and Wozniewski, Mike and Settel, Zack and Cooperstock, Jeremy R},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {nime08},
pages = {189--192},
title = {{A Mobile Wireless Augmented Guitar}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}189.pdf},
year = {2008}
}
@inproceedings{Bencina2005,
address = {Vancouver, BC, Canada},
author = {Bencina, Ross},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {computational geometry,design,design support,high-level control,interpolation,mapping,of interpo-,this section reviews related,user interface,work in the field},
pages = {101--104},
title = {{The Metasurface -- Applying Natural Neighbour Interpolation to Two-to-Many Mapping}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}101.pdf},
year = {2005}
}
@inproceedings{nime2014:avanzandt,
abstract = {When performing a piece, a pianist's interpretation is communicated both through the sound produced and through body gestures. We present PiaF (Piano Follower), a prototype for augmenting piano performance by measuring gesture variations. We survey other augmented piano projects, several of which focus on gestural recognition, and present our prototype which uses machine learning techniques for gesture classification and estimation of gesture variations in real-time. Our implementation uses the Kinect depth sensor to track body motion in space, which is used as input data. During an initial learning phase, the system is taught a set of reference gestures, or templates. During performance, the live gesture is classified in real-time, and variations with respect to the recognized template are computed. These values can then be mapped to audio processing parameters, to control digital effects which are applied to the acoustic output of the piano in real-time. We discuss initial tests using PiaF with a pianist, as well as potential applications beyond live performance, including pedagogy and embodiment of recorded performance.},
address = {London, United Kingdom},
author = {Zandt-Escobar, Alejandro Van and Caramiaux, Baptiste and Tanaka, Atau},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {167--170},
publisher = {Goldsmiths, University of London},
title = {{PiaF: A Tool for Augmented Piano Performance Using Gesture Variation Following}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}511.pdf},
year = {2014}
}
@inproceedings{Wyse2011,
address = {Oslo, Norway},
author = {Wyse, Lonce and Mitani, Norikazu and Nanayakkara, Suranga},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {group,interactive music performance,listening,mobile phone,music play,visual support},
number = {June},
pages = {2--5},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{The Effect of Visualizing Audio Targets in a Musical Listening and Performance Task}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}304.pdf},
year = {2011}
}
@inproceedings{dAlessandro:2012,
abstract = {We present the integration of two musical interfaces into a new music-making system that seeks to capture the expe- rience of a choir and bring it into the mobile space. This system relies on three pervasive technologies that each support a different part of the musical experience. First, the mobile device application for performing with an artificial voice, called ChoirMob. Then, a central composing and conducting application running on a local interactive display, called Vuzik. Finally, a network protocol to synchronize the two. ChoirMob musicians can perform music together at any location where they can connect to a Vuzik central conducting device displaying a composed piece of music. We explored this system by creating a chamber choir of ChoirMob performers, consisting of both experienced musicians and novices, that performed in rehearsals and live concert scenarios with music composed using the Vuzik interface.},
address = {Ann Arbor, Michigan},
author = {D'Alessandro, Nicolas and Pon, Aura and Wang, Johnty and Eagle, David and Sharlin, Ehud and Fels, Sidney},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {ChoirMob,OSC,Vuzik,choir,interactive display,interface design,mobile music,singing synthesis,social music},
publisher = {University of Michigan},
title = {{A Digital Mobile Choir: Joining Two Interfaces towards Composing and Performing Collaborative Mobile Music}},
year = {2012}
}
@inproceedings{Lopez2011,
abstract = {The DJ culture uses a gesture lexicon strongly rooted in thetraditional setup of turntables and a mixer. As novel toolsare introduced in the DJ community, this lexicon is adaptedto the features they provide. In particular, multitouch technologies can offer a new syntax while still supporting the oldlexicon, which is desired by DJs.We present a classification of DJ tools, from an interaction point of view, that divides the previous work into Traditional, Virtual and Hybrid setups. Moreover, we presenta multitouch tabletop application, developed with a groupof DJ consultants to ensure an adequate implementation ofthe traditional gesture lexicon.To conclude, we conduct an expert evaluation, with tenDJ users in which we compare the three DJ setups with ourprototype. The study revealed that our proposal suits expectations of Club/Radio-DJs, but fails against the mentalmodel of Scratch-DJs, due to the lack of haptic feedback torepresent the record's physical rotation. Furthermore, testsshow that our multitouch DJ setup, reduces task durationwhen compared with Virtual setups.},
address = {Oslo, Norway},
author = {Lopez, Pedro and Ferreira, Alfredo and Pereira, J A Madeiras},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {DJing,Expert User evaluation,HCI,Multitouch Interaction},
pages = {367--372},
title = {{Battle of the DJs: an HCI Perspective of Traditional, Virtual, Hybrid and Multitouch DJing}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}367.pdf},
year = {2011}
}
@inproceedings{Tremblay2010,
address = {Sydney, Australia},
author = {Tremblay, Pierre Alexandre and Schwarz, Diemo},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {1,audio mosaic,context - the sandboxes,corpus-based concatenative synthesis,haptic interface,laptop improvisation,multi-dimensional mapping,nime10},
number = {June},
pages = {15--18},
title = {{Surfing the Waves : Live Audio Mosaicing of an Electric Bass Performance as a Corpus Browsing Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}447.pdf},
year = {2010}
}
@inproceedings{Paradiso2003,
address = {Montreal, QC, Canada},
author = {Paradiso, Joseph A},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {228--234},
title = {{Dual-Use Technologies for Electronic Music Controllers: A Personal Perspective}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}228.pdf},
year = {2003}
}
@inproceedings{Castellano2007,
address = {New York City, NY, United States},
author = {Castellano, Ginevra and Bresin, Roberto and Camurri, Antonio and Volpe, Gualtiero},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {expressive interaction,interactive,multimodal environments,nime07},
pages = {390--391},
title = {{Expressive Control of Music and Visual Media by Full-Body Movement}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}390.pdf},
year = {2007}
}
@inproceedings{Roma2008,
abstract = {We present an audio waveform editor that can be operated in real time through a tabletop interface. The systemcombines multi-touch and tangible interaction techniques inorder to implement the metaphor of a toolkit that allows direct manipulation of a sound sample. The resulting instrument is well suited for live performance based on evolvingloops.},
address = {Genoa, Italy},
author = {Roma, Gerard and Xamb{\'{o}}, Anna},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {interaction techniques,musical performance,tabletop interface,tangible interface},
pages = {249--252},
title = {{A Tabletop Waveform Editor for Live Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}249.pdf},
year = {2008}
}
@inproceedings{Kuhara2011,
address = {Oslo, Norway},
author = {Kuhara, Yasuo and Kobayashi, Daiki},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {ipad,iphone,ipod touch,kinetics,particle,physics engine,smart phone,tablet pc,touch screen},
number = {June},
pages = {136--137},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Kinetic Particles Synthesizer Using Multi-Touch Screen Interface of Mobile Devices}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}136.pdf},
year = {2011}
}
@inproceedings{Nam:2013,
abstract = {This paper describes the Musical Poi (mPoi), a unique sensor-based musicalinstrument rooted in the ancient art of poi spinning. The trajectory ofcircular motion drawn by the performance and the momentum of the mPoiinstrument are converted to the energetic and vibrant sound, which makesspiritual and meditative soundscape that opens everyone up the aura and clearsthe thought forms away. The mPoi project and its concepts will be introducedfirst and then its interaction with a performer will be discussed.The mPoi project seeks to develop a prototype for a set of mobile musicalinstrument based on electronic motion sensors and circuit boards. Thistechnology is installed in egg-shaped structure and allows communicationbetween a performer and the mPoi instrument. The principal motivation for themPoi project has been a desire to develop an extensible interface that willsupport the Poi performance, which is a style of performance art originatedwith the Maori people of New Zealand involving swinging tethered weightsthrough a variety of rhythmical and geometric patterns. As an extension of the body and the expansion of the movement, the mPoiutilizes the creative performance of Poi to make spatial and spiritual soundand music. The aims of the mPoi project are:to create a prototype of mPoi instrument that includes circuit board thatconnects the instrument to a sensor.to develop a software, which includes programming of the circuit board and forthe sound generation.to make a new artistic expression to refine the captured sound into artisticmusical notes. The creative part of the project is to design a unique method to translate theperformer's gesture into sound. A unique algorithm was developed to extractfeatures of the swing motion and translate them into various patterns of sound.},
address = {Daejeon, Republic of Korea},
author = {Nam, Sangbong},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Jwibulnori,Musical Poi,Poi,mPoi,sensor-based musical instrument},
pages = {148--151},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Musical Poi (mPoi)}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Klugel:2013,
abstract = {Controlling the timbre generated by an audio synthesizerin a goal-oriented way requires a profound understandingof the synthesizer's manifold structural parameters. Especially shapingtimbre expressively to communicate emotional affect requires expertise.Therefore, novices in particular may not be able to adequately control timbrein viewof articulating the wealth of affects musically. In this context, the focus ofthis paper is the development of a model that can represent a relationshipbetween timbre and an expected emotional affect . The results of the evaluationof the presented model are encouraging which supports its use in steering oraugmenting the control of the audio synthesis. We explicitly envision thispaper as a contribution to the field of Synthesis by Analysis in the broadersense, albeit being potentially suitable to other related domains.},
address = {Daejeon, Republic of Korea},
author = {Kl{\"{u}}gel, Niklas and Groh, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Analysis by Synthesis,Deep Belief Networks,Emotional affect,Machine Learning,Timbre},
month = {may},
pages = {525--530},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Towards Mapping Timbre to Emotional Affect}},
url = {http://www.nime.org/proceedings/2013/},
year = {2013}
}
@inproceedings{Miranda2005,
address = {Vancouver, BC, Canada},
author = {Miranda, Eduardo and Brouse, Andrew},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {216--219},
title = {{Toward Direct Brain-Computer Musical Interfaces}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}216.pdf},
year = {2005}
}
@inproceedings{pdahlstedt1:2014,
abstract = {The idea behind the YouHero was two-fold. First, to make an expressive instrument out of the computer game toy guitar controller from the famous game GuitarHero. With its limited amount of control parameters, this was a challenge. Second, through this instrument we wanted to provide an alternative to the view that you become a hero by perfect imitation of your idols. Instead, play yourself. You are the hero. In this paper, we describe the design of the instrument, including its novel mapping approach based on switched timbre vectors scaled by accellerometer data, unconventional sound engines and the sound and mapping editing features, including manual editing of individual vectors. The instrument is evaluated through its practical applications during the whole project, with workshops with teenagers, a set of state-funded commissions from professional composers, and the development of considerable skill by the key performers. We have also submitted a performance proposal for this project.},
address = {London, United Kingdom},
author = {Dahlstedt, Palle and Karlsson, Patrik and Widell, Katarina and Blomdahl, Tony},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {403--406},
publisher = {Goldsmiths, University of London},
title = {{YouHero Making an Expressive Concert Instrument from the GuitarHero Controller}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}513.pdf},
year = {2014}
}
@inproceedings{Kimura2006,
address = {Paris, France},
author = {Kimura, Mari and Risset, Jean-Claude},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {auditory illusions,kimura,nal processing,psycho-acoustic phenomena,risset,sig-,subharmonics,violin},
pages = {407--408},
title = {{Auditory Illusion and Violin: Demonstration of a Work by Jean-Claude Risset Written for Mari Kimura}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}407.pdf},
year = {2006}
}
@inproceedings{Camurri2007a,
address = {New York City, NY, United States},
author = {Camurri, Antonio and Coletta, Paolo and Varni, Giovanna and Ghisio, Simone},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::;::},
keywords = {2,4 of eyesweb,eyesweb,multimodal interactive systems,nime07,performing arts,release 3 to release,the eyesweb evolution from,the eyesweb xmi platform},
pages = {305--308},
title = {{Developing Multimodal Interactive Systems with EyesWeb XMI}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}305.pdf},
year = {2007}
}
@inproceedings{Bullock2009,
abstract = {In this paper we describe findings related to user interfacerequirements for live electronic music arising from researchconducted as part of the first three-year phase of the EUfunded Integra project. A number of graphical user interface(GUI) prototypes developed during the Integra project initial phase are described and conclusions drawn about theirdesign and implementation.},
address = {Pittsburgh, PA, United States},
author = {Bullock, Jamie and Coccioli, Lamberto},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Design,Integra,Live Electronics,Music Technology,Usability,User Interface},
pages = {266--267},
title = {{Towards a Humane Graphical User Interface for Live Electronic Music}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}266.pdf},
year = {2009}
}
@inproceedings{Bouillot2009,
address = {Pittsburgh, PA, United States},
author = {Bouillot, Nicolas and Cooperstock, Jeremy R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {audio streaming,glitch detection,high-fidelity,latency measurement,networked musical performance,nime09},
pages = {135--140},
title = {{Challenges and Performance of High-Fidelity Audio Streaming for Interactive Performances}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}135.pdf},
year = {2009}
}
@inproceedings{Winkler2002,
address = {Dublin, Ireland},
author = {Winkler, Todd},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {dance,movement sensor,very,video processing,vns},
pages = {199--200},
title = {{Fusing Movement, Sound, and Video in Falling Up, an Interactive Dance/Theatre Production}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}199.pdf},
year = {2002}
}
@inproceedings{Barbosa2008,
address = {Genoa},
author = {Barbosa, {\`{A}}lvaro},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {algorithmic composition,behavioral driven,electronic music instruments,interfaces,network music instruments,nime08,performance,public music,real-time collaborative,sound},
pages = {9--12},
title = {{Ten-Hand Piano : A Networked Music Installation}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}009.pdf},
year = {2008}
}
@inproceedings{Leider2009a,
address = {Pittsburgh, PA, United States},
author = {Leider, Colby and Mann, Doug and Plazas, Daniel and Battaglia, Michael and Draper, Reid},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09,user customization,user modeling},
number = {Figure 1},
pages = {147--148},
title = {{The elBo and footPad : Toward Personalized Hardware for Audio Manipulation}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}147.pdf},
year = {2009}
}
@inproceedings{Schlessinger2009,
abstract = { We present the Kalichord: a small, handheld electro/acoustic instrument in which the player's right hand plucks virtual strings while his left hand uses buttons to play independent bass lines. The Kalichord uses the analog signal from plucked acoustic tines to excite a physical string model, allowing a nuanced and intuitive plucking experience. First, we catalog instruments related to the Kalichord. Then we examine the use of analog signals to excite a physical string model and discuss the expressiveness and form factors that this technique affords. We then describe the overall construction of the Kalichord and possible playing styles, and finally we consider ways we hope to improve upon the current prototype. },
address = {Pittsburgh, PA, United States},
author = {Schlessinger, Daniel and Smith, Julius O},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Kalichord,accordion,electro-acoustic instruments,kalimba,physical model,piezo,plucked string,tine},
pages = {98--101},
title = {{The Kalichord : A Physically Modeled Electro-Acoustic Plucked String Instrument}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}098.pdf},
year = {2009}
}
@inproceedings{Jorda2001,
address = {Seattle, WA, United States},
author = {Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
file = {::},
pages = {46--50},
title = {{New Musical Interfaces and New Music-making Paradigms}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}046.pdf},
year = {2001}
}
@inproceedings{Young2003a,
address = {Montreal, QC, Canada},
author = {Young, Diana and Serafin, Stefania},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
keywords = {evaluation},
mendeley-tags = {evaluation},
pages = {104--108},
title = {{Playability Evaluation of a Virtual Bowed String Instrument}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}104.pdf},
year = {2003}
}
@inproceedings{Laurson2008,
address = {Genoa, Italy},
author = {Laurson, Mikael and Kuuskankare, Mika},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {expressive timing,playing styles,synthesis control},
pages = {34--37},
title = {{Towards Idiomatic and Flexible Score-based Gestural Control with a Scripting Language}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}034.pdf},
year = {2008}
}
@inproceedings{nime2014:bjohnston,
abstract = {This paper details the creation, design, implementation and uses of a series of new mechanically bowed string instruments. These instruments have been designed with the objective of allowing for multiple parameters of musical expressivity, as well as including the physical and spatial features of the instruments to be integral aspects of their perception as instruments and sonic objects. This paper focuses on the hardware design, software implementation, and present musical uses of the ensemble.},
address = {London, United Kingdom},
author = {Johnston, Blake and Thrush, Henry Dengate and Kapur, Ajay and Murphy, Jim and Moleta, Tane},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {557--560},
publisher = {Goldsmiths, University of London},
title = {{Polus: The Design and Development of a New, Mechanically Bowed String Instrument Ensemble}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}355.pdf},
year = {2014}
}
@inproceedings{Miller2010,
abstract = {The console gaming industry is experiencing a revolution in terms of user control, and a large part to Nintendo's introduction of the Wii remote. The online open source development community has embraced the Wii remote, integrating the inexpensive technology into numerous applications. Some of the more interesting applications demonstrate how the remote hardware can be leveraged for nonstandard uses. In this paper we describe a new way of interacting with the Wii remote and sensor bar to produce music. The Wiiolin is a virtual instrument which can mimic a violin or cello. Sensor bar motion relative to the Wii remote and button presses are analyzed in real-time to generate notes. Our design is novel in that it involves the remote's infrared camera and sensor bar as an integral part of music production, allowing users to change notes by simply altering the angle of their wrist, and henceforth, bow. The Wiiolin introduces a more realistic way of instrument interaction than other attempts that rely on button presses and accelerometer data alone. },
address = {Sydney, Australia},
author = {Miller, Jace and Hammond, Tracy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {Wii remote,cello,gesture recognition.,human computer interaction,motion recognition,violin,virtual instrument},
pages = {497--500},
title = {{Wiiolin : a Virtual Instrument Using the Wii Remote}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}497.pdf},
year = {2010}
}
@inproceedings{Jacobs2008,
abstract = {None},
address = {Genoa, Italy},
author = {Jacobs, Robert and Feldmeier, Mark and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {None},
pages = {193--196},
title = {{A Mobile Music Environment Using a PD Compiler and Wireless Sensors}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}193.pdf},
year = {2008}
}
@inproceedings{Robertson2008,
address = {Genoa, Italy},
author = {Robertson, Andrew and Plumbley, Mark D and Bryan-Kinns, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {Automatic Accompaniment,Beat Tracking,Human-Computer Interaction,Musical Interface Evaluation},
pages = {319--324},
title = {{A Turing Test for B-Keeper : Evaluating an Interactive}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}319.pdf},
year = {2008}
}
@inproceedings{Stanza2002,
address = {Dublin, Ireland},
author = {Stanza, ,},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
pages = {165--166},
title = {{Amorphoscapes {\&} Soundtoys}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}165.pdf},
year = {2002}
}
@inproceedings{Lehrman2005,
abstract = {Electronic Musical Instrument Design is an excellent vehiclefor bringing students from multiple disciplines together towork on projects, and help bridge the perennial gap betweenthe arts and the sciences. This paper describes how at TuftsUniversity, a school with no music technology program,students from the engineering (electrical, mechanical, andcomputer), music, performing arts, and visual arts areas usetheir complementary skills, and teach each other, to developnew devices and systems for music performance and control.},
address = {Vancouver, BC, Canada},
author = {Lehrman, Paul D and Ryan, Todd M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {MIDI.,Science education,electronic music,engineering,gesture controllers,music education},
pages = {136--139},
title = {{Bridging the Gap Between Art and Science Education Through Teaching Electronic Musical Instrument Design}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}136.pdf},
year = {2005}
}
@inproceedings{nime2014:nklugel,
abstract = {In this contribution we will show three prototypical applications that allow users to collaboratively create rhythmic structures with successively more degrees of freedom to generate rhythmic complexity. By means of a user study we analyze the impact of this on the users' satisfaction and further compare it to data logged during the experiments that allow us to measure the rhythmic complexity created.},
address = {London, United Kingdom},
author = {Kl{\"{u}}gel, Niklas and Hagerer, Gerhard and Groh, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {50--53},
publisher = {Goldsmiths, University of London},
title = {{TreeQuencer: Collaborative Rhythm Sequencing - A Comparative Study}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}498.pdf},
year = {2014}
}
@inproceedings{Bevilacqua2007,
address = {New York City, NY, United States},
author = {Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Gu{\'{e}}dy, Fabrice and Schnell, Norbert and Fl{\'{e}}ty, Emmanuel and Leroy, Nicolas},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
doi = {10.1145/1279740.1279762},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {gesture recognition,gesture-follower,interface,music pedagogy,nime07,technology-enhanced learning},
pages = {124--129},
title = {{Wireless Sensor Interface and Gesture-Follower for Music Pedagogy}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}124.pdf},
year = {2007}
}
@inproceedings{Tanaka2006,
address = {Paris, France},
author = {Tanaka, Atau and Gemeinboeck, Petra},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {locative media,mobile music,urban fiction},
pages = {26--30},
title = {{A Framework for Spatial Interaction in Locative Media}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}026.pdf},
year = {2006}
}
@inproceedings{Place2008,
abstract = {An approach for creating structured Open Sound Control(OSC) messages by separating the addressing of node valuesand node properties is suggested. This includes a methodfor querying values and properties. As a result, it is possibleto address complex nodes as classes inside of more complextree structures using an OSC namespace. This is particularly useful for creating flexible communication in modularsystems. A prototype implementation is presented and discussed.},
address = {Genoa, Italy},
author = {Place, Timothy and Lossius, Trond and Jensenius, Alexander R and Peters, Nils},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {jamoma,namespace,nime08,osc,standardization},
pages = {181--184},
title = {{Addressing Classes by Differentiating Values and Properties in OSC}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}181.pdf},
year = {2008}
}
@inproceedings{Brent:2012,
abstract = {This paper introduces the Gesturally Extended Piano---an augmented instrument controller that relies on information drawn from performer motion tracking in order to control real-time audiovisual processing and synthesis. Specifically, the positions, heights, velocities, and relative distances and angles of points on the hands and forearms are followed. Technical details and installation of the tracking system are covered, as well as strategies for interpreting and mapping the resulting data in relation to synthesis parameters. Design factors surrounding mapping choices and the interrelation between mapped parameters are also considered.},
address = {Ann Arbor, Michigan},
author = {Brent, William},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Augmented instruments,controllers,mapping,motion tracking},
month = {may},
publisher = {University of Michigan},
title = {{The Gesturally Extended Piano}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}102.pdf},
year = {2012}
}
@inproceedings{Bisig:2013,
abstract = {This paper presents a proof of concept implementation of an interface entitledCoral. The interface serves as a physical and haptic extension of a simulatedcomplex system, which will be employed as an intermediate mechanism for thecreation of generative music and imagery. The paper discusses the motivationand conceptual context that underly the implementation, describes its technicalrealisation and presents some first interaction experiments. The paper focuseson the following two aspects: the interrelation between the physical andvirtual behaviours and properties of the interface and simulation, and thecapability of the interface to enable an intuitive and tangible exploration ofthis hybrid dynamical system.},
address = {Daejeon, Republic of Korea},
author = {Bisig, Daniel and Schiesser, S{\'{e}}bastien},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {generative art,haptic interface,swarm simulation},
pages = {385--388},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Coral - a Physical and Haptic Extension of a Swarm Simulation}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Marier:2012,
abstract = {A new method for interpolating between presets is described. The interpolation algorithm called Intersecting N-Spheres Interpolation is simple to compute and its generalization to higher dimensions is straightforward. The current imple- mentation in the SuperCollider environment is presented as a tool that eases the design of many-to-many mappings for musical interfaces. Examples of its uses, including such mappings in conjunction with a musical interface called the sponge, are given and discussed.},
address = {Ann Arbor, Michigan},
author = {Marier, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Interpolation,Mapping,Preset,Sponge,SuperCollider},
publisher = {University of Michigan},
title = {{Designing Mappings for Musical Interfaces Using Preset Interpolation}},
year = {2012}
}
@inproceedings{Wang2005a,
abstract = {ChucK is a programming language for real-time sound synthesis. It provides generalized audio abstractions and precise control over timing and concurrency - combining the rapid-prototyping advantages of high-level programming tools, such as Pure Data, with the flexibility and controllability of lower-level, text-based languages like C/C++. In this paper, we present a new time-based paradigm for programming controllers with ChucK. In addition to real-time control over sound synthesis, we show how features such as dynamic patching, on-the-fly controller mapping, multiple control rates, and precisely-timed recording and playback of sensors can be employed under the ChucK programming model. Using this framework, composers, programmers, and performers can quickly write (and read/debug) complex controller/synthesis programs, and experiment with controller mapping on-the-fly.},
address = {Vancouver, BC, Canada},
author = {Wang, Ge and Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Controller mapping,concurrency.,on-the-fly programming,programming language,real-time interaction},
pages = {196--199},
title = {{Yeah, ChucK It! = {\textgreater} Dynamic , Controllable Interface Mapping}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}196.pdf},
year = {2005}
}
@inproceedings{Kim:2013,
abstract = {"We discuss how to model ""gestures"" in music performance with statisticallatent-states models. A music performance can be described with compositionaland expressive properties varying over time. In those property changes we oftenobserve particular patterns, and such a pattern can be understood as a""gesture"", since it serves as a medium transferring specific emotions. Assuminga finite number of latent states on each property value changes, we candescribe those gestures with statistical latent-states models, and train themby unsupervised learning algorithms. In addition, model entropy provides us ameasure for different effects of each properties on the gesture implementation.Test result on some of real performances indicates that the trained modelscould capture the structure of gestures observed in the given performances, anddetect their boundaries. The entropy-based measure was informative tounderstand the effectiveness of each property on the gesture implementation.Test result on large corpora indicates that our model has potentials for afurther model improvement."},
address = {Daejeon, Republic of Korea},
author = {Kim, Taehun and Weinzierl, Stefan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Musical gestures,performance analysis,unsupervised machine learning},
pages = {427--430},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Modelling Gestures in Music Performance with Statistical Latent-State Models}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Mase2001,
address = {Seattle, WA, United States},
author = {Mase, Kenji and Yonezawa, Tomoko},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
file = {::},
keywords = {dance,image processing,music controller,new interface,stuffed toy,water interface},
pages = {34--37},
title = {{Body , Clothes , Water and Toys : Media Towards Natural Music Expressions with Digital Sounds}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}034.pdf},
year = {2001}
}
@inproceedings{Lai:2012,
abstract = {This paper addresses the issue of engaging the audience with new musical instruments in live performance context. We introduce design concerns that we consider influential to enhance the communication flow between the audience and the performer. We also propose and put in practice a design approach that considers the use of performance space as a way to engage with the audience. A collaborative project, Sound Gloves, presented here exemplifies such a concept by dissolving the space between performers and audience. Our approach resulted in a continuous interaction between audience and performers, in which the social dynamics was changed in a positive way in a live performance context of NIMEs. Such an approach, we argue, may be considered as one way to further engage and interact with the audience.},
address = {Ann Arbor, Michigan},
author = {Lai, Chi-Hsia and Tahiroglu, Koray},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {NIME,design approach,performance,wearable electronics},
publisher = {University of Michigan},
title = {{A Design Approach to Engage with Audience with Wearable Musical Instruments: Sound Gloves}},
year = {2012}
}
@inproceedings{Meier2010b,
author = {Meier, Max and Schranner, Max},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
file = {::},
keywords = {algorithmic composition,soft constraints,tangible interaction},
number = {Nime},
pages = {501--504},
title = {{The Planets}},
year = {2010}
}
@inproceedings{Savage2010,
address = {Sydney, Australia},
author = {Savage, Norma S and Ali, Syed R and Chavez, Norma E},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {accelerome-,audience,dj,interaction,interactive,mixer,mobile,multi-modal,music,n900,nime10,nokia,phone,smart phones,ter,touch screen},
number = {Nime},
pages = {395--398},
title = {{Mmmmm : A Multi-modal Mobile Music Mixer}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}395.pdf},
year = {2010}
}
@inproceedings{Pugliese:2012,
abstract = {In this paper strategies for augmenting the social dimension of collaborative music making, in particular in the form of bodily and situated interaction are presented. Mobile instruments are extended by means of relational descriptors democratically controlled by the group and mapped to sound parameters. A qualitative evaluation approach is described and a user test with participants playing in groups of three conducted. The results of the analysis show core- categories such as familiarity with instrument and situation, shift of focus in activity, family of interactions and different categories of the experience emerging from the interviews. Our evaluation shows the suitability of our approach but also the need for iterating on our design on the basis of the perspectives brought forth by the users. This latter observation confirms the importance of conducting a thorough interview session followed by data analysis on the line of grounded theory.},
address = {Ann Arbor, Michigan},
author = {Pugliese, Roberto and Tahiroglu, Koray and Goddard, Callum and Nesfield, James},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Collaborative music making,evaluation methods,human-human interaction.,mobile music},
publisher = {University of Michigan},
title = {{Augmenting human-human interaction in mobile group improvisation}},
year = {2012}
}
@inproceedings{Johnson:2013,
abstract = {This paper presents a new technique for interface-driven diffusion performance.Details outlining the development of a new tabletop surface-based performanceinterface, named tactile.space, are discussed. User interface and amplitudepanning processes employed in the creation of tactile.space are focused upon,and are followed by a user study-based evaluation of the interface. It is hopedthat the techniques described in this paper afford performers and composers anenhanced level of creative expression in the diffusion performance practice.This paper introduces and evaluates tactile.space, a multi-touch performanceinterface for diffusion built on the BrickTable. It describes how tactile.spaceimplements Vector Base Amplitude Panning to achieve real- time sourcepositioning. The final section of this paper presents the findings of a userstudy that was conducted by those who performed with the interface, evaluatingthe interface as a performance tool with a focus on the increased creativeexpression the interface affords, and directly comparing it to the traditionaldiffusion user interface.},
address = {Daejeon, Republic of Korea},
author = {Johnson, Bridget and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Multi touch,VBAP,diffusion,tabletop surface},
pages = {213--216},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{MULTI-TOUCH INTERFACES FOR PHANTOM SOURCE POSITIONING IN LIVE SOUND DIFFUSION}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Blaine2003,
address = {Montreal, QC, Canada},
author = {Blaine, Tina and Fels, Sidney S},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {:C$\backslash$:/{\_}ICSTdocs/7-literature/sources/Blaine, Fels - 2003 - Contexts of Collaborative Musical Experiences.pdf:pdf},
pages = {129--134},
title = {{Contexts of Collaborative Musical Experiences}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}129.pdf},
year = {2003}
}
@inproceedings{Naef2006,
address = {Paris, France},
author = {Naef, Martin and Collicott, Daniel},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
pages = {57--60},
title = {{A VR Interface for Collaborative 3D Audio Performance}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}057.pdf},
volume = {36},
year = {2006}
}
@inproceedings{Nicolls2009,
address = {Pittsburgh, PA, United States},
author = {Nicolls, Sarah},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {application in making more,gestural,interactive,motors,nime09,performance,piano,sensor,technologically integrated,technology},
pages = {203--206},
title = {{Twenty-First Century Piano}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}203.pdf},
year = {2009}
}
@inproceedings{Steiner2005,
address = {Vancouver, BC, Canada},
author = {Steiner, Hans-christoph},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {140--143},
title = {{[hid] toolkit: a Unified Framework for Instrument Design}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}140.pdf},
year = {2005}
}
@inproceedings{Kapuscinski2009,
address = {Pittsburgh, PA, United States},
author = {Kapuscinski, Jaroslaw and Sanchez, Javier},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {and aural elements,disklavier,intermedia,mapping,nime09,on counterpoint,piano,played by individual performers,show strong intermodal,to achieve it visual,visual music,wacom cintiq},
pages = {222--225},
title = {{Interfacing Graphic and Musical Elements in Counterlines}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}222.pdf},
year = {2009}
}
@inproceedings{Schedel2008,
address = {Genoa, Italy},
author = {Schedel, Margaret and Rootberg, Alison and de Martelly, Elizabeth},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {nime08},
pages = {339--342},
title = {{Scoring an Interactive, Multimedia Performance Work}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}339.pdf},
year = {2008}
}
@inproceedings{Holm2006,
address = {Paris, France},
author = {Holm, Jukka and Arrasvuori, Juha and Havukainen, Kai},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {background music,games,midi,midi-controlled,music,musically controlled games,reactive games,rhythm games,virtual sequencer},
pages = {65--70},
title = {{Using MIDI to Modify Video Game Content}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}065.pdf},
year = {2006}
}
@inproceedings{Hoofer2009,
address = {Pittsburgh, PA, United States},
author = {H{\"{o}}ofer, Andreas and Hadjakos, Aristotelis and M{\"{u}}hlh{\"{a}}user, Max},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {nime09},
pages = {175--176},
title = {{Gyroscope-Based Conducting Gesture Recognition}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}175.pdf},
year = {2009}
}
@inproceedings{Jackson:2012,
address = {Ann Arbor, Michigan},
author = {Jackson, Jay Alan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
publisher = {University of Michigan},
title = {{Bubble Drum-agog-ing: Polyrhythm Games {\&} Other Inter Activities}},
year = {2012}
}
@inproceedings{Baird2005,
abstract = {No Clergy is an interactive music performance/installation inwhich the audience is able to shape the ongoing music. In it,members of a small acoustic ensemble read music notation fromcomputer screens. As each page refreshes, the notation is alteredand shaped by both stochastic transformations of earlier musicwith the same performance and audience feedback, collected viastandard CGI forms.},
address = {Vancouver, BC, Canada},
author = {Baird, Kevin C},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Lilypond,Python,audience,interactive,notation,stochastic},
pages = {240--241},
title = {{Real-Time Generation of Music Notation via Audience Interaction Using Python and {\{}GNU{\}} Lilypond}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}240.pdf},
year = {2005}
}
@inproceedings{Carlile2005,
address = {Vancouver, BC, Canada},
author = {Carlile, Jennifer and Hartmann, Bj{\"{o}}rn},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
keywords = {Collaborative Control,Haptic Interfaces,Musical Controller},
pages = {250--251},
title = {{{\{}OR{\}}OBORO: A Collaborative Controller with Interpersonal Haptic Feedback}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}250.pdf},
year = {2005}
}
@inproceedings{Kvifte2006,
address = {Paris, France},
author = {Kvifte, Tellef and Jensenius, Alexander R},
booktitle = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME-06)},
editor = {Schnell, Norbert and Bevilacqua, Fr{\'{e}}d{\'{e}}ric},
file = {::},
keywords = {gestures,mapping,musical instrument design,organology},
pages = {220--225},
title = {{Towards a Coherent Terminology and Model of Instrument Description and Design}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}220.pdf},
year = {2006}
}
@inproceedings{Pardue2002,
address = {Dublin, Ireland},
author = {Pardue, Laurel S and Paradiso, Joseph A},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {music sequencer interface,passive tag,position tracking},
pages = {145--147},
title = {{Musical Navigatrics: New Musical Interactions with Passive Magnetic Tags}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}145.pdf},
year = {2002}
}
@inproceedings{Freeman2007,
address = {New York City, NY, United States},
author = {Freeman, Jason},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {audience interaction,collaboration,composition,flash,graph,internet,music,nime07,residency,traveling salesman,violin},
pages = {260--263},
title = {{Graph Theory : Interfacing Audiences Into the Compositional Process}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}260.pdf},
year = {2007}
}
@inproceedings{Spowage2009,
address = {Pittsburgh, PA, United States},
author = {Spowage, Neal},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {29--30},
title = {{The Ghetto Bastard : A Portable Noise Instrument}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}029.pdf},
year = {2009}
}
@inproceedings{Milmoe2007,
address = {New York City, NY, United States},
author = {Milmoe, Andrew},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {nime07},
pages = {423},
title = {{NIME Performance {\&} Installation : Sonic Pong V3.0}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}423.pdf},
year = {2007}
}
@inproceedings{Couturier2002,
address = {Dublin, Ireland},
author = {Couturier, Jean-Michel},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {graphics tablet,meta-parameters,multi-touch tactile,scanned synthesis,surface},
pages = {43--45},
title = {{A Scanned Synthesis Virtual Instrument}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}043.pdf},
year = {2002}
}
@inproceedings{Han:2012a,
abstract = {FutureGrab is a new wearable musical instrument for live performance that is highly intuitive while still generating an interesting sound by subtractive synthesis. Its sound effects resemble the human vowel pronunciation, which were mapped to hand gestures that are similar to the mouth shape of human to pronounce corresponding vowel. FutureGrab also provides all necessary features for a lead musical instrument such as pitch control, trigger, glissando and key adjustment. In addition, pitch indicator was added to give visual feedback to the performer, which can reduce the mistakes during live performances. This paper describes the motivation, system design, mapping strategy and implementation of FutureGrab, and evaluates the overall experience.},
address = {Ann Arbor, Michigan},
author = {Han, Yoonchang and Na, Jinsoo and Lee, Kyogu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Pure Data,Wearable musical instrument,data-glove,formant synthesis,gestural synthesis,subtractive synthesis,visual feedback},
publisher = {University of Michigan},
title = {{FutureGrab: A wearable subtractive synthesizer using hand gesture}},
year = {2012}
}
@inproceedings{Zbyszynski2007,
address = {New York City, NY, United States},
author = {Zbyszynski, Michael and Wright, Matthew and Momeni, Ali and Cullen, Daniel},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
doi = {10.1145/1279740.1279758},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {1,algorithmic composition,digitizing tablet,expressivity,gesture,mapping,nime07,position sensing,wacom tablet,why the wacom tablet},
pages = {100--105},
title = {{Ten Years of Tablet Musical Interfaces at CNMAT}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}100.pdf},
year = {2007}
}
@inproceedings{Dannenberg:2004,
address = {Hamamatsu, Japan},
author = {Dannenberg, Roger B},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
pages = {132--137},
title = {{Aura II: Making Real-Time Systems Safe for Music}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}132.pdf},
year = {2004}
}
@inproceedings{Goudeseune2001,
address = {Seattle, WA, United States},
author = {Goudeseune, Camille and Garnett, Guy and Johnson, Timothy},
booktitle = {Proceedings of the 2001 International Conference on New Interfaces for Musical Expression (NIME-01)},
editor = {Poupyrev, Ivan and Lyons, Michael J and Fels, Sidney S and Blaine, Tina},
file = {::},
keywords = {and decay time,bandwidth is coupled to,decay,decay times and vice,linux pc modulates the,time,tracking data from the,versa,wide bandwidths have short,width},
pages = {24--26},
title = {{Resonant Processing of Instrumental Sound Controlled by Spatial Position}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}024.pdf},
year = {2001}
}
@inproceedings{Carrascal2011,
address = {Oslo, Norway},
author = {Carrascal, Juan P and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {audio mixing,control surface,multitouch,touchscreen},
number = {June},
pages = {100--103},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Multitouch Interface for Audio Mixing}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}100.pdf},
year = {2011}
}
@inproceedings{Stark2007,
abstract = {We present a new group of audio effects that use beat tracking, the detection of beats in an audio signal, to relate effectparameters to the beats in an input signal. Conventional audio effects are augmented so that their operation is related tothe output of a beat tracking system. We present a temposynchronous delay effect and a set of beat synchronous lowfrequency oscillator effects including tremolo, vibrato andauto-wah. All effects are implemented in real-time as VSTplug-ins to allow for their use in live performance.},
address = {New York City, NY, United States},
author = {Stark, Adam M and Plumbley, Mark D and Davies, Matthew E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {a beat-synchronous tremolo effect,audio effects,beat tracking,figure 1,im-,nime07,plemented as a vst,plug-in,real-time,the rate is controlled,vst plug-in},
pages = {344--345},
title = {{Real-Time Beat-Synchronous Audio Effects}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}344.pdf},
year = {2007}
}
@inproceedings{Lieberman2009,
address = {Pittsburgh, PA, United States},
author = {Lieberman, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {nime09},
pages = {321},
title = {{Anigraphical Etude 9}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}321.pdf},
year = {2009}
}
@inproceedings{Baba2010,
address = {Sydney, Australia},
author = {Baba, Takashi and Hashida, Mitsuyo and Katayose, Haruhiro},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {conducting system,heuristics,nime10,sensor,template},
number = {Nime},
pages = {263--270},
title = {{"VirtualPhilharmony" : A Conducting System with Heuristics of Conducting an Orchestra}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}263.pdf},
year = {2010}
}
@inproceedings{mcartwright:2014,
abstract = {Programming an audio synthesizer can be a difficult task for many. However, if a user has a general idea of the sound they are trying to program, they may be able to imitate it with their voice. This paper presents SynthAssist, a system for interactively searching the synthesis space of an audio synthesizer. In this work, we present how to use the system for querying a database of audio synthesizer patches (i.e. settings/parameters) by vocal imitation and user feedback. To account for the limitations of the human voice, it uses both absolute and relative time series representations of features and relevance feedback on both the feature weights and time series to refine the query. The method presented in this paper can be used to search through large databases of previously existing ``factory presets" or program a synthesizer using the data-driven approach to automatic synthesizer programming.},
address = {London, United Kingdom},
author = {Cartwright, Mark and Pardo, Bryan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {363--366},
publisher = {Goldsmiths, University of London},
title = {{SynthAssist: Querying an Audio Synthesizer by Vocal Imitation}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}446.pdf},
year = {2014}
}
@inproceedings{Gong2009,
address = {Pittsburgh, PA, United States},
author = {Gong, Nan-Wei and Laibowitz, Mat and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {group performing activity,interactive music control,midi,nime09,pen controller,writing instrument},
pages = {74--77},
title = {{MusicGrip : A Writing Instrument for Music Control}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}074.pdf},
year = {2009}
}
@inproceedings{Smyth2002,
address = {Dublin, Ireland},
author = {Smyth, Tamara and Smith, Julius O},
booktitle = {Proceedings of the 2002 Conference on New Instruments for Musical Expression (NIME-02)},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
file = {::},
keywords = {Bioacoustics,Buckling mechanism,Cicada,Controllers,Physical Modeling},
pages = {24--27},
title = {{Creating Sustained Tones with the Cicada's Rapid Sequential Buckling Mechanism}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}161.pdf},
year = {2002}
}
@inproceedings{nime2014:dwikstrom,
abstract = {In this paper an emotionally justified approach for controlling sound with physiology is presented. Measurements of listeners' physiology, while they are listening to recorded music of their own choosing, are used to create a regression model that predicts features extracted from music with the help of the listeners' physiological response patterns. This information can be used as a control signal to drive musical composition and synthesis of new sounds - an approach involving concatenative sound synthesis is suggested. An evaluation study was conducted to test the feasibility of the model. A multiple linear regression model and an artificial neural network model were evaluated against a constant regressor, or dummy model. The dummy model outperformed the other models in prediction accuracy, but the artificial neural network model achieved significant correlations between predictions and target values for many acoustic features.},
address = {London, United Kingdom},
author = {Wikstr{\"{o}}m, D J Valtteri},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {549--552},
publisher = {Goldsmiths, University of London},
title = {{Musical Composition by Regressional Mapping of Physiological Responses to Acoustic Features}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}311.pdf},
year = {2014}
}
@inproceedings{Mills2010,
address = {Sydney, Australia},
author = {Mills, John A and {Di Fede}, Damien and Brix, Nicolas},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {audio library,minim,mu-,music programming,nime10,processing},
number = {Nime},
pages = {37--42},
title = {{Music Programming in Minim}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}037.pdf},
year = {2010}
}
@inproceedings{Mealla2011,
abstract = {The use of physiological signals in Human Computer Interaction (HCI) is becoming popular and widespread, mostlydue to sensors miniaturization and advances in real-timeprocessing. However, most of the studies that use physiologybased interaction focus on single-user paradigms, and itsusage in collaborative scenarios is still in its beginning. Inthis paper we explore how interactive sonification of brainand heart signals, and its representation through physicalobjects (physiopucks) in a tabletop interface may enhancemotivational and controlling aspects of music collaboration.A multimodal system is presented, based on an electrophysiology sensor system and the Reactable, a musical tabletop interface. Performance and motivation variables wereassessed in an experiment involving a test "Physio" group(N=22) and a control "Placebo" group (N=10). Pairs ofparticipants used two methods for sound creation: implicitinteraction through physiological signals, and explicit interaction by means of gestural manipulation. The resultsshowed that pairs in the Physio Group declared less difficulty, higher confidence and more symmetric control thanthe Placebo Group, where no real-time sonification was provided as subjects were using pre-recorded physiological signal being unaware of it. These results support the feasibilityof introducing physiology-based interaction in multimodalinterfaces for collaborative music generation.},
address = {Oslo, Norway},
author = {Mealla, Sebasti{\'{a}}n and V{\"{a}}aljam{\"{a}}ae, Aleksander and Bosi, Mathieu and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {also at specs laboratory,bci,collaboration,cscw,hci,multimodal interfaces,music,physiological computing,physiopucks,tabletops,universitat pompeu fabra},
pages = {149--154},
title = {{Listening to Your Brain : Implicit Interaction in Collaborative Music Performances}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}149.pdf},
year = {2011}
}
@inproceedings{Kaneko:2013,
abstract = {In this paper, a function-oriented musical interface, named the sound wheel{\_}x0011{\_},is presented. This interface is designed to manipulate musical functions likepitch class sets, tonal centers and scale degrees, rather than the {\_}x0010{\_}musicalsurface{\_}x0011{\_}, i.e. the individual notes with concrete note heights. The sound wheelhas an interface summarizing harmony theory, and the playing actions haveexplicit correspondencewith musical functions. Easy usability is realized by semi-automatizing theconversion process from musical functions into the musical surface. Thus, theplayer can use this interface with concentration on the harmonic structure,without having his attention caught by manipulating the musical surface.Subjective evaluation indicated the e{\_}x001B{\_}ffectiveness of this interface as a toolhelpful for understanding the music theory. Because of such features, thisinterface can be used for education and interactive training of tonal musictheory.},
address = {Daejeon, Republic of Korea},
author = {Kaneko, Shoken},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Interactive tonal music generation,Music education},
pages = {202--205},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Function-Oriented Interface for Music Education and Musical Expressions: ``the Sound Wheel''}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Collins2010,
address = {Sydney, Australia},
author = {Collins, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {beat tracking,contrary,musical agent,stream analysis},
pages = {125--129},
title = {{Contrary Motion : An Oppositional Interactive Music System}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}125.pdf},
year = {2010}
}
@inproceedings{Friberg2011,
abstract = {This is an overview of the three installations Hoppsa Universum, CLOSE and Flying Carpet. They were all designed as choreographed sound and music installations controlled by the visitors movements. The perspective is from an artistic goal/vision intention in combination with the technical challenges and possibilities. All three installations were realized with video cameras in the ceiling registering the users' position or movement. The video analysis was then controlling different types of interactive software audio players. Different aspects like narrativity, user control, and technical limitations are discussed. },
address = {Oslo, Norway},
author = {Friberg, Anders and K{\"{a}}llblad, Anna},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Gestures,choreography,dance,interactive music.,music installation},
pages = {128--131},
title = {{Experiences from Video-Controlled Sound Installations}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}128.pdf},
year = {2011}
}
@inproceedings{Torre2010,
address = {Sydney, Australia},
author = {Torre, Giuseppe and O'Leary, Mark and Tuohy, Brian},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {3d physics emulator,computer labs,design,educational tools,installation,interactive,network,nime10,public spaces,site-specific art,sound},
number = {Nime},
pages = {375--376},
title = {{POLLEN A Multimedia Interactive Network Installation}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}375.pdf},
year = {2010}
}
@inproceedings{Roberts:2013,
abstract = {We present research that extends the scope of the mobile application Control, aprototyping environment for defining multimodal interfaces that controlreal-time artistic and musical performances. Control allows users to rapidlycreate interfaces employing a variety of modalities, including: speechrecognition, computer vision, musical feature extraction, touchscreen widgets,and inertial sensor data. Information from these modalities can be transmittedwirelessly to remote applications. Interfaces are declared using JSON and canbe extended with JavaScript to add complex behaviors, including the concurrentfusion of multimodal signals. By simplifying the creation of interfaces viathese simple markup files, Control allows musicians and artists to make novelapplications that use and combine both discrete and continuous data from thewide range of sensors available on commodity mobile devices.},
address = {Daejeon, Republic of Korea},
author = {Roberts, Charles and Forbes, Angus and H{\"{o}}llerer, Tobias},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Music,interaction,mobile,multimodal},
pages = {102--105},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Enabling Multimodal Mobile Interfaces for Musical Performance}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Geiger2008,
abstract = {Being one of the earliest electronic instruments the basic principles of the Theremin have often been used to design new musical interfaces. We present the structured design and evaluation of a set of 3D interfaces for a virtual Theremin, the VRemin. The variants differ in the size of the interaction space, the interface complexity, and the applied IO devices. We conducted a formal evaluation based on the well-known AttrakDiff questionnaire for evaluating the hedonic and pragmatic quality of interactive products. The presented work is a first approach towards a participatory design process for musical interfaces that includes user evaluation at early design phases. },
address = {Genoa, Italy},
author = {Geiger, Christian and Reckter, Holger and Paschke, David and Schulz, Florian and Poepel, Cornelius},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
keywords = {3d interaction techniques,an important concept for,both hands,evaluation,few wimp interface concepts,in contrast the use,make efficient use of,nime08,of both hands is,theremin-based interfaces},
pages = {303--306},
title = {{Towards Participatory Design and Evaluation of Theremin-based Musical Interfaces}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}303.pdf},
year = {2008}
}
@inproceedings{Pardue:2013a,
abstract = {This paper explores the potential of near-field optical reflective sensing formusical instrument gesture capture. Near-field optical sensors are inexpensive,portable and non-intrusive, and their high spatial and temporal resolutionmakes them ideal for tracking the finer motions of instrumental performance.The paper discusses general optical sensor performance with detailedinvestigations of three sensor models. An application is presented to violinbow position tracking using reflective sensors mounted on the stick. Bowtracking remains a difficult task, and many existing solutions are expensive,bulky, or offer limited temporal resolution. Initial results indicate that bowposition and pressure can be derived from optical measurements of thehair-string distance, and that similar techniques may be used to measure bowtilt.},
address = {Daejeon, Republic of Korea},
author = {Pardue, Laurel and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
file = {::},
keywords = {LED,bow tracking,gesture,near-field sensing,optical sensor,photodiode,phototransistor,reflectance,violin},
pages = {363--368},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Near-Field Optical Reflective Sensing for Bow Tracking}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Tanaka:2002,
abstract = {This paper describes a technique of multimodal, multichannel control of electronic musical devices using two control methodologies, the Electromyogram (EMG) and relative position sensing. Requirements for the application of multimodal interaction theory in the musical domain are discussed. We introduce the concept of bidirectional complementarity to characterize the relationship between the component sensing technologies. Each control can be used independently, but together they are mutually complementary. This reveals a fundamental difference from orthogonal systems. The creation of a concert piece based on this system is given as example. },
address = {Dublin},
author = {Tanaka, Atau and Knapp, Benjamin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Casey, Ciar{\'{a}}n and Schneider, Kimberly and Hammond, Ed},
keywords = {Electromyogram,Human Computer Interaction,Musical Controllers,Position Sensing,Sensor Instruments},
pages = {171--176},
title = {{Multimodal Interaction in Music Using the Electromyogram and Relative Position Sensing}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}171.pdf},
year = {2002}
}
@inproceedings{mmainsbridge:2014,
abstract = {This paper explores the challenge of achieving nuanced control and physical engagement with gestural interfaces in performance. Performances with a prototype gestural performance system, Gestate, provide the basis for insights into the application of gestural systems in live contexts. These reflections stem from a performer's perspective, outlining the experience of prototyping and performing with augmented instruments that extend vocal or instrumental technique through ancillary gestures. Successful implementation of rapidly evolving gestural technologies in real-time performance calls for new approaches to performing and musicianship, centred around a growing understanding of the body's physical and creative potential. For musicians hoping to incorporate gestural control seamlessly into their performance practice a balance of technical mastery and kinaesthetic awareness is needed to adapt existing systems to their own purposes. Within non-tactile systems, visual feedback mechanisms can support this process by providing explicit visual cues that compensate for the absence of haptic or tangible feedback. Experience gained through prototyping and performance can yield a deeper understanding of the broader nature of gestural control and the way in which performers inhabit their own bodies.},
address = {London, United Kingdom},
author = {Mainsbridge, Mary and Beilharz, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
month = {jun},
pages = {110--113},
publisher = {Goldsmiths, University of London},
title = {{Body As Instrument: Performing with Gestural Interfaces}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}393.pdf},
year = {2014}
}
@inproceedings{Birchfield2005,
address = {Vancouver, BC, Canada},
author = {Birchfield, David and Lorig, David and Phillips, Kelly},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
keywords = {computing,dynamic systems,evolutionary,generative arts,installation art,music,robotics,sculpture,sound},
pages = {160--163},
title = {{Sustainable: a dynamic, robotic, sound installation}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}160.pdf},
year = {2005}
}
@inproceedings{Dolphin2009a,
address = {Pittsburgh, PA, United States},
author = {Dolphin, Andy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {animated interfaces,as,characteristic game models of,competition are not present,game engines,max,more common interests with,msp,nime09,open work,sound toys,the field of sound,toys},
pages = {159--160},
title = {{MagNular : Symbolic Control of an External Sound Engine Using an Animated Interface}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}159.pdf},
year = {2009}
}
@inproceedings{nime2014:ssuh,
abstract = {For the electric guitar, which takes a large proportion in modern pop music, effects unit (or effector) is no longer optional. Many guitarists already `play' their effects with their instrument. However, it is not easy to control these effects during the play, so lots of new controllers and interfaces have been devised; one example is a pedal type effects that helps players to control effects with a foot while their hands are busy. Some players put a controller on their guitars. However, our instruments are so precious to drill a hole, and the stage is too big for the player who is just kneeling behind the pedals and turning the knobs. In this paper, we designed a new control system for electric guitar and bass. This paper is about a gesture-based sound control system that controls the electric guitar effects (like delay time, reverberation or pitch) with the player's hand gesture. This system utilizes TAPIR signal to trace player's hand motion. TAPIR signal is an acoustic signal that can rarely be received by most people, because its frequency exists between 18 kHz to 22 kHz [TAPIR article]. This system consists of a signal generator, an electric guitar and a sound processor. From the generator that is attached on the player's hand, the TAPIR signal transfers to the magnetic pickup equipped on the electric guitar. Player's gesture is captured as a Doppler shift and the processor calculates the value as the sound effect parameter. In this paper, we focused on the demonstration of the signal transfer on aforementioned system.},
address = {London, United Kingdom},
author = {Suh, Sangwon and Lee, Jeong-seob and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {90--93},
publisher = {Goldsmiths, University of London},
title = {{A Gesture Detection with Guitar Pickup and Earphones}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}333.pdf},
year = {2014}
}
@inproceedings{Jones2007,
address = {New York City, NY, United States},
author = {Jones, Randy and Schloss, Andrew},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
doi = {10.1145/1279740.1279742},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {expressive control,instrument design,multi-touch,nime07,performance,physical modeling},
pages = {27--30},
title = {{Controlling a Physical Model with a 2D Force Matrix}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}027.pdf},
year = {2007}
}
@inproceedings{Fiebrink2009,
abstract = {Supervised learning methods have long been used to allow musical interface designers to generate new mappings by example. We propose a method for harnessing machine learning algorithms within a radically interactive paradigm, in which the designer may repeatedly generate examples, train a learner, evaluate outcomes, and modify parameters in real-time within a single software environment. We describe our meta-instrument, the Wekinator, which allows a user to engage in on-the-fly learning using arbitrary control modalities and sound synthesis environments. We provide details regarding the system implementation and discuss our experiences using the Wekinator for experimentation and performance. },
address = {Pittsburgh, PA, United States},
author = {Fiebrink, Rebecca and Trueman, Dan and Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
keywords = {Machine learning,mapping,tools.},
pages = {280--285},
title = {{A Meta-Instrument for Interactive, On-the-Fly Machine Learning}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}280.pdf},
year = {2009}
}
@inproceedings{Kim2010,
address = {Sydney, Australia},
author = {Kim, Hyun-Soo and Yoon, Je-Han and Jung, Moon-Sik},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {midi,mobile device,music composer,nime10,pattern composing},
number = {Nime},
pages = {444--446},
title = {{Interactive Music Studio : The Soloist}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}444.pdf},
year = {2010}
}
@inproceedings{Gaye2003,
address = {Montreal, QC, Canada},
author = {Gaye, Lalya and Maz{\'{e}}, Ramia and Holmquist, Lars E},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {109--115},
title = {{Sonic City: The Urban Environment as a Musical Interface}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}109.pdf},
year = {2003}
}
@inproceedings{Hamilton:2013,
abstract = {With a nod towards digital puppetry and game-based film genres such asmachinima, recent additions to UDKOSC of- fer an Open Sound Control (OSC)control layer for external control over both third-person ''pawn'' entitiesand camera controllers in fully rendered game-space. Real-time OSC input,driven by algorithmic process or parsed from a human-readable timed scriptingsyntax allows users to shape choreographies of gesture, in this case actormotion and action, as well as an audiences view into the game-spaceenvironment. As UDKOSC outputs real-time coordinate and action data generatedby UDK pawns and players with OSC, individual as well as aggregate virtualactor gesture and motion can be leveraged as a driver for both creative andprocedural/adaptive gaming music and audio concerns.},
address = {Daejeon, Republic of Korea},
author = {Hamilton, Rob},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Open Sound Control,game music,interactive sonification,procedural audio,procedural music},
pages = {446--449},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Sonifying Game-Space Choreographies With UDKOSC}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Nakra2003,
address = {Montreal, QC, Canada},
author = {Nakra, Teresa M},
booktitle = {Proceedings of the 2003 Conference on New Interfaces for Musical Expression (NIME-03)},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
file = {::},
pages = {151--152},
title = {{Immersion Music: a Progress Report}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}151.pdf},
year = {2003}
}
@inproceedings{Cerqueira:2013,
abstract = {SoundCraft is a framework that enables real-time data gathering from aStarCraft 2 game to external software applications, allowing for musicalinterpretation of the game's internal structure and strategies in novel ways.While players battle each other for victory within the game world, a customStarCraft 2 map collects and writes out data about players' decision-making,performance, and current focus on the map. This data is parsed and transmittedover Open Sound Control (OSC) in real-time, becoming the source for thesoundscape that accompanies the player's game. Using SoundCraft, we havecomposed a musical work for two em StarCraft 2 players, entitled GG Music. Thispaper details the technical and aesthetic development of SoundCraft, includingdata collection and sonic mapping. Please see the attached video file for a performance of GG Music using theSoundCraft framework.},
address = {Daejeon, Republic of Korea},
author = {Cerqueira, Mark and Salazar, Spencer and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {StarCraft 2,interactive game music,interactive sonification},
pages = {243--247},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{SoundCraft: Transducing StarCraft 2}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Bull2007,
address = {New York City, NY, United States},
author = {Bull, Steve and Gresham-Lancaster, Scot and Mintchev, Kalin and Svoboda, Terese},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
keywords = {nime07},
pages = {420},
title = {{Cellphonia : WET}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}420.pdf},
year = {2007}
}
@inproceedings{Feehan2009,
address = {Pittsburgh, PA, United States},
author = {Feehan, Noah},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Zahler, Noel and Dannenberg, Roger B and Sullivan, Tom},
file = {::},
keywords = {3-d positioning,collaborative performance,nime09,wii},
pages = {161--162},
title = {{Audio Orienteering  Navigating an Invisible Terrain}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}161.pdf},
year = {2009}
}
@inproceedings{Scott:2013,
abstract = {Digital music technology has transformed the listener experience and creatednew avenues for creative interaction and expression within the musical domain.The barrier to music creation, distribution and collaboration has been reduced,leading to entirely new ecosystems of musical experience. Software editingtools such as digital audio workstations (DAW) allow nearly limitlessmanipulation of source audio into new sonic elements and textures and havepromoted a culture of recycling and repurposing of content via mashups andremixes. We present a multi-touch application that allows a user to customizetheir listening experience by blending various versions of a song in real time.},
address = {Daejeon, Republic of Korea},
author = {Scott, Jeffrey and Moorhead, Mickey and Chapman, Justin and Schwabe, Ryan and Kim, Youngmoo E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Interactive media,Mobile devices,Multi-touch,Multi-track},
pages = {417--420},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Personalized Song Interaction Using a Multi Touch Interface}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Nesfield:2012,
abstract = {A general strategy for encouraging embodied engagement within musical interface design is introduced. A pair of ex- ample implementations of this strategy are described, one tangible and one graphical. As part of a potentially larger set within our general approach, two separate relationships are described termed `decay and contribution' and `instability and adjustment', which are heavily dependent on the action requirements and timeliness of the interaction. By suggesting this process occurs on a timescale of less than one second it is hoped attentiveness and engagement can be en- couraged to the possible benefit of future developments in digital musical instrument design.},
address = {Ann Arbor, Michigan},
author = {Nesfield, James},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {NIME,decay,design,embodiment,engagement,flow,instability},
publisher = {University of Michigan},
title = {{Strategies for Engagement in Computer-Mediated Musical Performance}},
year = {2012}
}
@inproceedings{Essl2005,
address = {Vancouver, BC, Canada},
author = {Essl, Georg and O'Modhrain, Sile},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {70--75},
title = {{Scrubber: An Interface for Friction-induced Sounds}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}070.pdf},
year = {2005}
}
@inproceedings{Wang2012,
abstract = {We have added a dynamic bio-mechanical mapping layer that contains a model of the human vocal tract with tongue muscle activations as input and tract geometry as output to a real time gesture controlled voice synthesizer system used for musical performance and speech research. Using this mapping layer, we conducted user studies comparing controlling the model muscle activations using a 2D set of force sensors with a position controlled kinematic input space that maps directly to the sound. Preliminary user evaluation suggests that it was more difficult to using force input but the resultant output sound was more intelligible and natural compared to the kinematic controller. This result shows that force input is a potentially feasible for browsing through a vowel space for an articulatory voice synthesis system, although further evaluation is required.},
address = {Ann Arbor, Michigan},
author = {Wang, Johnty and D'Alessandro, Nicolas and Fels, Sidney and Pritchard, Robert},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {Articulatory,Gesture,Mapping,Singing,Speech,Synthesis},
month = {may},
publisher = {University of Michigan},
title = {{Investigation of Gesture Controlled Articulatory Vocal Synthesizer using a Bio-Mechanical Mapping Layer}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}291.pdf},
year = {2012}
}
@inproceedings{Berdahl:2013,
abstract = {Satellite CCRMA is a platform for making embedded musical instruments andembedded installations. The project aims to help prototypes live longer byproviding a complete prototyping platform in a single, small, and stand-aloneembedded form factor. A set of scripts makes it easier for artists andbeginning technical students to access powerful features, while advanced usersenjoy the flexibility of the open-source software and open-source hardwareplatform.This paper focuses primarily on networking capabilities of Satellite CCRMA andnew software for enabling interactive control of the hardware-acceleratedgraphical output. In addition, some results are presented from robustness testsalongside specific advice and software support for increasing the lifespan ofthe flash memory.},
address = {Daejeon, Republic of Korea},
author = {Berdahl, Edgar and Salazar, Spencer and Borins, Myles},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Yeo, W and Lee, K and Sigman, A and H., Ji and Wakefield, G},
keywords = {Interface.js,Node.js,OpenGLES,Satellite CCRMA,SimpleGraphicsOSC,embedded installations,embedded musical instruments,hardware-accelerated graphics,union file system,write endurance},
pages = {325--330},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Embedded Networking and Hardware-Accelerated Graphics with Satellite CCRMA}},
url = {http://nime2013.kaist.ac.kr/},
year = {2013}
}
@inproceedings{Crevoisier2008,
address = {Genoa, Italy},
author = {Crevoisier, Alain and Kellum, Greg},
booktitle = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
editor = {Camurri, Antonio and Volpe, Gualtiero and Serafin, Stefania},
file = {::},
keywords = {computer vision,multi-touch interaction,musical interfaces,nime08},
pages = {113--116},
title = {{Transforming Ordinary Surfaces into Multi-touch Controllers}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}113.pdf},
year = {2008}
}
@inproceedings{nime2014:ibukvic,
abstract = {The following paper showcases new integrated Pd-L2Ork system and its K12 educational counterpart running on Raspberry Pi hardware. A collection of new externals and abstractions in conjunction with the Modern Device LOP shield transforms Raspberry Pi into a cost-efficient sensing hub providing Arduino-like connectivity with 10 digital I/O pins (including both software and hardware implementations of pulse width modulation) and 8 analog inputs, while offering a number of integrated features, including audio I/O, USB and Ethernet connectivity and video output.},
address = {London, United Kingdom},
author = {Bukvic, Ivica},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {163--166},
publisher = {Goldsmiths, University of London},
title = {{Pd-L2Ork Raspberry Pi Toolkit as a Comprehensive Arduino Alternative in K-12 and Production Scenarios}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}377.pdf},
year = {2014}
}
@inproceedings{Gutknecht2005,
address = {Vancouver, BC, Canada},
author = {Gutknecht, J{\"{u}}rg and Clay, Art and Frey, Thomas},
booktitle = {Proceedings of the 2005 International Conference on New Interfaces for Musical Expression (NIME-05)},
editor = {Fels, Sidney S and Blaine, Tina and Schloss, Andy and Jord{\`{a}}, Sergi},
file = {::},
pages = {148--151},
title = {{GoingPublik: Using Realtime Global Score Synthesis}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}148.pdf},
year = {2005}
}
@inproceedings{Kapur2007,
address = {New York City, NY, United States},
author = {Kapur, Ajay and Singer, Eric and Benning, Manjinder S and Tzanetakis, George and Trimpin, Trimpin},
booktitle = {Proceedings of the 2007 International Conference on New Interfaces for Musical Expression (NIME-07)},
editor = {Parkinson, Carol and D'Arcangelo, Gideon and Singer, Eric},
file = {::},
keywords = {electronic sitar,hyperinstruments,information retrieval,mir,music,musical robotics,nime07},
pages = {238--241},
title = {{Integrating HyperInstruments , Musical Robots {\&} Machine Musicianship for North Indian Classical Music}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}238.pdf},
year = {2007}
}
@inproceedings{Settel2003,
address = {Montreal},
author = {Settel, Zack and Lippe, Cort},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Wanderley, Marcelo M and McKenzie, Richard and Ostiguy, Louise},
pages = {197--200},
title = {{Convolution Brother{\{}$\backslash$textquoteright{\}}s Instrument Design}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}197.pdf},
year = {2003}
}
@inproceedings{Bokesoy2011,
address = {Oslo, Norway},
author = {B{\"{o}}kesoy, Sinan and Adler, Patrick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander R and Tveit, Anders and Godoy, Rolf I and Overholt, Dan},
keywords = {Sound installation,interactive systems,robotic music},
pages = {52--55},
title = {{1city1001vibrations : Development of a Interactive Sound Installation with Robotic Instrument Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}052.pdf},
year = {2011}
}
@inproceedings{DArcangelo2004,
address = {Hamamatsu, Japan},
author = {D'Arcangelo, Gideon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
file = {::},
keywords = {call and response,dj tools,oral culture,turntablism},
pages = {55--58},
title = {{Recycling Music, Answering Back: Toward an Oral Tradition of Electronic Music}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}055.pdf},
year = {2004}
}
@inproceedings{nime2014:pmathews,
abstract = {Networked musical performance - using networks of computers for live performance of electronic music - has evolved over a number of decades but has tended to rely upon customized and highly specialized software designed specifically for particular artistic goals. This paper presents Tangle, a flexible software framework designed to provide a basis for performance on any number of distinct instruments. The network includes features to simplify the control of robotic instruments, such as automated latency compensation and self-testing, while being simple to extend in order to implement device-specific logic and failsafes. Tangle has been tested on two diverse systems incorporating a number of unique and complex mechatronic instruments.},
address = {London, United Kingdom},
author = {Mathews, Paul and Morris, Ness and Murphy, Jim and Kapur, Ajay and Carnegie, Dale},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {187--190},
publisher = {Goldsmiths, University of London},
title = {{Tangle: a Flexible Framework for Performance with Advanced Robotic Musical Instruments}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}314.pdf},
year = {2014}
}
@inproceedings{nime2014:cnash,
abstract = {This paper explores the concept of end-user programming languages in music composition, and introduces the Manhattan system, which integrates formulas with a grid-based style of music sequencer. Following the paradigm of spreadsheets, an established model of end-user programming, Manhattan is designed to bridge the gap between traditional music editing methods (such as MIDI sequencing and typesetting) and generative and algorithmic music -- seeking both to reduce the learning threshold of programming and support flexible integration of static and dynamic musical elements in a single work. Interaction draws on rudimentary knowledge of mathematics and spreadsheets to augment the sequencer notation with programming concepts such as expressions, built-in functions, variables, pointers and arrays, iteration (for loops), branching (goto), and conditional statements (if-then-else). In contrast to other programming tools, formulas emphasise the visibility of musical data (e.g. notes), rather than code, but also allow composers to interact with notated music from a more abstract perspective of musical processes. To illustrate the function and use cases of the system, several examples of traditional and generative music are provided, the latter drawing on minimalism (process-based music) as an accessible introduction to algorithmic composition. Throughout, the system and approach are evaluated using the cognitive dimensions of notations framework, together with early feedback for use by artists.},
address = {London, United Kingdom},
author = {Nash, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau},
pages = {221--226},
publisher = {Goldsmiths, University of London},
title = {{Manhattan: End-User Programming for Music}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}371.pdf},
year = {2014}
}
@inproceedings{Levinson:2012,
abstract = {TedStick is a new wireless musical instrument that processes acoustic sounds resonating within its wooden body and ma- nipulates them via gestural movements. The sounds are transduced by a piezoelectric sensor inside the wooden body, so any tactile contact with TedStick is transmitted as audio and further processed by a computer. The main method for performing with TedStick focuses on extracting diverse sounds from within the resonant properties of TedStick it- self. This is done by holding TedStick in one hand and a standard drumstick in the opposite hand while tapping, rubbing, or scraping the two against each other. Gestural movements of TedStick are then mapped to parameters for several sound effects including pitch shift, delay, reverb and low/high pass filters. Using this technique the hand holding the drumstick can control the acoustic sounds/interaction between the sticks while the hand holding TedStick can fo- cus purely on controlling the sound manipulation and effects parameters.},
address = {Ann Arbor, Michigan},
author = {Levinson, Cory},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
keywords = {digital sound manipulation,gestural per- formance,piezoelectric sensors,tangible user interface},
publisher = {University of Michigan},
title = {{TedStick: A Tangible Electrophonic Drumstick}},
year = {2012}
}
@inproceedings{Roh2011,
address = {Oslo, Norway},
author = {Roh, Jung-Sim and Mann, Yotam and Freed, Adrian and Wessel, David},
booktitle = {Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME2011)},
editor = {Jensenius, Alexander R and Tveit, Anders and God{\o}y, Rolf I and Overholt, Dan},
file = {::},
keywords = {drum controller,e-,fabric sensor,multitouch,piezoresistive,surface interaction,tangible computing,textiles},
number = {June},
pages = {393--398},
publisher = {University of Oslo and Norwegian Academy of Music},
title = {{Robust and Reliable Fabric, Piezoresistive Multitouch Sensing Surfaces for Musical Controllers}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}393.pdf},
year = {2011}
}
@inproceedings{Yamaguchi2010,
address = {Sydney, Australia},
author = {Yamaguchi, Tomoyuki and Kobayashi, Tsukasa and Ariga, Anna and Hashimoto, Shuji},
booktitle = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
file = {::},
keywords = {dance performance,embodied sound media,musical interface,nime10},
number = {Nime},
pages = {116--119},
title = {{TwinkleBall : A Wireless Musical Interface for Embodied Sound Media}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}116.pdf},
year = {2010}
}
@inproceedings{Migneco:2012,
abstract = {Platforms for mobile computing and gesture recognitionprovide enticing interfaces for creative expression on virtualmusical instruments. However, sound synthesis on thesesystems is often limited to sample-based synthesizers, whichlimits their expressive capabilities. Source-filter models areadept for such interfaces since they provide flexible, algorithmic sound synthesis, especially in the case of the guitar.In this paper, we present a data-driven approach for modeling guitar excitation signals using principal componentsderived from a corpus of excitation signals. Using thesecomponents as features, we apply nonlinear principal components analysis to derive a feature space that describesthe expressive attributes characteristic to our corpus. Finally, we propose using the reduced dimensionality space asa control interface for an expressive guitar synthesizer.},
address = {Ann Arbor, Michigan},
author = {Migneco, Raymond and Kim, Youngmoo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Essl, G and Gillespie, B and Gurevich, M and O'Modhrain, S},
file = {::},
keywords = {PCA,Source-filter models,musical instrument synthesis,touch musical interfaces},
publisher = {University of Michigan},
title = {{A Component-Based Approach for Modeling Plucked-Guitar Excitation Signals}},
year = {2012}
}
@inproceedings{Mulder2010,
address = {Sydney, Australia},
author = {Mulder, Jos},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Beilharz, Kirsty and Bongers, Bert and Johnston, Andrew and Ferguson, Sam},
keywords = {amplification,modal perception,multi,musical instruments,nime10,performance practice,sound technology},
pages = {13--18},
title = {{The Loudspeaker as Musical Instrument}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}013.pdf},
year = {2010}
}
